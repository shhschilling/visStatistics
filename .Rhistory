#is equal to the total count in that row.
#
#The width of the box is the proportion of individuals in the row which fall into that cell.
# #Full mosaic plot with all data only if unique number of samples and fact below threshold
maxfactors = max(length(unique(samples)), length(unique(fact)))
threshold = 6
if (length(unique(samples)) < threshold &
length(unique(fact)) < threshold)
{
res = mosaic(
counts,
shade = TRUE,
legend = TRUE,  #shows pearsons residual
pop = F
#,main = titletext
)
tab <-
as.table(ifelse(counts < 0.005 * sum(counts), NA, counts))
#puts numbers on count
if (numbers == TRUE) {
labeling_cells(text = tab, margin = 0)(counts)
}
} else{
#
##Elimintate rows and columns distributing less than minperc total number of counts
rowSum = rowSums(counts)
colSum = colSums(counts)
total = sum(counts)
countscolumn_row_reduced = as.table(counts[which(rowSum > minperc * total),
which(colSum > minperc * total)])
#check dimensions after reduction: must be a contingency table
test = dim(as.table(countscolumn_row_reduced))
if (is.na(test[2]))
{
countsreduced = counts
}
else{
countsreduced = countscolumn_row_reduced
}
res = mosaic(
countsreduced,
shade = TRUE,
legend = TRUE,
cex.axis = 50 / maxfactors,
labeling_args = list(gp_labels = (gpar(
fontsize = 70 / maxfactors
))),
# main = titletext,
pop = F
)
if (numbers == TRUE) {
labeling_cells(text = countsreduced, margin = 0)(countsreduced)
}
my_list <-
list(
"mosaic_stats" =res
)
return(my_list)
}
#Helper functions--------------------------------------
#Check for type of samples and fact
type_sample_fact = function(samples, fact)
{
typesample = class(samples)
typefactor = class(fact)
listsf = list("typesample" = typesample, "typefactor" = typefactor)
return(listsf)
}
#helper function odds ratio
#calculation of odds ratio
odds_ratio = function(a, b, c, d, alpha, zerocorrect) {
attr(odds_ratio, "help") <-
"odds_ratio calculates odds ratio OR=(a/b)/(c/d) and corresponding upper and lower confidence intervalls\n INPUT: a = group 1 positive, c = group 2 positive, b=group 1 non positive, d = group 2 non positive, 1-alpha: confidence level, default alpha=0.05"
# "odds_ratio calculates odds ratio OR=(a/b)/(c/d) and corresponding upper and lower confidence intervalls\n
# INPUT: a=number of positives in  group 1, c=group 2 positive, b=group 1 non positive, d =group 2 non positive,default alpha=0.05, OR=(a/b)/(c/d)"\n
# a,b,c,d can be vectors, elementwise calculation
#
if (missing(alpha)) {
alpha = 0.05
}
if (missing(zerocorrect)) {
zerocorrect = TRUE
}
#odds ratio:=OR=a/b/(c/d)
#eliminate columns with zeros
#a=c=0 or b=d 0: no positive or no negative cases in both groups
# Higgins and Green 2011:
if (zerocorrect == TRUE)
{
#eliminate columns with zeros, if
#a=c=0 or b=d=0: no positive or no control cases in BOTH groups
# Higgins and Green 2011:
doublezero = which(a == 0 &
c == 0 | b == 0 & d == 0, arr.ind = T)
a[doublezero] = NaN
b[doublezero] = NaN
c[doublezero] = NaN
d[doublezero] = NaN
#Where zeros cause problems with computation of effects or standard errors, 0.5 is added to all cells (a, b, c, d)
singlezero = which(a == 0 |
b == 0 | c == 0 | d == 0, arr.ind = T)
a[singlezero] = a[singlezero] + 0.5
b[singlezero] = b[singlezero] + 0.5
c[singlezero] = c[singlezero] + 0.5
d[singlezero] = d[singlezero] + 0.5
}
oddA = a / b
oddB = c / d
OR = oddA / oddB
#confidence intervall
#SE of ln(OR)
SE = sqrt(1 / a + 1 / b + 1 / c + 1 / d)
alpha = 0.05
zalph <- qnorm(1 - alpha / 2)
logLOW = log(OR) - zalph * SE
logUP = log(OR) + zalph * SE
lowconf = exp(logLOW) #lower confidence
upconf = exp(logUP)
output = rbind(OR, lowconf, upconf, SE)
my_list = ("odds_ratio_statistics" = output)
return(my_list)
}
#create sorted table
makeTable = function(samples, fact, samplename, factorname)
{
counts = data.frame(fact, samples)
colnames(counts) = c(factorname, samplename)
counts2 = table(counts)
#sort by column sums
counts3 = counts2[, order(colSums(counts2), decreasing = T)]
#sort by row sums
counts4 = counts3[order(rowSums(counts3), decreasing = T),]
#remove columnns with all entries zero
counts4 = counts4[, colSums(counts4 != 0) > 0]
return(counts4)
}
fisher_chi = function(counts)
{
#if Cochran requirements for chi2 not given: fisher test is performed
# if more than 20% of cells have count smaller 5
#
if (any(counts == 0) #at least one cell with zero enry
|
sum(counts < 5) / length(counts) > 0.2# more than 20% of cells have count smaller 5
&
#Fisher Tests breaks down for too large tables
dim(counts)[2] < 7) {
#fisher.test
testFisherChi = fisher.test(
counts,
workspace = 1e9,
simulate.p.value = T,
hybrid = F,
B = 1e5
)
} else{
testFisherChi = chisq.test(counts)
}
return(testFisherChi)
}
side_of_nh = function(alternative)
{
if (alternative == "less") {
compare = c(">=")
} else if (alternative == "greater") {
compare = c("<=")
} else
compare = c("equals")
return(compare)
}
create_two_samples_vector = function(samples, fact)
{
#Creates column vector built out of two samples
#samples all in one column
levels = unique(sort(fact))
#two levels
if (length(levels) > 2) {
return(warning(
"warning: create_two_samples_vector: only two level input allowed"
))
} else{
samples1 = samples[fact == levels[1]]
samples1 <- samples1[!is.na(samples1)]
if (length(samples1) == 0) {
return(warning("each group needs at least one entry"))
} else{
samples2 = samples[fact == levels[2]]
samples2 <- samples2[!is.na(samples2)]
if (length(samples2) == 0) {
return(warning("each group needs at least one entry"))
} else {
x = c(samples1, samples2)
my_list = list(
"sample1" = samples1,
"sample2" = samples2,
"sample1and2" = x
)
return(my_list)
}
calc_min_max_of_y_axis = function(samples,
lowerExtramargin,
upperExtramargin)
{
maximum = max(samples, na.rm = T)
minimum = min(samples, na.rm = T)
spread = maximum - minimum
min_y_axis = minimum - lowerExtramargin * spread
max_y_axis = maximum + upperExtramargin * spread
return(list(min_y_axis, max_y_axis))
}
check_assumptions_shapiro = function(x)
{
x <- sort(x[complete.cases(x)])
n <- length(x)
rng <- x[n] - x[1L]#1L is integer
checkSize = !(is.na(n) || n < 3L || n > 5000L) #FALSE or TRUE
if (checkSize == FALSE)
{
warning("sample size must be between 3 and 5000")
return(FALSE)
}
if (rng == 0)
{
warning("all 'x' values are identical")
return(FALSE)
}
return(TRUE)
}
check_assumption_shapiro_size_range_two_samples = function(x1, x2) {
boolean1 = check_assumptions_shapiro(x1)
boolean2 = check_assumptions_shapiro(x2)
if (boolean1 == TRUE & boolean2 == TRUE)
{
return(TRUE)
} else{
return(FALSE)
}
check_assumptions_count_data = function(samples, fact)
{
counts = table(samples, fact)
sr <- rowSums(counts)
sc <- colSums(counts)
counts <- counts[sr > 0, sc > 0, drop = FALSE]
nr <- as.integer(nrow(counts))
nc <- as.integer(ncol(counts))
if (is.null(dim(counts)))
{
warning("no entries in count table ")
return(FALSE)
} else if (is.na(nr) || is.na(nc) || is.na(nr * nc))
{
warning("invalid nrow  or ncol in count data ", domain = NA)
return(FALSE)
} else if (nr <= 1L)
{
warning("need 2 or more non-zero row marginals")
return(FALSE)
} else if (nc <= 1L)
{
warning("need 2 or more non-zero column marginals")
return(FALSE)
} else{
return(TRUE)
}
sig_diffs_nongauss <- function(samples, fact)
{
# function to produce a table similar to that produced for TukeyHSD,
# but for non-normally distributed data
# calculate p values for each data classification based on pairwise.wilcox.test
ufactor = levels(fact)
pwt = pairwise.wilcox.test(samples, fact)
factormeans = matrix(0, length(ufactor), 1)
for (ii in 1:length(ufactor)) {
pos = which(fact == ufactor[ii])
factormeans[ii] = mean(samples[pos])
}
# make a matrix with a row for every possible combination of
# 2 data classifications and populate it with the calculated
# p values
xcomb = combn(length(ufactor), 2)
tukeylike = matrix(0, ncol(xcomb), 4)
colnames(tukeylike) <- c("diff", "lwr", "upr", "p adj")
tukeynames = vector("list", ncol(xcomb))
for (ii in 1:ncol(xcomb)) {
tukeynames[ii] =
paste(ufactor[xcomb[2, ii]], "-", ufactor[xcomb[1, ii]], sep = "")
p_value = pwt$p.value[xcomb[2, ii] - 1, xcomb[1, ii]]
if (is.na(p_value)) {
p_value = 1
}
tukeylike[ii, 4] = p_value
tukeylike[ii, 1] = 0
tukeylike[ii, 2] = 0
tukeylike[ii, 3] = 0
}
rownames(tukeylike) = tukeynames
# re-format the table slightly so it is the same as that produced
# by TukeyHSD and output
tukeylike2 = list(tukeylike)
#print(tukeylike2)
return(tukeylike2)
}
conf_band = function(x, reg, P, up) {
#reg: result of linear regression lm
#up: fact plus or minus
if (missing(P)) {
P = 0.05
}
if (missing(up)) {
up = 1
}
a = reg$coefficients[2]
b = reg$coefficients[1]
md = x - mean(x)
result = x
for (i in 1:length(x)) {
result[i] = a * x[i] + b + up * qt(P, length(x) - 2) * sqrt(sum(reg$resid *
reg$resid) / (length(x) - 2)) * sqrt(1 / (length(x) - 2) + md[i] ^ 2 / sum(md *
md))
}
return(result)
}
progn_band = function(x, reg, P, up) {
if (missing(P)) {
P = 0.05
}
if (missing(up)) {
up = 1
}
a = reg$coefficients[2]
b = reg$coefficients[1]
md = x - mean(x)
result = x
for (i in 1:length(x)) {
result[i] = a * x[i] + b + up * qt(P, length(x) - 2) * sqrt(sum(reg$resid *
reg$resid) / (length(x) - 2)) * sqrt(1 + 1 / (length(x) - 2) + md[i] ^ 2 /                                                                                                             sum(md * md))
}
return(result)
}
# Check for normality with Shapiro-Wilk-test without visualization----
test_norm = function(x) {
#Remove NA from x
x <- x[!is.na(x)]
#  KS = ks.test(x, pnorm, mean(x), sd(x))
shapiro_wilk_test = shapiro.test(x)
# my_list = list("Kolmogorov-Smirnoff" = KS, "Shapiro" =SH)
return(shapiro_wilk_test)
}
#Check length of distributions for t-test----
check_assumption_sample_size_t_test = function(x1, x2, minimum_size) {
#x1 sample 1
#x2 sample 2
#minimum_size:return TRUE if length> minimum_size
if (length(x1) > minimum_size & length(x2) > minimum_size)
{
return(TRUE)
} else{
return(FALSE)
}
#Define color scheme-----
#'\code{colorscheme(x)} selects color scheme of graphical output. Function parameter NULL lists all available color schemes, 1 a color tuple of green and blue
#'2 a color tuple of dark green and turquoi, 3 a colorplaette as defined by RcolorBrewer
#'
#' @param colorcode selects color scheme. parameters NULL: list of all available color schemes, 1: colortuple, 2, colortuple2, 3, ColorPalette
#' @return selected color scheme, colors are given with their Hex Code #RRGGBB names
colorscheme = function(colorcode = NULL)
{
browserLightGreen = "#B8E0B8" #matched part group0
browserLightBlue = "#B3D1EF"#matched part group1
browserLightTurquois = "#B3E1EF"#light turquois
browserDarkGreen = "#5CB85C" #dark green
colortuple = c(browserLightGreen, browserLightBlue)
colortuple2 = c(browserDarkGreen, browserLightTurquois)
#from package RColorBrewer Set 3
ColorPalette = c(
"#8DD3C7" ,
"#FFFFB3" ,
"#BEBADA" ,
"#FB8072",
"#80B1D3",
"#FDB462",
"#B3DE69",
"#FCCDE5",
"#D9D9D9",
"#BC80BD" ,
"#CCEBC5" ,
"#FFED6F"
)
my_list = list(
"colortuple" = colortuple,
"colortuple2" = colortuple2,
"ColorPalette" = ColorPalette
)
if (is.null(colorcode))
{
return(my_list)
}
else if (colorcode == 1) {
return(colortuple)
} else if (colorcode == 2) {
return(colortuple2)
} else if (colorcode == 3) {
return(ColorPalette)
} else{
message("Choose valid parameter: NULL, 1,2 or 3")
}
resetPar <- function() {
dev.new
while (!is.null(dev.list()))  dev.off() #restores to default values
oldpar <- par(no.readonly = TRUE)
return(oldpar)
}
oldparmosaic
oldparmosaic <- par(no.readonly = TRUE)
oldparmosaic$new=FALSE
on.exit(par(oldparmosaic))
par()
oldparmosaic
par(oldparmosaic)
test=par(oldparmosaic)
test
#Pearsons Chi squared, mosaic plot with Pearson's residuals
titanic_chi=visstat(titanic_train,"Survived","Pclass")
warnings()
titanic_chi=visstat(titanic_train,"Survived","Pclass",graphicsoutput = "png")
oldparCairo <- resetPar() #flashes the device and sets to standard values
on.exit(par(oldparCairo))
par()
titanic_chi=visstat(titanic_train,"Survived","Pclass",graphicsoutput = "png")
#Pearsons Chi squared, mosaic plot with Pearson's residuals
titanic_chi=visstat(titanic_train,"Survived","Pclass")
titanic_chi=visstat(titanic_train,"Survived","Pclass",graphicsoutput = "png")
titanic_chi=visstat(titanic_train,"Survived","Pclass")
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat_no_plots.R', echo=TRUE)
warnings()
##Examples------
library(visStatistics)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/R/visstat.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/R/test_and_visuals.R', echo=TRUE)
# linear regression: trees data set:----
linear_regression_trees=visstat(trees,"Girth","Height")
linear_regression_trees=visstat(trees,"Girth","Height",graphicsoutput = "png",plotDirectory=filedir)
linear_regression_trees=visstat(trees,"Girth","Height")
#display stats of linear regression
linear_regression_trees
#Welch two sample t.test: mtcars data set ----
mtcars$am = as.factor(mtcars$am)
welch_cars=visstat(mtcars,"mpg","am")
welch_cars=visstat(mtcars,"mpg","am",graphicsoutput = "png",plotDirectory=filedir)
welch_cars
#Kruskal-Wallis test: iris----
iris_kruskal=visstat(iris,"Petal.Width", "Species")
iris_kruskal=visstat(iris,"Petal.Width", "Species",graphicsoutput="png",plotDirectory=filedir)
iris_kruskal=visstat(iris,"Petal.Width", "Species") #error: overlaying plots
iris_kruskal
#Welch two sample t.test: InsectSprays ----
# select sprays A and B
InsectSpraysAB <- InsectSprays[ which(InsectSprays$spray == 'A'
| InsectSprays$spray == 'B'), ]
InsectSpraysAB$spray = factor(InsectSpraysAB$spray)
#Welcht-t-Test
insect_t_test=visstat(InsectSpraysAB,"count","spray")
insect_t_test
#t-test weight
women_weight <- c(38.9, 61.2, 73.3, 21.8, 63.4, 64.6, 48.4, 48.8, 48.5)
men_weight <- c(67.8, 60, 63.4, 76, 89.4, 73.3, 67.3, 61.3, 62.4)
# Create a data frame
weight_gender <- data.frame(
gender = as.factor(rep(c("Woman", "Man"), each = 9)),
weight = c(women_weight,  men_weight)
)
visstat(weight_gender,"weight","gender")
#Chi squared, mosaic plots with Titanic data set----
#install.packages("titanic")
#example categorical data,
library(titanic)
titanic_train$Survived = as.factor(titanic_train$Survived)
titanic_train$Pclass = as.factor(titanic_train$Pclass)
#Pearsons Chi squared, mosaic plot with Pearson's residuals
titanic_chi=visstat(titanic_train,"Survived","Pclass")
titanic_chi=visstat(titanic_train,"Survived","Pclass",graphicsoutput = "png")
titanic_chi=visstat(titanic_train,"Survived","Pclass")
titanic_chi
HairEyeColorMaleFisher =  HairEyeColor[,,1]
#replace cells to smaller values to enforce Cochran's rule
HairEyeColorMaleFisher[HairEyeColorMaleFisher<10] = 4
HairEyeColorMaleFisher = counts_to_cases(as.data.frame(HairEyeColorMaleFisher));
hair_chi=visstat(HairEyeColorMaleFisher,"Hair","Eye") #test statistics stored in res_chi
#2x2 contingency tables----
HairEyeColorMaleFisher = HairEyeColor[,,1]
#slicing out a 2 x2 contingency table
blackBrownHazelGreen = HairEyeColorMaleFisher[1:2,3:4]
fishertest = blackBrownHazelGreen
blackBrownHazelGreen = counts_to_cases(as.data.frame(blackBrownHazelGreen));
fisher_stats=visstat(blackBrownHazelGreen,"Hair","Eye")
fisher_stats
graphicaltypes=c(".png")
for (i in graphicaltypes) {
plotname=dir(filedir,pattern=i)
print(file.path(filedir,plotname))
file.remove(file.path(filedir,plotname))
}
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat_no_plots.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/R/test_and_visuals.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/R/test_and_visuals.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/R/test_and_visuals.R', echo=TRUE)
source('~/OneDrive - Hochschule Luzern/Forschung/visStatistics/tests/examples/examples_visstat.R', echo=TRUE)
traceback()
