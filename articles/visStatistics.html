<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>visStatistics: The right test, visualised â€¢ visStatistics</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="visStatistics: The right test, visualised">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">visStatistics</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.8</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/visStatistics.html">Vignette</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/shhschilling/visStatistics/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>visStatistics: The right test, visualised</h1>
                        <h4 data-toc-skip class="author">Sabine
Schilling</h4>
            <address class="author_afil">
      Institute of Tourism and Mobility, Lucerne University of Applied
Sciences and
Arts<br><a class="author_email" href="mailto:#"></a><a href="mailto:sabine.schilling@protonmail.com" class="email">sabine.schilling@protonmail.com</a>
      </address>
                  
            <h4 data-toc-skip class="date">2025-06-29</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/shhschilling/visStatistics/blob/master/vignettes/visStatistics.Rmd" class="external-link"><code>vignettes/visStatistics.Rmd</code></a></small>
      <div class="hidden name"><code>visStatistics.Rmd</code></div>

    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/shhschilling/visStatistics" class="external-link">visStatistics</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2">
<h2 id="abstract">Abstract<a class="anchor" aria-label="anchor" href="#abstract"></a>
</h2>
<p><code>visStatistics</code> automatically selects and visualises
appropriate statistical hypothesis tests between two column vectors of
type of class <code>"numeric"</code>, <code>"integer"</code>, or
<code>"factor"</code>. The choice of test depends on the
<code>class</code>, distribution, and sample size of the vectors, as
well as the user-defined â€˜conf.levelâ€™. The main function
<code><a href="../reference/visstat.html">visstat()</a></code> visualises the selected test with appropriate
graphs (box plots, bar charts, regression lines with confidence bands,
mosaic plots, residual plots, Q-Q plots), annotated with the main test
results, including any assumption checks and post-hoc analyses. This
scripted workflow is particularly suited for browser-based interfaces
that rely on server-side R applications connected to secure databases,
where users have no direct access, or for quick data visualisations and
test selection, e.g., in statistical consulting projects.</p>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<!-- Introductory statistics courses typically cover a fixed set of hypothesis tests, but selecting the correct method in practice can be error-prone.  -->
<!-- `visStatistics` addresses this by applying a deterministic, transparent test selection logic.  -->
<p>While numerous R packages provide statistical testing functionality,
few are designed with pedagogical accessibility as a primary concern.
The visStatistics package addresses this challenge by automating test
selection using deterministic decision logic, removing the burden of
manual test choice. This automation enables users to focus directly on
interpreting statistical outcomes rather than navigating test
selection.</p>
<p>The tailored visual outputsâ€”annotated with test results and, where
appropriate, assumption checks and post-hoc analysesâ€”further support
comprehension and help ensure valid conclusions from the outset. The
package is particularly valuable in statistical consulting for student
research projects, where time constraints demand streamlined,
assumption-aware output that prioritises interpretation over technical
execution. The implemented tests cover the typical content of an
introductory undergraduate course in statistics.</p>
<p>The package also suits server-based applications where users have
limited interaction: they provide only two input vectors, and the
software returns valid, interpretable results without requiring further
statistical knowledge. This supports reproducibility and correct
inference even in such constrained environments.</p>
<p>The remainder of this vignette is organised as follows:</p>
<ul>
<li><p>Section 3 focuses on the installation and the main function
call.</p></li>
<li><p>Section 4 briefly introduces the General Linear Model and the
testing of its assumptions in <code>visstatistic</code>.</p></li>
<li><p>Section 5 summarises the decision logic used to select a
statistical test.</p></li>
<li><p>Sections 6â€“8 provide background on the implemented tests and
illustrate the decision logic using examples. Function names in
parentheses in the headings indicate the corresponding statistical
hypothesis test function in R.</p></li>
<li><p>Section 9 outlines the main limitations of the package.</p></li>
<li><p>Section 10 provides an overview of the implemented
tests.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<div class="section level5">
<h5 class="unnumbered" id="install-the-latest-development-version-from-github">1. Install the latest development version from
GITHUB<a class="anchor" aria-label="anchor" href="#install-the-latest-development-version-from-github"></a>
</h5>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">install_github</span><span class="op">(</span><span class="st">"shhschilling/visStatistics"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 class="unnumbered" id="load-the-package">2. Load the package<a class="anchor" aria-label="anchor" href="#load-the-package"></a>
</h5>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/shhschilling/visStatistics" class="external-link">visStatistics</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 class="unnumbered" id="minimal-function-call">3. Minimal function call<a class="anchor" aria-label="anchor" href="#minimal-function-call"></a>
</h5>
<p>The function <code><a href="../reference/visstat.html">visstat()</a></code> accepts input in two ways:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Standardised form (recommended):</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Backward-compatible form</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">dataframe</span>, <span class="st">"namey"</span>, <span class="st">"namex"</span><span class="op">)</span></span></code></pre></div>
<p>In the standardised form, <code>x</code> and <code>y</code> must be
vectors of class <code>"numeric"</code>, <code>"integer"</code>, or
<code>"factor"</code>.</p>
<p>In the backward-compatible form, <code>"namex"</code> and
<code>"namey"</code> must be character strings naming columns in
<code>dataframe</code>, which must themselves be of class
<code>"numeric"</code>, <code>"integer"</code>, or
<code>"factor"</code>. This is equivalent to writing:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">dataframe</span><span class="op">[[</span><span class="st">"namex"</span><span class="op">]</span><span class="op">]</span>, <span class="va">dataframe</span><span class="op">[[</span><span class="st">"namey"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="general-linear-model-glm">General linear model (GLM)<a class="anchor" aria-label="anchor" href="#general-linear-model-glm"></a>
</h2>
<p>General Linear Models (GLM) <span class="citation">(<a href="#ref-Searle:1971">Searle 1971</a>)</span> are linear regression
models for a continuous response variable and given continuous and/or
categorical predictors. They provide a unified mathematical framework
underlying many common statistical tests like t-tests, ANOVA, and simple
linear regression implemented in <code>visStatistics</code>.</p>
<!-- The R-implementations of these tests,  `t.test(...var=equal)`, `aov()`, and `lm().` thus produce  mathematically equivalent results. -->
<!-- As a consequence, all these thests have the same underlying assumptions. -->
<!-- The General Linear Model (GLM) provides a unified mathematical framework underlying many common statistical tests like Student's t-tests, ANOVA, and simple linear regression. -->
<p>These tests are not distinct statistical methods, but rather
different implementations of the same underlying mathematical
framework.</p>
<!-- `t.test(..., var=equal)`, `aov()`, and `lm()` solve identical normal equations and produce mathematically equivalent results. - -->
<!-- The General Linear Model (GLM)  [@Nelder:1972,@McCullagh:1989] \ provides a unified statistical framework encompassing linear  -->
<!-- regression, analysis of variance (ANOVA), and t-tests within a single mathematical structure.  -->
<p>The GLM can be expressed in matrix form as:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ˜</mi><mo>=</mo><mi>ğ—</mi><mi>ğ›ƒ</mi><mo>+</mo><mi>ğ›†</mi></mrow><annotation encoding="application/x-tex">\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}</annotation></semantics></math></p>
<p>where:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ˜</mi><annotation encoding="application/x-tex">\mathbf{Y}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>Ã—</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
vector of observed responses</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
is an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>Ã—</mo><mi>p</mi></mrow><annotation encoding="application/x-tex">n \times p</annotation></semantics></math>
design matrix containing predictor variables<br>
</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ›ƒ</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
is a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>Ã—</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">p \times 1</annotation></semantics></math>
vector of unknown parameters</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ›†</mi><mo>âˆ¼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>ğŸ</mn><mo>,</mo><msup><mi>Ïƒ</mi><mn>2</mn></msup><mi>ğˆ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})</annotation></semantics></math>
are independent error (=residual) terms</li>
</ul>
<p>The least squares solution minimizing the sum of squared residuals
is:</p>
<p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>ğ›ƒ</mi><mo accent="true">Ì‚</mo></mover><mo>=</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ—</mi><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo>âˆ’</mo><mn>1</mn></mrow></msup><msup><mi>ğ—</mi><mi>T</mi></msup><mi>ğ˜</mi></mrow><annotation encoding="application/x-tex">\hat{\boldsymbol{\beta}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}</annotation></semantics></math></p>
<p>All GLM variants assume the response vector follows
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ˜</mi><mo>âˆ¼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>ğ›</mi><mo>,</mo><msup><mi>Ïƒ</mi><mn>2</mn></msup><mi>ğˆ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\mathbf{Y} \sim N(\boldsymbol{\mu},
\sigma^2\mathbf{I})</annotation></semantics></math>, where the mean
structure
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ›</mi><mo>=</mo><mi>ğ—</mi><mi>ğ›ƒ</mi></mrow><annotation encoding="application/x-tex">\boldsymbol{\mu} = \mathbf{X}\boldsymbol{\beta}</annotation></semantics></math>
connects predictors to response through different design matrices
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ğ—</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>.
Since these methods share identical distributional assumptions, they
require the same diagnostic procedures and assumption checking -
revealing the fundamental unity underlying seemingly distinct tests.</p>
<div class="section level3">
<h3 id="the-assumptions-of-the-glm-vis_anova_assumptions">The assumptions of the GLM (vis_anova_assumptions())<a class="anchor" aria-label="anchor" href="#the-assumptions-of-the-glm-vis_anova_assumptions"></a>
</h3>
<p>The key assumptions of the GLM are:</p>
<ol style="list-style-type: decimal">
<li>Independence of observations**: each observation is independent of
others</li>
<li>
<strong>Linearity</strong>: the relationship between predictors and
response is linear</li>
<li>
<strong>Normality of residuals</strong>:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ğ›†</mi><mo>âˆ¼</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>ğŸ</mn><mo>,</mo><msup><mi>Ïƒ</mi><mn>2</mn></msup><mi>ğˆ</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\boldsymbol{\varepsilon} \sim N(\mathbf{0}, \sigma^2\mathbf{I})</annotation></semantics></math>
</li>
<li>
<strong>Homoscedasticity</strong>: constant variance of residuals
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ïƒ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math>
is constant)</li>
</ol>
<!-- Importantly, it is the **residuals** (not the raw data in each group) that must be approximately normally distributed. --><p>The function <code>vis_anvaa_assumption()</code> checks the normality
of the standardized residuals by providing
<!-- When sample sizes are large, the Central Limit Theorem suggests that mild to moderate violations of the normality assumption have minimal impact on Type I error rates, though statistical power may still be affected [@Blanca:2017]. -->
diagnostic plots (Q-Q plots, residual vs.Â fitted values and histograms
of residuals ) alongside formal tests (Shapiro-Wilk, Anderson-Darling
for testing the normality of the residuals.</p>
<p>For testing homoscedacity), Levene-Test and Bartlett test or testing
homoscedacity) help users assess whether the assumptions of the GLM are
reasonably met.</p>
<div class="section level4">
<h4 id="normality-of-residuals-shapiro-test-and-ad-test">Normality of residuals (<code>shapiro.test()</code> and
<code>ad.test()</code>)<a class="anchor" aria-label="anchor" href="#normality-of-residuals-shapiro-test-and-ad-test"></a>
</h4>
<p>The <code><a href="../reference/vis_anova_assumptions.html">vis_anova_assumptions()</a></code> function evaluates the
normality of standardised residuals from the <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>-model
(which itself is a wrapper function for categorical features around the
<code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> function) using both the Shapiroâ€“Wilk test
(<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>) and the Andersonâ€“Darling test
(<code>ad.test()</code>)<span class="citation">(<a href="#ref-Gross:2015">Gross and Ligges 2015</a>)</span>. These tests
offer complementary strengths: Shapiroâ€“Wilk generally exhibits greater
power across a range of non-normal distributions in small samples,
whereas Andersonâ€“Darling is highly sensitive to tail deviations and
performs reliably in larger samples <span class="citation">(<a href="#ref-Razali:2011">Razali and Wah 2011</a>; <a href="#ref-Yap:2011">Yap and Sim 2011</a>)</span>.</p>
<p>Assessing assumptions solely through p-values can lead to both type I
errors (false positives) and type II errors (false negatives). In large
samples, even minor, random deviations from the null hypothesisâ€”such as
the assumption of normalityâ€”can result in statistically significant
p-values, leading to type I errors. Conversely, in small samples,
substantial violations of the assumption may not reach statistical
significance, resulting in type II errors <span class="citation">(<a href="#ref-Kozak:2018">Kozak and Piepho 2018</a>)</span>.</p>
<p>Thus, the robustness of statistical tests depends on both the sample
size and the shape of the underlying distribution. This is evident in
the limitations of the normality tests used: for instance, the
Shapiroâ€“Wilk test is unreliable for large samples
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>&gt;</mo><mn>5000</mn></mrow><annotation encoding="application/x-tex">N &gt; 5000</annotation></semantics></math>),
while the Andersonâ€“Darling test requires at least 7 observations.</p>
<p>Moreover, assumption tests provide no information on the nature of
deviations from the expected distribution <span class="citation">(<a href="#ref-Shatz:2024">Shatz 2024</a>)</span>. Therefore, the assessment
of normality should not rely solely on p-values but should be
complemented by visual inspection. <code><a href="../reference/visstat.html">visstat()</a></code> produces
diagnostic plots including: (1) a histogram of the standardised
residuals overlaid with the standard normal distribution, (2) a scatter
plot of the standardised residuals versus the fitted values for each
predictor level, and (3) a Qâ€“Q plot of the residuals.</p>
<p>Since algorithmic logic cannot replace the combination of formal
tests and expert visual judgement, the function defaults to assuming
normality if the Shapiroâ€“Wilk test yields a p-value greater than alpha.
Simulation studies suggest that Shapiroâ€“Wilk has the highest power among
normality tests in small to moderate samples <span class="citation">(<a href="#ref-Razali:2011">Razali and Wah 2011</a>)</span>.</p>
</div>
<div class="section level4">
<h4 id="equal-variances-across-groups-levene-test-and-bartlett-test">Equal variances across groups
(<code>levene.test() and bartlett.test()</code>)<a class="anchor" aria-label="anchor" href="#equal-variances-across-groups-levene-test-and-bartlett-test"></a>
</h4>
<p>Both <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> assess whether
two or more samples drawn from normal distributions have the same mean.
While <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> assumes homogeneity of variances across groups,
<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> does not require equal variances.</p>
<p>The decision logic of <code>visStatistics</code> assumes homogeneity
of variances if the Leveneâ€“Brownâ€“Forsythe test (implemented as
<code>levene.test()</code>) <span class="citation">(<a href="#ref-Brown:1974">Brown and Forsythe 1974</a>)</span> yields a
p-value greater than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.</p>
<p>The Leveneâ€“Brownâ€“Forsythe test evaluates the null hypothesis that all
groups have equal variances by testing whether the absolute deviations
from group medians are equal across groups.</p>
<p>For each observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">y_{ij}</annotation></semantics></math>
in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
it computes the absolute deviation from the group median:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><mover><msub><mi>y</mi><mi>i</mi></msub><mo accent="true">Ìƒ</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">z_{ij} = |y_{ij} - \tilde{y_i}|</annotation></semantics></math>,</p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><msub><mi>y</mi><mi>i</mi></msub><mo accent="true">Ìƒ</mo></mover><annotation encoding="application/x-tex">\tilde{y_i}</annotation></semantics></math>
is the median of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
<p>The test statistic is the F-statistic from a one-way ANOVA on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">z_{ij}</annotation></semantics></math>
values:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mfrac><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo>âˆ’</mo><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><msub><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>âˆ’</mo><mi>k</mi></mrow></mfrac></mfrac><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>âˆ’</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo>âˆ’</mo><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><msub><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">F = \frac{\frac{\sum_{i=1}^{k} n_i (\bar{z}_i - \bar{z})^2}{k-1}}{\frac{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (z_{ij} - \bar{z}_i)^2}{N-k}} = \frac{(N-k) \sum_{i=1}^{k} n_i (\bar{z}_i - \bar{z})^2}{(k-1) \sum_{i=1}^{k} \sum_{j=1}^{n_i} (z_{ij} - \bar{z}_i)^2}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the total sample size,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\bar{z}_i</annotation></semantics></math>
is the mean of absolute deviations from the median in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>z</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar{z}</annotation></semantics></math>
is the overall mean of all absolute deviations.</p>
<p>Under the null hypothesis of equal variances, the test statistic
follows an F-distribution:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>âˆ¼</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo>,</mo><mi>N</mi><mo>âˆ’</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F \sim F(k-1, N-k)</annotation></semantics></math>.</p>
<p>The Leveneâ€“Brownâ€“Forsythe test improves upon Leveneâ€™s original test
<span class="citation">(<a href="#ref-Levene:1960">Levene
1960</a>)</span> by using the median instead of the mean to centre the
data. This makes it more robust to skewed data or data with outliers
providing more reliable results in many practical situations. Note that
<code>levene.test()</code> mimics the default behaviour of
<code>leveneTest()</code> in the <code>car</code> package <span class="citation">(<a href="#ref-Fox:2019">Fox and Weisberg
2019</a>)</span>.</p>
<p>Additionally, homoscedasticity is assessed via Bartlettâ€™s test
(<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>), which has more power than the
Brownâ€“Forsythe version of Leveneâ€™s test <span class="citation">(<a href="#ref-Brown:1974">Brown and Forsythe 1974</a>)</span> when the
normality assumption is met <span class="citation">(<a href="#ref-Allingham:2012">Allingham and Rayner 2012</a>)</span></p>
<p>Bartlettâ€™s test evaluates whether sample variances are equal across
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
normally distributed groups. The test statistic is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mn>2</mn></msup><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>âˆ’</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><mo>âˆ’</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mi>i</mi></msub><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mrow><mn>3</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><mn>1</mn><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mo>âˆ’</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>âˆ’</mo><mi>k</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
K^2 = \frac{(N - k) \ln s_p^2 - \sum_{i=1}^k (n_i - 1) \ln s_i^2}{
            1 + \frac{1}{3(k - 1)} \left( \sum_{i=1}^k \frac{1}{n_i - 1} 
            - \frac{1}{N - k} \right)},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_i^2</annotation></semantics></math>
is the sample variance of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_p^2</annotation></semantics></math>
is the pooled variance:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>âˆ’</mo><mi>k</mi></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mi>i</mi></msub><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
s_p^2 = \frac{1}{N - k} \sum_{i=1}^k (n_i - 1) s_i^2.
</annotation></semantics></math></p>
<p>Under the null hypothesis that all group variances are equal and the
data are normally distributed, the test statistic approximately follows
a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-distribution
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>
degrees of freedom <span class="citation">(<a href="#ref-Bartlett:1937">Bartlett 1937</a>)</span>.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="decision-logic">Decision logic<a class="anchor" aria-label="anchor" href="#decision-logic"></a>
</h2>
<p>Throughout the remainder, data of class <code>"numeric"</code> or
<code>"integer"</code> are referred to by their common <code>mode</code>
<code>numeric</code>, while data of class <code>"factor"</code> are
referred to as categorical. The significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
used throughout for hypothesis testing, is defined as
<code>1 - conf.level</code>, where <code>conf.level</code> is a
user-controllable argument (defaulting to <code>0.95</code>).</p>
<p>The choice of statistical tests performed by the function
<code><a href="../reference/visstat.html">visstat()</a></code> depends on whether the data are numeric or
categorical, the number of levels in the categorical variable, the
distribution of the data, as well as the user-defined â€˜conf.levelâ€™.</p>
<p>The function prioritizes interpretable visual output and tests that
remain valid under the following decision logic:</p>
<div class="section level3">
<h3 id="numeric-response-and-categorical-predictor-comparing-central-tendencies">Numeric response and categorical predictor: Comparing central
tendencies<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor-comparing-central-tendencies"></a>
</h3>
<p>When the response is numeric and the predictor is categorical, a
statistical hypothesis test of central tendencies is selected.</p>
<ul>
<li><p>If the categorical predictor has exactly two levels, Welchâ€™s
t-test (<code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>) is applied when both groups contain more
than 30 observations. This heuristic is based on the central limit
theorem, which ensures approximate normality of the sampling
distribution of the mean <span class="citation">(<a href="#ref-Rasch:2011">Rasch, Kubinger, and Moder 2011</a>; <a href="#ref-Lumley:2002">Lumley et al. 2002</a>; <a href="#ref-Kwak:2017">Kwak and Kim 2017</a>)</span>. Welchâ€™s t-test is
the default method in R for comparing means and is generally preferred
over Studentâ€™s t-test because it does not assume equal variances. It
maintains comparable power even when variances are equal and outperforms
Studentâ€™s test when variances differ <span class="citation">(<a href="#ref-Moser:1992">Moser and Stevens 1992</a>; <a href="#ref-Fagerland:2009">Fagerland and Sandvik 2009</a>; <a href="#ref-Delacre:2017">Delacre, Lakens, and Leys
2017</a>)</span>.</p></li>
<li><p>For smaller samples, group-wise normality is assessed using the
Shapiro-Wilk test [<span class="citation">SHAPIRO and WILK (<a href="#ref-Shapiro:1965">1965</a>)</span> (<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>)
at a significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
Simulation studies show that the Shapiro-Wilk test is the most powerful
for detecting non-normality across most distributions, especially with
smaller sample sizes.<span class="citation">(<a href="#ref-Razali:2011">Razali and Wah 2011</a>; <a href="#ref-Ghasemi:2012">Ghasemi and Zahediasl 2012</a>)</span> If both
groups are found to be approximately normally distributed according to
the Shapiroâ€“Wilk test, Welchâ€™s t-test is applied; otherwise, the
Wilcoxon rank-sum test (<code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>) is used.</p></li>
<li><p>For predictors with more than two levels, a model of Fisherâ€™s
one-way analysis of variables (ANOVA) (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) is initially
fitted.<!-- The normality of residuals is evaluated by the  the Shapiro--Wilk test (`shapiro.test()`) and the Anderson-Darling test (`ad.test()`); residuals are considered approximately normal if at least one of the two tests yields a result exceeding the significance threshold $\alpha$. -->
The normality of residuals is evaluated by the Shapiro-Wilk test
(<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>); residuals are considered approximately
normal if it yields a result exceeding the significance threshold
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
If this condition is met, the Leveneâ€“Brownâ€“Forsythe test (implemented as
<code>levene.test()</code>) <span class="citation">(<a href="#ref-Brown:1974">Brown and Forsythe 1974</a>)</span> assesses
homoscedasticity. When variances are homogeneous
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>Î±</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>),
Fisherâ€™s one-way ANOVA (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) is applied with Tukeyâ€™s
Honestly Significant Differences (HSD) (<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>) for
post-hoc comparison. If variances differ significantly
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>â‰¤</mo><mi>Î±</mi></mrow><annotation encoding="application/x-tex">p \le \alpha</annotation></semantics></math>),
Welchâ€™s heteroscedastic one-way ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) is
used, also followed by Tukeyâ€™s HSD. If residuals are not normally
distributed according to both tests
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>â‰¤</mo><mi>Î±</mi></mrow><annotation encoding="application/x-tex">p \le \alpha</annotation></semantics></math>),
the Kruskalâ€“Wallis test (<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>) is selected,
followed by pairwise Wilcoxon tests
(<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code>). A graphical overview of the
decision logic used is provided in the figure below.</p></li>
</ul>
<div style="border: 1px solid #666; padding: 10px; display: inline-block; text-align: center;">
<img src="figures/decision_tree.png" width="100%" alt="Decision tree used to select the appropriate statistical test."><p style="font-style: italic; font-size: 90%; margin-top: 0.5em;">
Decision tree used to select the appropriate statistical test for a
categorical predictor and numeric response, based on the number of
factor levels, normality, and homoscedasticity.
</p>
</div>
</div>
<div class="section level3">
<h3 id="numeric-response-and-numeric-predictor-simple-linear-regression">Numeric response and numeric predictor: Simple linear
regression<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor-simple-linear-regression"></a>
</h3>
<p>When both the response and predictor are numeric, a simple linear
regression model (<code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code>) is fitted and analysed in detail,
including residual diagnostics, formal tests, and the plotting of fitted
values with confidence bands. Note that <strong>only one</strong>
predictor variable is allowed, as the function is designed for
two-dimensional visualisation.</p>
</div>
<div class="section level3">
<h3 id="both-variables-categorical-comparing-proportions">Both variables categorical: Comparing proportions<a class="anchor" aria-label="anchor" href="#both-variables-categorical-comparing-proportions"></a>
</h3>
<p>When both variables are categorical, no direction is assumed; the
order of variables in the function call does not affect the test
statistic, but it does influence the graphical output. For consistency,
we continue referring to the variables as <em>predictor</em> and
<em>response</em>.</p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis that the variables
are independent using either Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
test (<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>) or Fisherâ€™s exact test
(<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>), depending on expected cell counts. The
choice of test is based on Cochranâ€™s rule <span class="citation">(<a href="#ref-Cochran:1954">Cochran 1954</a>)</span>, which advises that
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
approximation is reliable only if no expected cell count is less than 1
and no more than 20 percent of cells have expected counts below 5.</p>
</div>
</div>
<div class="section level2">
<h2 id="numeric-response-and-categorical-predictor-comparing-central-tendencies-1">Numeric response and categorical predictor: Comparing central
tendencies<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor-comparing-central-tendencies-1"></a>
</h2>
<p>When the predictor consists of <code>class</code>
â€œ<code>factor</code>â€ with two or more levels and the response is of
<code>class</code> â€œ<code>numeric</code>â€ or â€œ<code>integer</code>â€
(both having mode â€œ<code>numeric</code>â€), statistical tests are applied
to compare the central tendencies across groups. This section describes
the conditions under which parametric and non-parametric tests are
chosen, based on the response type, the number of factor levels, and the
underlying distributional assumptions.</p>
<div class="section level3">
<h3 id="categorical-predictor-with-two-levels-welchs-t-test-and-wilcoxon-rank-sum">Categorical predictor with two levels: Welchâ€™s t-test and Wilcoxon
rank-sum<a class="anchor" aria-label="anchor" href="#categorical-predictor-with-two-levels-welchs-t-test-and-wilcoxon-rank-sum"></a>
</h3>
<p>When the predictor variable has exactly two levels, Welchâ€™s t-test or
the Wilcoxon rank-sum test is applied.</p>
<div class="section level4">
<h4 id="welchs-t-test-t-test">Welchâ€™s t-test (<code>t.test()</code>)<a class="anchor" aria-label="anchor" href="#welchs-t-test-t-test"></a>
</h4>
<p>Welchâ€™s t-test (<code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>) assumes that the observations
are independent and that the response variable is approximately normally
distributed within each group.</p>
<p>It evaluates the null hypothesis that the means of two groups are
equal without assuming equal variances.</p>
<p>The test statistic is given by <span class="citation">(<a href="#ref-Welch:1947">Welch 1947</a>; <a href="#ref-Satterthwaite:1946">Satterthwaite 1946</a>)</span></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mfrac><mrow><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mn>1</mn></msub><mo>âˆ’</mo><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mn>2</mn></msub></mrow><msqrt><mrow><mfrac><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>2</mn></msub></mfrac></mrow></msqrt></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}},
</annotation></semantics></math> where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\bar{x}_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mn>2</mn></msub><annotation encoding="application/x-tex">\bar{x}_2</annotation></semantics></math>
are the sample means,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_1^2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_2^2</annotation></semantics></math>
the sample variances, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>
the sample sizes in the two groups. The statistic follows a
<em>t</em>-distribution with degrees of freedom approximated by the
Welch-Satterthwaite equation:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î½</mi><mo>â‰ˆ</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>2</mn></msub></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mi>/</mi><msub><mi>n</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mi>/</mi><msub><mi>n</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>âˆ’</mo><mn>1</mn></mrow></mfrac></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\nu \approx \frac{
\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2
}{
\frac{(s_1^2 / n_1)^2}{n_1 - 1} + \frac{(s_2^2 / n_2)^2}{n_2 - 1}
}.
</annotation></semantics></math></p>
<p>The resulting p-value is computed from the <em>t</em>-distribution
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î½</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math>
degrees of freedom.</p>
<p>Welchâ€™s t-test remains valid and exhibits only minimal loss of power
even when the assumptions of Studentâ€™s t-test â€“ namely, normality and
equal variances of the response variable across groups â€“ are satisfied
<span class="citation">(<a href="#ref-Moser:1992">Moser and Stevens
1992</a>; <a href="#ref-Delacre:2017">Delacre, Lakens, and Leys
2017</a>)</span>. It is therefore the default implementation of the
t-test in R.</p>
</div>
<div class="section level4">
<h4 id="wilcoxon-rank-sum-test-wilcox-test">Wilcoxon rank-sum test (<code>wilcox.test()</code>)<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test-wilcox-test"></a>
</h4>
<p>The two-sample Wilcoxon rank-sum test (also known as the Mann-Whitney
test) is a non-parametric alternative that does not require the response
variable to be approximately normally distributed within each group. It
tests for a difference in location between two independent distributions
<span class="citation">(<a href="#ref-Mann:1947">Mann and Whitney
1947</a>)</span>. If the two groups have distributions that are
sufficiently similar in shape and scale, the Wilcoxon rank-sum test can
be interpreted as testing whether the medians of the two populations are
equal <span class="citation">(<a href="#ref-Hollander:2014">Hollander,
Chicken, and Wolfe 2014</a>)</span>.</p>
<p>The two-level factor variable <code>x</code> defines two groups, with
sample sizes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>.
All
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">N=n_1 + n_2</annotation></semantics></math>
observations are pooled and assigned ranks from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mn>1</mn></msub><annotation encoding="application/x-tex">W_1</annotation></semantics></math>
denote the sum of the ranks assigned to the group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>
corresponding to the first level of <code>x</code> containing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>
observations:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mn>1</mn></msub></munderover><mi>R</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
W_{1}= \sum_{i=1}^{n_1} R(x_{1,i})</annotation></semantics></math>,</p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(x_{1,i})</annotation></semantics></math>
is the rank of observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">x_{1,i}</annotation></semantics></math>
in the pooled sample.</p>
<p>The test statistic
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
returned by <code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code> is then computed as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><msub><mi>U</mi><mn>1</mn></msub><mo>=</mo><msub><mi>W</mi><mn>1</mn></msub><mo>âˆ’</mo><mfrac><mrow><msub><mi>n</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mn>2</mn></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
W =U_{1}=W_{1} - \frac{n_1(n_1 + 1)}{2}.
</annotation></semantics></math> It corresponds to the Mann-Whitney
<span class="citation">(<a href="#ref-Mann:1947">Mann and Whitney
1947</a>)</span>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>
statistic of the first group.</p>
<p>If both groups contain fewer than 50 observations and the data
contain no ties, the <em>p</em>-value is computed exactly. Otherwise, a
normal approximation with continuity correction is used.</p>
</div>
<div class="section level4">
<h4 id="graphical-output">Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output"></a>
</h4>
<!-- `visstat()` selects between Welch's t-test and the Wilcoxon rank-sum test
as follows. If both groups contain more than 30 observations, Welch's t-test is
always applied, relying on the central limit theorem to justify its application
regardless of underlying normality [@Rasch:2011; @Lumley:2002]. -->
<!-- If either group contains fewer than 30 observations, the Shapiro--Wilk test
(`shapiro.test()`) is applied separately to each group. Welch's t-test is used
if both tests do not reject normality at the significance level $\alpha$;
otherwise, the Wilcoxon rank-sum test is applied. -->
<p>Both tests show two plot panes, the first for checking the assumption
of normality in both groups, the second the actual data with the chosen
test.</p>
<p>Welchâ€™s t-test does not fit a regression model, so there are no
â€œresidualsâ€ in the regression sense. Therefore the function
<code>vis_ttest_assumptions()</code>generates histograms overlaid with
the normal distribution for both groups acompanied by the p-values of
<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code> and <code>ad.tset()</code>.</p>
<p>The graphical output of the second pane consists of box plots
overlaid with jittered points to display individual observations. When
Welchâ€™s t-test is applied, the function includes confidence intervals
based on the user-specified <code>conf.level</code>.</p>
<!-- The title is structured as follows: -->
<!-- -   First line: Test name and chosen significance level $\alpha$. -->
<!-- -   Second line: Null hypotheses automatically adapted based on the user-specified response and grouping variable. -->
<!-- -   Third line: Test statistic, p-value and automated comparison with $\alpha$ -->
<p>The function returns a list containing the results of the applied
test and the summary statistics used to construct the plot.</p>
</div>
<div class="section level4">
<h4 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="welchs-t-test">Welchâ€™s t-test<a class="anchor" aria-label="anchor" href="#welchs-t-test"></a>
</h5>
<p>The <em>Motor Trend Car Road Tests</em> dataset (<code>mtcars</code>)
contains 32 observations, where <code>mpg</code> denotes miles per (US)
gallon, and <code>am</code> represents the transmission type
(<code>0</code> = automatic, <code>1</code> = manual).</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span><span class="op">)</span></span>
<span><span class="va">t_test_statistics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span>, <span class="va">mtcars</span><span class="op">$</span><span class="va">mpg</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-1-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-1-2.png" width="700"></p>
<p>Increasing the confidence level <code>conf.level</code> from the
default 0.95 to 0.99 results in wider confidence intervals, as a higher
confidence level requires more conservative bounds to ensure that the
interval includes the true parameter value with greater certainty.</p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="wilcoxon-rank-sum-test">Wilcoxon rank sum test<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test"></a>
</h5>
<p>The Wilcoxon rank sum test is exemplified on differences between the
central tendencies of grades of â€œboysâ€ and â€œgirlsâ€ in a class:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">grades_gender</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  sex <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="st">"girl"</span>, <span class="fl">21</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="st">"boy"</span>, <span class="fl">23</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  grade <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="fl">19.3</span>, <span class="fl">18.1</span>, <span class="fl">15.2</span>, <span class="fl">18.3</span>, <span class="fl">7.9</span>, <span class="fl">6.2</span>, <span class="fl">19.4</span>,</span>
<span>    <span class="fl">20.3</span>, <span class="fl">9.3</span>, <span class="fl">11.3</span>, <span class="fl">18.2</span>, <span class="fl">17.5</span>, <span class="fl">10.2</span>, <span class="fl">20.1</span>, <span class="fl">13.3</span>, <span class="fl">17.2</span>, <span class="fl">15.1</span>, <span class="fl">16.2</span>, <span class="fl">17.0</span>,</span>
<span>    <span class="fl">16.5</span>, <span class="fl">5.1</span>, <span class="fl">15.3</span>, <span class="fl">17.1</span>, <span class="fl">14.8</span>, <span class="fl">15.4</span>, <span class="fl">14.4</span>, <span class="fl">7.5</span>, <span class="fl">15.5</span>, <span class="fl">6.0</span>, <span class="fl">17.4</span>,</span>
<span>    <span class="fl">7.3</span>, <span class="fl">14.3</span>, <span class="fl">13.5</span>, <span class="fl">8.0</span>, <span class="fl">19.5</span>, <span class="fl">13.4</span>, <span class="fl">17.9</span>, <span class="fl">17.7</span>, <span class="fl">16.4</span>, <span class="fl">15.6</span>, <span class="fl">17.3</span>, <span class="fl">19.9</span>, <span class="fl">4.4</span>, <span class="fl">2.1</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">wilcoxon_statistics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">grades_gender</span><span class="op">$</span><span class="va">sex</span>, <span class="va">grades_gender</span><span class="op">$</span><span class="va">grade</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-3-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-3-2.png" width="700"></p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="categorical-predictor-with-more-than-two-levels">Categorical predictor with more than two levels<a class="anchor" aria-label="anchor" href="#categorical-predictor-with-more-than-two-levels"></a>
</h3>
<p>If the predictor is of <code>class</code> â€œ<code>factor</code>â€ with
<strong>more than two levels</strong> and the response is of
<code>mode</code> â€œ<code>numeric</code>â€, <code><a href="../reference/visstat.html">visstat()</a></code> either
performs Fisherâ€™s one- way ANOVA <span class="citation">(<a href="#ref-Fisher:1935">Roland A. Fisher 1971</a>)</span>
(<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>), Welchâ€™s heteroscedastic one-way ANOVA <span class="citation">(<a href="#ref-Welch:1951">Welch 1951</a>)</span>
(<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) or, as a non-parametric alternative, the
Kruskal -Wallis test <span class="citation">(<a href="#ref-Kruskal:1952">Kruskal and Wallis 1952</a>)</span>
(<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>).</p>
<p>In the remainder of this section, we briefly introduce the tests
themselves, the assumption checks, and the post-hoc procedures, and
illustrate each test with an example.</p>
<div class="section level4">
<h4 id="fishers-one-way-anova-aov">Fisherâ€™s one-way ANOVA (<code>aov()</code>)<a class="anchor" aria-label="anchor" href="#fishers-one-way-anova-aov"></a>
</h4>
<p>Fisherâ€™s one-way ANOVA (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) tests the null hypothesis
that the means of multiple groups are equal. It assumes independent
observations, normally distributed residuals, and
<strong>homogeneous</strong> variances across groups. The test statistic
is the ratio of the variance explained by differences among group means
(between-group variance) to the unexplained variance within groups <span class="citation">(<a href="#ref-Fisher:1990">Ronald A. Fisher and Yates
1990</a>)</span></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mfrac><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>âˆ’</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><mo>=</mo><mfrac><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo>âˆ’</mo><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow></mfrac><mfrac><mrow><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>âˆ’</mo><mi>k</mi></mrow></mfrac></mfrac></mrow><annotation encoding="application/x-tex">F  = \frac{MS_{between}}{MS_{within}}=
\frac{SS_{between}/(k-1)}{SS_{within}/(N-k)}== \frac{\frac{\sum_{i=1}^{k} n_i (\bar{x}_i - \bar{x})^2}{k - 1}}
{\frac{\sum_{i=1}^{k}\sum_{j=1}^{n_i}(x_{ij}-\bar{x}_i)^2}{N - k}}</annotation></semantics></math></p>
<p>where: - where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">MS_{between}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">MS_{within}</annotation></semantics></math>
are the mean square between groups and mean Square within groups
respectively. -
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{between}</annotation></semantics></math>
= Sum of Squares between groups (variance due to group differences) -
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{within}</annotation></semantics></math>
= Sum of Squares within groups (error variance)<br>
-
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
= number of groups -
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
= total sample size</p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\bar{x}_i</annotation></semantics></math>
is the mean of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">â€¾</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math>
is the overall mean,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">x_{ij}</annotation></semantics></math>
is the observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the total number of observations.</p>
<p>Under the null hypothesis, this statistic follows an F-distribution
with two parameters for degrees of freedom:
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>)
and
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>âˆ’</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">N - k</annotation></semantics></math>):
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>âˆ¼</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>âˆ’</mo><mn>1</mn><mo>,</mo><mi>N</mi><mo>âˆ’</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F \sim F(k-1, N-k)</annotation></semantics></math>
The resulting p-value is computed from this distribution.</p>
</div>
<div class="section level4">
<h4 id="welchs-heteroscedastic-one-way-anova-oneway-test">Welchâ€™s heteroscedastic one-way ANOVA
(<code>oneway.test()</code>)<a class="anchor" aria-label="anchor" href="#welchs-heteroscedastic-one-way-anova-oneway-test"></a>
</h4>
<p>When only the assumptions of independent observations and normally
distributed residuals are met, but <em>homogeneous variances</em> across
groups <em>cannot be assumed</em> , Welchâ€™s heteroscedastic one-way
ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) <span class="citation">(<a href="#ref-Welch:1951">Welch 1951</a>)</span> provides an alternative to
<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>. It compares group means using weights based on
sample sizes and variances. The degrees of freedom are adjusted using a
Satterthwaite-type approximation <span class="citation">(<a href="#ref-Satterthwaite:1946">Satterthwaite 1946</a>)</span>, resulting
in an F-statistic with non-integer degrees of freedom.</p>
</div>
<div class="section level4">
<h4 id="kruskalwallis-test-kruskal-test">Kruskalâ€“Wallis test (<code>kruskal.test()</code>)<a class="anchor" aria-label="anchor" href="#kruskalwallis-test-kruskal-test"></a>
</h4>
<p>When the assumption of normality is not met, the Kruskalâ€“Wallis test
provides a non-parametric alternative. It compares group distributions
based on ranked values and tests the null hypothesis that the groups
come from the same population â€” specifically, that the distributions
have the same location <span class="citation">(<a href="#ref-Kruskal:1952">Kruskal and Wallis 1952</a>)</span>. If the
group distributions are sufficiently similar in shape and scale, then
the Kruskalâ€“Wallis test can be interpreted as testing for equality of
medians across groups <span class="citation">(<a href="#ref-Hollander:2014">Hollander, Chicken, and Wolfe
2014</a>)</span>.</p>
<p>The test statistic is defined as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mfrac><mn>12</mn><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>R</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><mo>âˆ’</mo><mover><mi>R</mi><mo accent="true">â€¾</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
H = \frac{12}{N(N+1)} \sum_{i=1}^{k} n_i \left(\bar{R}_i - \bar{R} \right)^2,
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>R</mi><mo accent="true">â€¾</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\bar{R}_i</annotation></semantics></math>
is the average rank of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the total sample size, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>R</mi><mo accent="true">â€¾</mo></mover><mo>=</mo><mfrac><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\bar{R} = \frac{N+1}{2}</annotation></semantics></math>
is the average of all ranks. Under the null hypothesis,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>
approximately follows a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
distribution with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>âˆ’</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>
degrees of freedom.</p>
</div>
</div>
<div class="section level3">
<h3 id="testing-the-assumptions-vis_anova_assumptions">Testing the assumptions (<code>vis_anova_assumptions()</code>)<a class="anchor" aria-label="anchor" href="#testing-the-assumptions-vis_anova_assumptions"></a>
</h3>
<p>The test logic for <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>
follows from their respective assumptions. <code><a href="../reference/visstat.html">visstat()</a></code>
initially models the data using <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and analyses the
residuals.</p>
<p>If both of the following conditions are met: (1) the standardised
residuals follow the standard normal distribution, and (2) the residuals
exhibit homoscedasticity (equal variances across groups), then the test
statistic from <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> is returned.</p>
<p>If only the normality assumption is satisfied, <code><a href="../reference/visstat.html">visstat()</a></code>
applies <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>. If the normality assumption is
violated, <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code> is used instead.</p>
<p>These assumptions are tested using the
<code><a href="../reference/vis_anova_assumptions.html">vis_anova_assumptions()</a></code> function.</p>
</div>
<div class="section level3">
<h3 id="controlling-the-family-wise-error-rate">Controlling the family-wise error rate<a class="anchor" aria-label="anchor" href="#controlling-the-family-wise-error-rate"></a>
</h3>
<p>ANOVA is an omnibus test that evaluates a single null hypothesis: all
group means are equal. If the null hypothesis gets rejected, we would
like to identify which specific groups differ significantly from each
other. However, simple pairwise comparisons of group means following an
ANOVA increases the probability of incorrectly declaring a significant
difference when, in fact, there is none.</p>
<p>This error is quantified by the family-wise error rate â€œalpha per
family of testsâ€
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Î±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PF}</annotation></semantics></math>,
which refers to the probability of making at least one Type I error,
that is, falsely rejecting the null hypothesis across all pairwise
comparisons.</p>
<p>Given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
levels of the categorical variable, there are</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mfrac><mrow><mi>n</mi><mo>â‹…</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>âˆ’</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">
M = \frac{n \cdot (n - 1)}{2}
</annotation></semantics></math></p>
<p>pairwise comparisons possible, defining a <em>family of tests</em>
<span class="citation">(<a href="#ref-Abdi:2007">Abdi 2007</a>)</span>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
corresponds to the number of null hypotheses in the post hoc tests, each
testing whether the means of two specific groups are equal.</p>
</div>
<div class="section level3">
<h3 id="post-hoc-analysis">Post-hoc analysis<a class="anchor" aria-label="anchor" href="#post-hoc-analysis"></a>
</h3>
<div class="section level4">
<h4 class="unnumbered" id="Å¡idÃ¡k-correction">Å idÃ¡k correction<a class="anchor" aria-label="anchor" href="#%C5%A1id%C3%A1k-correction"></a>
</h4>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PT}</annotation></semantics></math>
(â€œalpha per testâ€) is the probability of making a Type I error in one
comparison, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>âˆ’</mo><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">1 - \alpha_{PT}</annotation></semantics></math>
is the probability of not making a Type I error in one comparison.</p>
<p>If all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
comparisons are <strong>independent</strong> of each other, the
probability of making no Type I error across the entire family of
pairwise comparisons is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>M</mi></msup><annotation encoding="application/x-tex">(1 -
\alpha_{PT})^M</annotation></semantics></math>. The family-wise error
rate is then given by its complement <span class="citation">(<a href="#ref-Abdi:2007">Abdi 2007</a>)</span>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>âˆ’</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>M</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\alpha_{PF} = 1 - (1 - \alpha_{PT})^M.
</annotation></semantics></math></p>
<!-- Let us illustrate the inflation of the family-wise error rate with increasing group number $n$ (equal to the number of `levels` in the categorical variable)) by the following examples: With $\alpha_{PT}=0.05$ pairwise comparison of  $n=3$ groups results in a family-wise error rate of $\alpha_{PF} \approx 14\%$, whereas pairwise comparing of $n=6$ groups (as in the examples below) already leads to the probability of at least one time falsely rejecting the null hypothesis of $\alpha_{PF} \approx 54\%$. -->
<p>Solving the last equation defining
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Î±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PF}</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PT}</annotation></semantics></math>
yields the Å idÃ¡k equation <span class="citation">(<a href="#ref-Sidak:1967">Å idÃ¡k 1967</a>)</span>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>âˆ’</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>1</mn><mi>/</mi><mi>M</mi></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\alpha_{PT}=1-(1-{\alpha_{PF}})^{1/M}.
</annotation></semantics></math> This shows that, in order to achieve a
given family-wise error rate, the corresponding per-test significance
level must be reduced when there are more than two groups.
<!-- ###### Å idÃ¡k  correction in `visstat()` {.unnumbered} --></p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> sets
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Î±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PF}</annotation></semantics></math>
to the user-defined
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î±</mi><mo>=</mo><mn>1</mn><mo>âˆ’</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>conf.level</code>, resulting in</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>âˆ’</mo><msup><mtext mathvariant="monospace">ğšŒğš˜ğš—ğš.ğš•ğšğšŸğšğš•</mtext><mrow><mn>1</mn><mi>/</mi><mi>M</mi></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 1 - \texttt{conf.level}^{1 / M}.
</annotation></semantics></math></p>
<p>With the default setting <code>conf.level = 0.95</code>, this leads
to:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n = 3</annotation></semantics></math>
groups:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>1.70</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 1.70\%</annotation></semantics></math>
</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">n = 6</annotation></semantics></math>
groups:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>0.34</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 0.34\%</annotation></semantics></math>
</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">n = 10</annotation></semantics></math>
groups:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>0.11</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 0.11\%</annotation></semantics></math>
</li>
</ul>
<p>These examples illustrate that the Å idÃ¡k approach becomes
increasingly conservative as the number of comparisons grows. Moreover,
since the method assumes independence among tests, it may be overly
conservative when this assumption is violated.</p>
</div>
<div class="section level4">
<h4 class="unnumbered" id="post-hoc-test-following-an-anova-tukeyhsd">Post-hoc test following an ANOVA:
<code>TukeyHSD()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-an-anova-tukeyhsd"></a>
</h4>
<p>In contrast to the general-purpose Å idÃ¡k correction, Tukeyâ€™s Honestly
Significant Differences procedure (<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>) is
specifically designed for pairwise mean comparisons following an ANOVA
(either <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> or <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>). It controls
the family-wise error rate using a critical value from the studentised
range distribution, which properly accounts for the correlated nature of
pairwise comparisons sharing a common residual variance <span class="citation">(<a href="#ref-Hochberg:1987">Hochberg and Tamhane
1987</a>)</span>.</p>
<p>Based on the user-specified confidence level
(<code>conf.level</code>), <code><a href="../reference/visstat.html">visstat()</a></code> constructs confidence
intervals for all pairwise <strong>differences</strong> between factor
level means. A significant difference between two means is indicated
when the corresponding confidence interval does not include zero.
<code><a href="../reference/visstat.html">visstat()</a></code> returns both the HSD-adjusted p-values and the
associated confidence intervals for all pairwise comparisons.</p>
<p>For graphical display, <code><a href="../reference/visstat.html">visstat()</a></code> uses a dual approach:
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> provides the statistical test results and
significance determinations, while Å idÃ¡k-corrected confidence intervals
around individual group means are shown for visualisation purposes. This
separation allows for optimal statistical testing while maintaining
clear, interpretable graphics.</p>
</div>
<div class="section level4">
<h4 class="unnumbered" id="post-hoc-test-following-the-kruskalwallis-rank-sum-test-pairwise-wilcox-test">Post-hoc test following the Kruskalâ€“Wallis rank
sum test: <code>pairwise.wilcox.test()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-the-kruskalwallis-rank-sum-test-pairwise-wilcox-test"></a>
</h4>
<p>As a post-hoc analysis following the Kruskalâ€“Wallis test,
<code><a href="../reference/visstat.html">visstat()</a></code> applies the pairwise Wilcoxon rank sum test using
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> to compare each pair of factor
levels (see section â€œWilcoxon rank-sum test
(<code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>)â€).</p>
<p>The resulting p-values from all pairwise comparisons are then
adjusted for multiple testing using Holmâ€™s method <span class="citation">(<a href="#ref-Holm:1979">Holm 1979</a>)</span>: The
p-values are first sorted from smallest to largest and tested against
thresholds that become less strict as their rank increases. This
stepwise adjustment does not assume independence among tests and is
typically less conservative than the Å idÃ¡k method, while still ensuring
strong control of the family-wise error rate.</p>
</div>
</div>
<div class="section level3">
<h3 id="graphical-output-1">Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output-1"></a>
</h3>
<p>The graphical output for all tests based on a numeric response and a
categorical predictor with more than two levels consists of two panels:
the first focuses on the residual analysis, the second on the actual
test chosen by the decision logic.</p>
<p>The residual panel addresses the assumption of normality, both
graphically and through formal tests. It displays a scatter plot of the
standardised residuals versus the predicted values, as well as a normal
Qâ€“Q plot comparing the sample quantiles to the theoretical quantiles. If
the residuals are normally distributed, no more than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math>
of the standardised residuals should exceed approximately
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mn>2</mn><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|2|</annotation></semantics></math>;
in the Qâ€“Q plot the data points should approximately follow the red
straight line.</p>
<p>The p-values of the formal tests for normality (Shapiroâ€“Wilk and
Andersonâ€“Darling) as well as the tests for homoscedasticity (Bartlettâ€™s
and Levene Brownâ€“Forsythe) are given in the title.
<!-- To assume normality of residuals, the formal tests for normality (Shapiro--Wilk and Anderson--Darling) should result in p-values greater than the user-defined $\alpha$. --></p>
<!-- If normality of the residuals can beassumed, Bartlett's test can be assesed to check for homogenity of variances. Otherwise, the Levene Brownâ€“Forsythe is a more robust test for homoscedasticity given deviations from the normality assumption of residuals  The p-values of the beforementioned tests are displayed in the title of the residual panel. -->
<p><code><a href="../reference/visstat.html">visstat()</a></code> then illustrates, in the subsequent graph,
either the <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>, the <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>,
or <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> result (see also Section â€œDecision logicâ€).</p>
<p>If neither normality of the residuals nor homogeneity of variances is
given, the <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code> is executed. The result is
illustrated using box plots alongside jittered data points, with the
title displaying the p-value from <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>.</p>
<p>Above each box plot, the number of observations per level is shown.
Different green letters below a pair of box plots indicate that the two
groups are considered significantly different based on Holmâ€™s-adjusted
pairwise Wilcoxon rank sum test p-values smaller than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
The letters are generated with the help of the function
<code>multcompLetters()</code> from the <code>multcompView</code>
package <span class="citation">(<a href="#ref-Graves:2024">Graves,
Piepho, and with help from Sundar Dorai-Raj 2024</a>)</span>).</p>
<p>If normality of the residuals can be assumed, a parametric test is
chosen: either <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>, if homoscedasticity is also assumed,
or <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> otherwise. <code><a href="../reference/visstat.html">visstat()</a></code> displays
the name of the test and the corresponding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>
and p-value in the title.</p>
<p>The graph shows both the <code>conf.level</code>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>â‹…</mo><mspace width="0.167em"></mspace><mn>100</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\cdot\,100\%</annotation></semantics></math>
confidence intervals corresponding to the null hypothesis of ANOVA (all
group means are equal) and the Å idÃ¡k-corrected
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>âˆ’</mo><msub><mi>Î±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>â‹…</mo><mn>100</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">(1 - \alpha_{PT}) \cdot 100\%</annotation></semantics></math>
confidence intervals used in the post hoc analysis.</p>
<p>In <code><a href="../reference/visstat.html">visstat()</a></code>, the Å idÃ¡k intervals are used only for
visualisation, as the underlying method assumes independent comparisons
and can become overly conservative when this assumption is violated.</p>
<p>The actual post hoc analysis should be based on
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>. A significant test result between two groups is
graphically represented by different green letters below a pair of group
means.</p>
<p>Besides the graphical output, <code><a href="../reference/visstat.html">visstat()</a></code> returns a list
containing the relevant test statistics along with the corresponding
post-hoc-adjusted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
for all pairwise comparisons.</p>
<div class="section level4">
<h4 id="examples-1">Examples<a class="anchor" aria-label="anchor" href="#examples-1"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="anova">ANOVA<a class="anchor" aria-label="anchor" href="#anova"></a>
</h5>
<p>The <code>npk</code> dataset reports the yield of peas (in pounds per
block) from an agricultural experiment conducted on six blocks. In this
experiment, the application of three different fertilisers â€“ nitrogen
(N), phosphate (P), and potassium (K) â€“ was varied systematically. Each
block received either none, one, two, or all three of the
fertilisers,</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">anova_npk</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">npk</span><span class="op">$</span><span class="va">block</span>,<span class="va">npk</span><span class="op">$</span><span class="va">yield</span>,conf.level<span class="op">=</span><span class="fl">0.90</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-4-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-4-2.png" width="700"></p>
<p>Normality of residuals is supported by graphical diagnostics
(histogram, scatter plot of standardised residuals, Q-Q plot) and formal
tests (Shapiroâ€“Wilk and Anderson- Darling, both with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>Î±</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>).homogeneity
of variances is not supported at the given confidence level by
<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>, but by <code>levene.test()</code>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>Î±</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>).
The decision logic is based on <code>levene.test()</code> and triggers
<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>. Post-hoc analysis with <code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> shows
no significant yield differences between blocks, as all share the same
group label (e.g., all green letters).</p>
<!-- {.unnumbered} -->
<!-- The `InsectSprays` dataset reports insect counts from agricultural experimental units treated with six different insecticides. To stabilise the variance in counts, we apply a square root transformation to the response variable. -->
<!-- ```{r} -->
<!-- insect_sprays_tr <- InsectSprays -->
<!-- insect_sprays_tr$count_sqrt <- sqrt(InsectSprays$count) -->
<!-- test_statistic_anova=visstat(insect_sprays_tr$spray, insect_sprays_tr$count_sqrt) -->
<!-- # test_statistic_anova  -->
<!-- ``` -->
<!-- After the transformation, the homogeneity of variances can be assumed ($p> -->
<!-- \alpha$ as calculated with the `bartlett.test()`), and the test statistic and p-value of Fisher's one-way ANOVA`aov()` is displayed. -->
<!-- <!-- The `ToothGrowth` data set studies the effect of vitamin C dosage C (0.5, 1, and 2 mg/day) on the the tooth growth of 60 guinea pigs. -->
<!-- <!-- ```{r} -->
<!-- <!-- visstat(ToothGrowth$dose, ToothGrowth$len) -->
<!-- <!-- ``` -->
<!-- <!-- Again, the homogeneity of variances can be assumed ($p> -->
<!-- <!-- \alpha$ as calculated with the `bartlett.test()`) and the test statistic and p-value of Fisher's one-way ANOVA`aov()` is displayed. -->
</div>
<div class="section level5">
<h5 class="unnumbered" id="kruskalwallis-rank-sum-test">Kruskalâ€“Wallis rank sum test<a class="anchor" aria-label="anchor" href="#kruskalwallis-rank-sum-test"></a>
</h5>
<p>The <code>iris</code> dataset contains petal width measurements (in
cm) for three different iris species.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span>, <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-5-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-5-2.png" width="700"></p>
<p>In this example, scatter plots of the standardised residuals and the
Q-Q plot suggest that the residuals are not normally distributed. This
is confirmed by very small p-values from both the Shapiroâ€“Wilk and
Anderson-Darling tests.</p>
<p>If both p-values are below the significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Î±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
<code><a href="../reference/visstat.html">visstat()</a></code> switches to the non-parametric
<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>. Post-hoc analysis using
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> shows significant differences in
petal width between all three species, as indicated by distinct group
labels (all green letters differ).</p>
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="numeric-response-and-numeric-predictor-simple-linear-regression-1">Numeric response and numeric predictor: Simple linear
regression<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor-simple-linear-regression-1"></a>
</h2>
<div class="section level3">
<h3 id="simple-linear-regression-lm">Simple linear regression (<code>lm()</code>)<a class="anchor" aria-label="anchor" href="#simple-linear-regression-lm"></a>
</h3>
<p>If both the predictor and the response are numeric and contain only
one level each, <code><a href="../reference/visstat.html">visstat()</a></code> performs a simple linear
regression.</p>
<p>The resulting regression plot displays the point estimate of the
regression line</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>â‹…</mo><mi>x</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
y = a + b \cdot x,
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the response variable,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is the predictor variable,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>a</mi><annotation encoding="application/x-tex">a</annotation></semantics></math>
is the intercept, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>b</mi><annotation encoding="application/x-tex">b</annotation></semantics></math>
is the slope of the regression line.</p>
<div class="section level4">
<h4 id="residual-analysis">Residual analysis<a class="anchor" aria-label="anchor" href="#residual-analysis"></a>
</h4>
<p><code><a href="../reference/visstat.html">visstat()</a></code> checks the normality of the standardised
residuals from <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> both with diagnostic plots and using
the Shapiroâ€“Wilk and Anderson-Darling tests. (via
<code>visAnovassumptions()</code>)</p>
<!-- If the p-values for the null hypothesis of normally distributed residuals from both tests are smaller than $1 -$`conf.int`, the title of the residual plot will display the message: "Requirement of normally distributed residuals not met". -->
<p>Note, that regardless of the result of the residual analysis,
<code><a href="../reference/visstat.html">visstat()</a></code> proceeds to perform the regression. The title of
the graphical output indicates the chosen confidence level
(<code>conf.level</code>), the estimated regression parameters with
their confidence intervals and p-values, and the adjusted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math>.
The plot displays the raw data, the fitted regression line, and both the
confidence and prediction bands corresponding to the specified
<code>conf.level</code>.</p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> returns a list containing the regression test
statistics, the p-values from the normality tests of the standardised
residuals, and the pointwise estimates of the confidence and prediction
bands.</p>
</div>
<div class="section level4">
<h4 id="examples-2">Examples<a class="anchor" aria-label="anchor" href="#examples-2"></a>
</h4>
<!-- The `women` reports the  average height in inches and weights in pounds (lbs) for American women aged 30â€“39.  -->
<!-- ```{r} -->
<!-- linreg_women <- visstat(women$height, women$weight) -->
<!-- ``` -->
<div class="section level5">
<h5 class="unnumbered" id="dataset-trees">dataset: `trees``<a class="anchor" aria-label="anchor" href="#dataset-trees"></a>
</h5>
<p>The <code>trees</code> data set contains the diameter
<code>Girth</code> in inches and <code>Volume</code> in cubic ft of 31
black cherry trees.</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linreg_trees</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">trees</span><span class="op">$</span><span class="va">Girth</span>, <span class="va">trees</span><span class="op">$</span><span class="va">Volume</span>,conf.level<span class="op">=</span><span class="fl">0.9</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-6-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-6-2.png" width="700"></p>
<p>p-values greater than <code>conf.level</code> in both the
Anderson-Darling normality test and the Shapiroâ€“Wilk test of the
standardised residuals indicate that the normality assumption of the
residuals underlying the linear regression is met.</p>
<p>Increasing the confidence level <code>conf.level</code> from the
default 0.9 to 0.99 results in wider confidence intervals of the
regression parameters as well as wider confidence and prediction
bands</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linreg_trees_99</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">trees</span><span class="op">$</span><span class="va">Girth</span>, <span class="va">trees</span><span class="op">$</span><span class="va">Volume</span>,conf.level <span class="op">=</span> <span class="fl">0.99</span><span class="op">)</span></span></code></pre></div>
<p>The <code>visStatistics</code> plot allows to display only the second
generated plot (the assumption plot is unchanged):</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">linreg_trees_99</span>,which<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-8-1.png" width="700"></p>
<!-- #### dataset: `cars`{.unnumbered} -->
<!-- The `cars` dataset reports the speed of 50 cars in miles per hour (`speed`) and the stopping distance in feet (`dist`) recorded in  the 1920s. In this example the assuptions of normally distributed residuals is not met. -->
<!-- ```{r} -->
<!-- linreg_cars <- visstat(cars$speed, cars$dist) -->
<!-- ``` -->
</div>
</div>
</div>
</div>
<div class="section level2">
<h2 id="both-variables-categorical-comparing-proportions-1">Both variables categorical: Comparing proportions<a class="anchor" aria-label="anchor" href="#both-variables-categorical-comparing-proportions-1"></a>
</h2>
<p>When both variables are categorical (i.e., of class
<code>factor</code>), <code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis
that the two variables are independent. Observed frequencies are
typically arranged in a contingency table, where rows index the levels
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the response variable and columns index the levels
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
of the predictor variable.</p>
<div class="section level3">
<h3 id="pearsons-residuals-and-mosaic-plots">Pearsonâ€™s residuals and mosaic plots<a class="anchor" aria-label="anchor" href="#pearsons-residuals-and-mosaic-plots"></a>
</h3>
<p>Mosaic plots provide a graphical representation of contingency
tables, where the area of each tile is proportional to the observed cell
frequency. To aid interpretation, tiles are coloured based on Pearson
residuals from a chi-squared test of independence. These residuals
measure the standardised deviation of observed from expected counts
under the null hypothesis of independence.</p>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">O_{ij}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">E_{ij}</annotation></semantics></math>
denote the observed and expected frequencies in row
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and column
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
of an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>Ã—</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">R \times C</annotation></semantics></math>
contingency table. The Pearson residual for each cell is defined as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><msqrt><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msqrt></mfrac><mo>,</mo><mspace width="1.0em"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>â€¦</mi><mo>,</mo><mi>R</mi><mo>,</mo><mspace width="1.0em"></mspace><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>â€¦</mi><mo>,</mo><mi>C</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}}}, \quad i = 1, \ldots, R,\quad
j = 1, \ldots, C.
</annotation></semantics></math> Positive residuals (shaded in blue)
indicate observed counts greater than expected, while negative values
suggest under-representation (shaded in red). Colour shading thus
highlights which combinations of categorical levels contribute most to
the overall association.</p>
</div>
<div class="section level3">
<h3 id="pearsons-chi2-test-chisq-test">Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-test
(<code>chisq.test()</code>)<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-chisq-test"></a>
</h3>
<p>The test statistic of Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-test
<span class="citation">(<a href="#ref-Pearson:1900">Pearson
1900</a>)</span> is the sum of squared Pearson residuals:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>Ï‡</mi><mn>2</mn></msup><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><msubsup><mi>r</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\chi^2 = \sum_{i=1}^{R} \sum_{j=1}^{C} r_{ij}^2 =
\sum_{i=1}^{R} \sum_{j=1}^{C} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}.
</annotation></semantics></math></p>
<p>The test statistic is compared to the chi-squared distribution with $
(R - 1)(C - 1)$ degrees of freedom. The resulting p-value corresponds to
the upper tail probability â€” that is, the probability of observing a
value greater than or equal to the test statistic under the null
hypothesis.</p>
<div class="section level4">
<h4 id="pearsons-chi2-test-with-yates-continuity-correction">Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
test with Yatesâ€™ continuity correction<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-with-yates-continuity-correction"></a>
</h4>
<p>Yatesâ€™ correction is applied to the Pearson
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
statistic in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>Ã—</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math>
contingency tables (with one degree of freedom). In this case, the
approximation of the discrete sampling distribution by the continuous
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
distribution tends to overestimate the significance level of the test.
To correct for this, Yates proposed subtracting 0.5 from each absolute
difference between observed and expected counts <span class="citation">(<a href="#ref-Yates:1934">Yates 1934</a>)</span>,
resulting in a smaller test statistic:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>Ï‡</mi><mtext mathvariant="normal">Yates</mtext><mn>2</mn></msubsup><mo>=</mo><munderover><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><munderover><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>âˆ’</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>âˆ’</mo><mn>0.5</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\chi^2_{\text{Yates}} = \sum_{i=1}^{2} \sum_{j=1}^{2}
\frac{(|O_{ij} - E_{ij}| - 0.5)^2}{E_{ij}}.
</annotation></semantics></math></p>
<p>This reduced test statistic yields a larger p-value, thereby lowering
the risk of a Type I error.</p>
<p>Yatesâ€™ continuity correction is applied by default by the underlying
routine <code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>. ## Fisherâ€™s exact test
(<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>)</p>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
approximation is considered reliable only if no expected cell count is
less than 1 and no more than 20 percent of cells have expected counts
below 5 <span class="citation">(<a href="#ref-Cochran:1954">Cochran
1954</a>)</span>). If this condition is not met, Fisherâ€™s exact test
<span class="citation">(<a href="#ref-Fisher:1970">Ronald Aylmer Fisher
1970</a>)</span> (<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>) is applied instead, as it
is a non-parametric method that does not rely on large-sample
approximations. The test calculates an exact p-value for testing
independence by conditioning on the observed margins: the row totals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_i = \sum_{j=1}^C O_{ij}</annotation></semantics></math>
and the column totals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>=</mo><msubsup><mo>âˆ‘</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></msubsup><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_j = \sum_{i=1}^R O_{ij}</annotation></semantics></math>,
defining the structure of the contingency table.</p>
<p>In the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>Ã—</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math>
case, the observed table can be written as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>C</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>C</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">Row sums</mtext></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi><mo>+</mo><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>c</mi><mo>+</mo><mi>d</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">Column sums</mtext></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi><mo>+</mo><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi><mo>+</mo><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>n</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{c|cc|c}
&amp; C_1 &amp; C_2 &amp; \text{Row sums} \\\\
\hline
R_1 &amp; a &amp; b &amp; a + b \\\\
R_2 &amp; c &amp; d &amp; c + d \\\\
\hline
\text{Column sums} &amp; a + c &amp; b + d &amp; n
\end{array}
</annotation></semantics></math></p>
<p>Let
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
O = \begin{bmatrix} a &amp; b \\\\ c &amp; d \end{bmatrix}
</annotation></semantics></math> denote the above observed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>Ã—</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math>
contingency table. The exact probability of observing this table under
the null hypothesis of independence, given the fixed margins, is given
by the hypergeometric probability mass function (PMF)</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>â„™</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>O</mi><mo>âˆ£</mo><msub><mi>R</mi><mn>1</mn></msub><mo>,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>,</mo><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><msub><mi>C</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mi>a</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mi>c</mi><mo>+</mo><mi>d</mi></mrow><mi>c</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>n</mi><mrow><mi>a</mi><mo>+</mo><mi>c</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathbb{P}(O \mid
R_1, R_2, C_1, C_2) =
\frac{\binom{a + b}{a} \binom{c + d}{c}}{\binom{n}{a + c}},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi><mo>+</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n = a + b + c + d</annotation></semantics></math>
is the total sample size.</p>
<!-- Because all other cell values are determined by $a$ and the fixed margins,
-->
<!-- this is often written more simply as: -->
<!-- $$ -->
<!-- P(a \mid \text{margins}) = -->
<!-- \frac{\binom{a + b}{a} \binom{c + d}{c}}{\binom{n}{a + c}}. -->
<!-- $$ -->
<p>The p-value is computed by summing the probabilities of all tables
with the same margins whose probabilities under the null are less than
or equal to that of the observed table.</p>
<p>For general
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>Ã—</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">R \times C</annotation></semantics></math>
tables, <code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code> generalises this approach using the
multivariate hypergeometric distribution.</p>
</div>
</div>
<div class="section level3">
<h3 id="test-choice-and-graphical-output">Test choice and graphical output<a class="anchor" aria-label="anchor" href="#test-choice-and-graphical-output"></a>
</h3>
<p>If the expected frequencies are sufficiently large - specifically, if
at least 80% of the cells have expected counts greater than 5 and no
expected count is smaller than 1, the function uses Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
(<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>).</p>
<p>Otherwise, it switches to Fisherâ€™s exact test
(<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>) <span class="citation">(<a href="#ref-Cochran:1954">Cochran 1954</a>)</span>.</p>
<p>For 2-by-2 contingency tables, Yatesâ€™ continuity correction <span class="citation">(<a href="#ref-Yates:1934">Yates 1934</a>)</span> is
always applied to Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test.</p>
<p>For all tests of independence <code><a href="../reference/visstat.html">visstat()</a></code> displays a
grouped column plot that includes the respective testâ€™s p-value in the
title, as well as a mosaic plot showing colour-coded Pearson residuals
and the p-value of Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-test.</p>
</div>
<div class="section level3">
<h3 id="transforming-a-contingency-table-to-a-data-frame">Transforming a contingency table to a data frame<a class="anchor" aria-label="anchor" href="#transforming-a-contingency-table-to-a-data-frame"></a>
</h3>
<p>The following examples for tests of categorical predictor and
response are all based on the <code>HairEyeColor</code> contingency
table.</p>
<p>Contingency tables must be converted to the required column-based
<code>data.frame</code> using the helper function
<code><a href="../reference/counts_to_cases.html">counts_to_cases()</a></code>. The function transforms the contingency
table <code>HairEyeColor</code> into <code>data.frame</code> named
<code>HairEyeColourDataFrame</code>.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">HairEyeColourDataFrame</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">HairEyeColor</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3">
<h3 id="examples-3">Examples<a class="anchor" aria-label="anchor" href="#examples-3"></a>
</h3>
<p>In all examples of this section, we will test the null hypothesis
that hair colour (â€œHairâ€) and eye colour (â€œEyeâ€) are independent of each
other.</p>
<div class="section level4">
<h4 id="pearsons-chi2-test-chisq-test-1">Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
(<code>chisq.test()</code>)<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-chisq-test-1"></a>
</h4>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_eye_colour_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">HairEyeColor</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">hair_eye_colour_df</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">hair_eye_colour_df</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-10-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-10-2.png" width="700"></p>
<p>The graphical output shows that the null hypothesis of Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
test â€“ namely, that hair colour and eye colour are independent â€“ must be
rejected at the default significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Î±</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">\alpha=0.05</annotation></semantics></math>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>2.33</mn><mo>â‹…</mo><msup><mn>10</mn><mrow><mo>âˆ’</mo><mn>25</mn></mrow></msup><mo>&lt;</mo><mi>Î±</mi></mrow><annotation encoding="application/x-tex">p = 2.33 \cdot 10^{-25} &lt;
\alpha</annotation></semantics></math>). The mosaic plot indicates that
the strongest deviations are due to over-representation of individuals
with black hair and brown eyes, and of those with blond hair and blue
eyes. In contrast, individuals with blond hair and brown eyes are the
most under-represented.</p>
<div class="section level5">
<h5 id="pearsons-chi2-test-with-yates-continuity-correction-1">Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
with Yateâ€™s continuity correction<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-with-yates-continuity-correction-1"></a>
</h5>
<p>In the following example, we restrict the data to participants with
either black or brown hair and either brown or blue eyes, resulting in a
2-by-2 contingency table.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_black_brown_eyes_brown_blue</span> <span class="op">&lt;-</span> <span class="va">HairEyeColor</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="op">]</span></span>
<span><span class="co"># Transform to data frame</span></span>
<span><span class="va">hair_black_brown_eyes_brown_blue_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">hair_black_brown_eyes_brown_blue</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Chi-squared test</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">hair_black_brown_eyes_brown_blue_df</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">hair_black_brown_eyes_brown_blue_df</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-11-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-11-2.png" width="700"></p>
<p>Also in this reduced dataset we reject the null hypothesis of
independence of the hair colors â€œbrownâ€ and â€œblackâ€ from the eye colours
â€œbrownâ€ and â€ blueâ€. The mosaic plot shows that blue-eyed persons with
black hair are under- represented. Note the higher p-value of Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
with Yateâ€™s continuity correction (p = 0.00354) compared to the p-value
of Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
(p = 0.00229) shown in the mosaic plot.</p>
</div>
</div>
<div class="section level4">
<h4 id="fishers-exact-test-fisher-test">Fisherâ€™s exact test (<code>fisher.test()</code>)<a class="anchor" aria-label="anchor" href="#fishers-exact-test-fisher-test"></a>
</h4>
<p>Again, we extract a 2-by-2 contingency table from the full dataset,
this time keeping only male participants with black or brown hair and
hazel or green eyes.</p>
<p>Pearsonâ€™s
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>Ï‡</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>
test applied to this table would yield an expected frequency less than 5
in one of the four cells (25% of all cells), which violates the
requirement that at least 80% of the expected frequencies must be 5 or
greater <span class="citation">(<a href="#ref-Cochran:1954">Cochran
1954</a>)</span>.</p>
<p>Therefore, <code><a href="../reference/visstat.html">visstat()</a></code> automatically selects Fisherâ€™s
exact test instead.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_eye_colour_male</span> <span class="op">&lt;-</span> <span class="va">HairEyeColor</span><span class="op">[</span>, , <span class="fl">1</span><span class="op">]</span></span>
<span><span class="co"># Slice out a 2 by 2 contingency table</span></span>
<span><span class="va">black_brown_hazel_green_male</span> <span class="op">&lt;-</span> <span class="va">hair_eye_colour_male</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span>
<span><span class="co"># Transform to data frame</span></span>
<span><span class="va">black_brown_hazel_green_male</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Fisher test</span></span>
<span><span class="va">fisher_stats</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-12-1.png" width="700"><img src="visStatistics_files/figure-html/unnamed-chunk-12-2.png" width="700"></p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="saving-the-graphical-output">Saving the graphical output<a class="anchor" aria-label="anchor" href="#saving-the-graphical-output"></a>
</h2>
<p>All generated graphics can be saved in any file format supported by
<code>Cairo()</code>, including â€œpngâ€, â€œjpegâ€, â€œpdfâ€, â€œsvgâ€, â€œpsâ€, and
â€œtiffâ€ in the user specified <code>plotDirectory</code>.</p>
<p>If the optional argument <code>plotName</code> is not given, the
naming of the output follows the pattern
<code>"testname_namey_namex."</code>, where <code>"testname"</code>
specifies the selected test and <code>"namey"</code> and
<code>"namex"</code> are character strings naming the selected data
vectors <code>y</code> and <code>x</code>, respectively. The suffix
corresponding to the chosen <code>graphicsoutput</code> (e.g.,
<code>"pdf"</code>, <code>"png"</code>) is then concatenated to form the
complete output file name.</p>
<p>In the following example, we store the graphics in <code>png</code>
format in the <code>plotDirectory</code> <code><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir()</a></code> with the
default naming convention:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Graphical output written to plotDirectory: In this example </span></span>
<span><span class="co"># a bar chart to visualise the Chi-squared test and mosaic plot showing</span></span>
<span><span class="co"># Pearson's residuals.</span></span>
<span><span class="co">#chi_squared_or_fisher_Hair_Eye.png and mosaic_complete_Hair_Eye.png</span></span>
<span><span class="va">save_fisher</span> <span class="op">=</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span>, <span class="st">"Hair"</span>, <span class="st">"Eye"</span>,</span>
<span>        graphicsoutput <span class="op">=</span> <span class="st">"png"</span>, plotDirectory <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The full file path of the generated graphics are stored as the
attribute <code>"plot_paths"</code> on the returned object of class
<code>"visstat"</code>.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">paths</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">save_fisher</span>, <span class="st">"plot_paths"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">paths</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "/tmp/Rtmp9rwxnB/chi_squared_or_fisher_Hair_Eye.png"</span></span>
<span><span class="co">## [2] "/tmp/Rtmp9rwxnB/mosaic_complete_Hair_Eye.png"</span></span></code></pre>
<p>Remove the graphical output from <code>plotDirectory</code>:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/files.html" class="external-link">file.remove</a></span><span class="op">(</span><span class="va">paths</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] TRUE TRUE</span></span></code></pre>
<p>When assumptions plots (residual and Q-Q plot) are generated, the
corresponding plot has the prefix <code>"assumption_</code>.</p>
</div>
<div class="section level2">
<h2 id="limitations">Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a>
</h2>
<div class="section level3">
<h3 id="limitations-by-default-settings">Limitations by default settings<a class="anchor" aria-label="anchor" href="#limitations-by-default-settings"></a>
</h3>
<p>The main purpose of this package is a decision-logic based automatic
visualisation of statistical test results. Therefore, except for the
user-adjustable <code>conf.level</code> parameter, all statistical tests
are applied using their default settings from the corresponding base R
functions. As a consequence, paired tests are currently not supported
and <code><a href="../reference/visstat.html">visstat()</a></code> does not allow to study interactions terms
between the different levels of an independent variable in an analysis
of variance. Focusing on the graphical representation of tests, only
simple linear regression is implemented, as multiple linear regressions
cannot be visualised.</p>
</div>
<div class="section level3">
<h3 id="limitations-of-of-decision-logic-based-on-p-values">Limitations of of decision-logic based on p-values<a class="anchor" aria-label="anchor" href="#limitations-of-of-decision-logic-based-on-p-values"></a>
</h3>
<p>The decision logic of the main function <code><a href="../reference/visstat.html">visstat()</a></code> is
based on the Shapiro-Wilk test for normality and the
Levene-Brown-Forsythe test for variance homogeneity. However, no single
test maintains optimal Type I error rates and statistical power across
all distributions <span class="citation">(<a href="#ref-Olejnik:1987">Olejnik and Algina 1987</a>)</span>, and
p-values obtained from these tests may be unreliable if their
assumptions are violated.</p>
<p>Combining multiple tests with differing assumptions using simple
majority voting inflates the overall Type I error rate, making such an
approach inadequate. Therefore, automated test selection based solely on
p-values cannot replace the visual inspection of sample distributions
provided by <code><a href="../reference/visstat.html">visstat()</a></code>. Based on the provided diagnostic
plots, it may be necessary to override the automated choice of test in
individual cases.</p>
</div>
<div class="section level3">
<h3 id="bootstrapping-as-modern-alternative-to-hypothesis-testing">Bootstrapping as modern alternative to hypothesis testing<a class="anchor" aria-label="anchor" href="#bootstrapping-as-modern-alternative-to-hypothesis-testing"></a>
</h3>
<p>Bootstrapping methods <span class="citation">(<a href="#ref-Wilcox:2021">Wilcox 2021</a>)</span> make minimal
distributional assumptions and can provide confidence intervals for
nearly any statistic. However, bootstrapping is computationally
intensive, often requiring thousands of resamples, and may perform
poorly with very small sample sizes.
<!-- Additionally, the empirical literature lacks sufficient evidence to establish the bootstrap as a universally reliable approach for uncertainty quantification across diverse statistical contexts.[@Zrimsek:2024] --></p>
<p>The computational intensity of bootstrap and runs counter to the
purpose of the <code>visStatistics</code> package, which is designed to
offer a rapid overview of the data, laying the groundwork for deeper
analysis in subsequent steps.</p>
<p>The package targets users with basic statistical literacy, such as
non-specialist professionals and students in applied fields. It covers
topics typically included in an undergraduate course on applied
statistics, intentionally excluding more advanced methods like
bootstrapping to keep the focus on foundational concepts.</p>
</div>
</div>
<div class="section level2">
<h2 id="implemented-tests">Implemented tests<a class="anchor" aria-label="anchor" href="#implemented-tests"></a>
</h2>
<div class="section level3">
<h3 id="numeric-response-and-categorical-predictor">Numeric response and categorical predictor<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor"></a>
</h3>
<p>When the response is numeric and the predictor is categorical, a test
of central tendency is selected:</p>
<p><code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>, <code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>,
<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>,
<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>,<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code></p>
<div class="section level4">
<h4 id="normality-assumption-check">Normality assumption check<a class="anchor" aria-label="anchor" href="#normality-assumption-check"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code> and <code>ad.test()</code> <span class="citation">(<a href="#ref-Gross:2015">Gross and Ligges
2015</a>)</span></p>
</div>
<div class="section level4">
<h4 id="homoscedasticity-assumption-check">Homoscedasticity assumption check<a class="anchor" aria-label="anchor" href="#homoscedasticity-assumption-check"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code> and <code>levene.test()</code></p>
</div>
<div class="section level4">
<h4 id="post-hoc-tests">Post-hoc tests<a class="anchor" aria-label="anchor" href="#post-hoc-tests"></a>
</h4>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> (for <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>and
<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>)</li>
<li>
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> (for
<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>)</li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="numeric-response-and-numeric-predictor">Numeric response and numeric predictor<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor"></a>
</h3>
<p>When both the response and predictor are numeric, a simple linear
regression model is fitted:</p>
<p><code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code></p>
</div>
<div class="section level3">
<h3 id="both-variables-categorical">Both variables categorical<a class="anchor" aria-label="anchor" href="#both-variables-categorical"></a>
</h3>
<p>When both variables are categorical, <code><a href="../reference/visstat.html">visstat()</a></code> tests the
null hypothesis of independence using one of the following:</p>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code> (default for larger samples)</li>
<li>
<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code> (used for small expected cell counts
based on Cochranâ€™s rule)</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="bibliography">Bibliography<a class="anchor" aria-label="anchor" href="#bibliography"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Abdi:2007" class="csl-entry">
Abdi, HervÃ©. 2007. <span>â€œThe Bonferonni and <span>Sidak</span>
Corrections for Multiple Comparisons.â€</span> <em>Encyclopedia of
Measurement and Statistics</em>.
</div>
<div id="ref-Allingham:2012" class="csl-entry">
Allingham, David, and J. C. W. Rayner. 2012. <span>â€œTesting
<span>Equality</span> of <span>Variances</span> for <span>Multiple
Univariate Normal Populations</span>.â€</span> <em>Journal of Statistical
Theory and Practice</em> 6 (3): 524â€“35. <a href="https://doi.org/10.1080/15598608.2012.695703" class="external-link">https://doi.org/10.1080/15598608.2012.695703</a>.
</div>
<div id="ref-Bartlett:1937" class="csl-entry">
Bartlett, M. S. 1937. <span>â€œProperties of Sufficiency and Statistical
Tests.â€</span> <em>Proceedings of the Royal Society of London. Series A,
Mathematical and Physical Sciences</em> 160 (901): 268â€“82. <a href="https://doi.org/10.1098/rspa.1937.0109" class="external-link">https://doi.org/10.1098/rspa.1937.0109</a>.
</div>
<div id="ref-Brown:1974" class="csl-entry">
Brown, Morton B., and Alan B. Forsythe. 1974. <span>â€œRobust
<span>Tests</span> for the <span>Equality</span> of
<span>Variances</span>.â€</span> <em>Journal of the American Statistical
Association</em> 69 (346): 364â€“67. <a href="https://doi.org/10.1080/01621459.1974.10482955" class="external-link">https://doi.org/10.1080/01621459.1974.10482955</a>.
</div>
<div id="ref-Cochran:1954" class="csl-entry">
Cochran, William G. 1954. <span>â€œThe <span>Combination</span> of
<span>Estimates</span> from <span>Different Experiments</span>.â€</span>
<em>Biometrics</em> 10 (1): 101. <a href="https://doi.org/10.2307/3001666" class="external-link">https://doi.org/10.2307/3001666</a>.
</div>
<div id="ref-Delacre:2017" class="csl-entry">
Delacre, Marie, DaniÃ«l Lakens, and Christophe Leys. 2017. <span>â€œWhy
Psychologists Should by Default Use <span>Welch</span>â€™s t-Test Instead
of <span>Student</span>â€™s t-Test.â€</span> <em>International Review of
Social Psychology</em> 30 (1): 92â€“101. <a href="https://doi.org/10.5334/irsp.82" class="external-link">https://doi.org/10.5334/irsp.82</a>.
</div>
<div id="ref-Fagerland:2009" class="csl-entry">
Fagerland, Morten W., and Leiv Sandvik. 2009. <span>â€œPerformance of Five
Two-Sample Location Tests for Skewed Distributions with Unequal
Variances.â€</span> <em>Contemporary Clinical Trials</em> 30 (5): 490â€“96.
<a href="https://doi.org/10.1016/j.cct.2009.06.007" class="external-link">https://doi.org/10.1016/j.cct.2009.06.007</a>.
</div>
<div id="ref-Fisher:1935" class="csl-entry">
Fisher, Roland A. 1971. <em>The Design of Experiments</em>. 9th ed.
Macmillan.
</div>
<div id="ref-Fisher:1990" class="csl-entry">
Fisher, Ronald A., and F Yates. 1990. <em>Statistical
<span>Methods</span>, <span>Experimental Design</span>, and
<span>Scientific Inference</span>: <span class="nocase">A
Re-issue</span> of <span>Statistical Methods</span> for <span>Research
Workers</span>, the <span>Design</span> of <span>Experiments</span> and
<span>Statistical Methods</span> and <span>Scientific
Inference</span></em>. Edited by J H Bennett. Oxford University
PressOxford. <a href="https://doi.org/10.1093/oso/9780198522294.001.0001" class="external-link">https://doi.org/10.1093/oso/9780198522294.001.0001</a>.
</div>
<div id="ref-Fisher:1970" class="csl-entry">
Fisher, Ronald Aylmer. 1970. <em>Statistical Methods for Research
Workers</em>. 14th ed., revised and enlarged. Edinburgh: <span>Oliver
and Boyd</span>.
</div>
<div id="ref-Fox:2019" class="csl-entry">
Fox, John, and Sanford Weisberg. 2019. <em>An <span>R</span> Companion
to Applied Regression</em>. 3rd ed. Thousand Oaks CA: Sage.
</div>
<div id="ref-Ghasemi:2012" class="csl-entry">
Ghasemi, Asghar, and Saleh Zahediasl. 2012. <span>â€œNormality
<span>Tests</span> for <span>Statistical Analysis</span>: <span>A
Guide</span> for <span>Non-Statisticians</span>.â€</span> <em>Int J
Endocrinol Metab</em> 10 (2): 486â€“89. <a href="https://doi.org/10.5812/ijem.3505" class="external-link">https://doi.org/10.5812/ijem.3505</a>.
</div>
<div id="ref-Graves:2024" class="csl-entry">
Graves, Spencer, Hans-Peter Piepho, and Luciano Selzer with help from
Sundar Dorai-Raj. 2024. <em><span class="nocase">multcompView</span>:
<span>Visualizations</span> of Paired Comparisons</em>. Manual. <a href="https://doi.org/10.32614/CRAN.package.multcompView" class="external-link">https://doi.org/10.32614/CRAN.package.multcompView</a>.
</div>
<div id="ref-Gross:2015" class="csl-entry">
Gross, Juergen, and Uwe Ligges. 2015. <em>Nortest: <span>Tests</span>
for Normality</em>. Manual. <a href="https://doi.org/10.32614/CRAN.package.nortest" class="external-link">https://doi.org/10.32614/CRAN.package.nortest</a>.
</div>
<div id="ref-Hochberg:1987" class="csl-entry">
Hochberg, Yosef, and Ajit C. Tamhane. 1987. <em>Multiple
<span>Comparison Procedures</span></em>. 1st ed. Wiley
<span>Series</span> in <span>Probability</span> and
<span>Statistics</span>. Wiley. <a href="https://doi.org/10.1002/9780470316672" class="external-link">https://doi.org/10.1002/9780470316672</a>.
</div>
<div id="ref-Hollander:2014" class="csl-entry">
Hollander, Myles, Eric Chicken, and Douglas A. Wolfe. 2014.
<em>Nonparametric Statistical Methods</em>. Third edition. Wiley Series
in Probability and Statistics. Hoboken, New Jersey: John Wiley &amp;
Sons, Inc.
</div>
<div id="ref-Holm:1979" class="csl-entry">
Holm, Sture. 1979. <span>â€œA <span>Simple Sequentially Rejective Multiple
Test Procedure</span>.â€</span> <em>Scandinavian Journal of
Statistics</em> 6 (2): 65â€“70. <a href="https://www.jstor.org/stable/4615733" class="external-link">https://www.jstor.org/stable/4615733</a>.
</div>
<div id="ref-Kozak:2018" class="csl-entry">
Kozak, M., and H.-P. Piepho. 2018. <span>â€œWhatâ€™s Normal Anyway?
<span>Residual</span> Plots Are More Telling Than Significance Tests
When Checking <span><span class="smallcaps">ANOVA</span></span>
Assumptions.â€</span> <em>J Agronomy Crop Science</em> 204 (1): 86â€“98. <a href="https://doi.org/10.1111/jac.12220" class="external-link">https://doi.org/10.1111/jac.12220</a>.
</div>
<div id="ref-Kruskal:1952" class="csl-entry">
Kruskal, William H., and W. Allen Wallis. 1952. <span>â€œUse of
<span>Ranks</span> in <span>One-Criterion Variance
Analysis</span>.â€</span> <em>Journal of the American Statistical
Association</em> 47 (260): 583â€“621. <a href="https://doi.org/10.2307/2280779" class="external-link">https://doi.org/10.2307/2280779</a>.
</div>
<div id="ref-Kwak:2017" class="csl-entry">
Kwak, Sang Gyu, and Jong Hae Kim. 2017. <span>â€œCentral Limit Theorem:
The Cornerstone of Modern Statistics.â€</span> <em>Korean J
Anesthesiol</em> 70 (2): 144â€“56. <a href="https://doi.org/10.4097/kjae.2017.70.2.144" class="external-link">https://doi.org/10.4097/kjae.2017.70.2.144</a>.
</div>
<div id="ref-Levene:1960" class="csl-entry">
Levene, Howard. 1960. <span>â€œRobust Tests for Equality of
Variances.â€</span> In <em>Contributions to Probability and Statistics:
<span>Essays</span> in Honor of Harold Hotelling</em>, edited by Ingram
Olkin, 278â€“92. Stanford, CA: Stanford University Press.
</div>
<div id="ref-Lumley:2002" class="csl-entry">
Lumley, Thomas, Paula Diehr, Scott Emerson, and Lu Chen. 2002.
<span>â€œThe <span>Importance</span> of the <span>Normality
Assumption</span> in <span>Large Public Health Data Sets</span>.â€</span>
<em>Annu. Rev. Public Health</em> 23 (1): 151â€“69. <a href="https://doi.org/10.1146/annurev.publhealth.23.100901.140546" class="external-link">https://doi.org/10.1146/annurev.publhealth.23.100901.140546</a>.
</div>
<div id="ref-Mann:1947" class="csl-entry">
Mann, Henry B., and Donald R. Whitney. 1947. <span>â€œOn a Test of Whether
One of Two Random Variables Is Stochastically Larger Than the
Other.â€</span> <em>The Annals of Mathematical Statistics</em> 18 (1):
50â€“60. <a href="https://doi.org/10.1214/aoms/1177730491" class="external-link">https://doi.org/10.1214/aoms/1177730491</a>.
</div>
<div id="ref-Moser:1992" class="csl-entry">
Moser, B K, and G. R. Stevens. 1992. <span>â€œHomogeneity of Variance in
the Two-Sample Means Test.â€</span> <em>The American Statistician</em>,
February, 19â€“21. <a href="https://doi.org/10.1080/00031305.1992.10475839" class="external-link">https://doi.org/10.1080/00031305.1992.10475839</a>.
</div>
<div id="ref-Olejnik:1987" class="csl-entry">
Olejnik, Stephen F., and James Algina. 1987. <span>â€œType <span>I Error
Rates</span> and <span>Power Estimates</span> of <span>Selected
Parametric</span> and <span>Nonparametric Tests</span> of
<span>Scale</span>.â€</span> <em>Journal of Educational Statistics</em>
12 (1): 45. <a href="https://doi.org/10.2307/1164627" class="external-link">https://doi.org/10.2307/1164627</a>.
</div>
<div id="ref-Pearson:1900" class="csl-entry">
Pearson, Karl. 1900. <span>â€œOn the Criterion That a Given System of
Deviations from the Probable in the Case of a Correlated System of
Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from
Random Sampling.â€</span> <em>The London, Edinburgh, and Dublin
Philosophical Magazine and Journal of Science</em> 50 (302): 157â€“75. <a href="https://doi.org/10.1080/14786440009463897" class="external-link">https://doi.org/10.1080/14786440009463897</a>.
</div>
<div id="ref-Rasch:2011" class="csl-entry">
Rasch, Dieter, Klaus D. Kubinger, and Karl Moder. 2011. <span>â€œThe
Two-Sample t Test: Pre-Testing Its Assumptions Does Not Pay Off.â€</span>
<em>Stat Papers</em> 52 (1): 219â€“31. <a href="https://doi.org/10.1007/s00362-009-0224-x" class="external-link">https://doi.org/10.1007/s00362-009-0224-x</a>.
</div>
<div id="ref-Razali:2011" class="csl-entry">
Razali, Nornadiah Mohd, and Yap Bee Wah. 2011. <span>â€œPower Comparisons
of <span>Shapiro-Wilk</span>, <span>Kolmogorov-Smirnov</span>,
<span>Lilliefors</span> and <span>Anderson-Darling</span> Tests.â€</span>
<em>Journal of Statistical Modeling and Analytics</em> 2 (1): 21â€“33.
</div>
<div id="ref-Satterthwaite:1946" class="csl-entry">
Satterthwaite, F. E. 1946. <span>â€œAn <span>Approximate
Distribution</span> of <span>Estimates</span> of <span>Variance
Components</span>.â€</span> <em>Biometrics Bulletin</em> 2 (6): 110â€“14.
<a href="https://doi.org/10.2307/3002019" class="external-link">https://doi.org/10.2307/3002019</a>.
</div>
<div id="ref-Searle:1971" class="csl-entry">
Searle, Shayle R. 1971. <em>Linear Models</em>. John Wiley &amp; Sons.
</div>
<div id="ref-Shapiro:1965" class="csl-entry">
SHAPIRO, S. S., and M. B. WILK. 1965. <span>â€œAn Analysis of Variance
Test for Normality (Complete Samples)<span></span>.â€</span>
<em>Biometrika</em> 52 (3-4): 591â€“611. <a href="https://doi.org/10.1093/biomet/52.3-4.591" class="external-link">https://doi.org/10.1093/biomet/52.3-4.591</a>.
</div>
<div id="ref-Shatz:2024" class="csl-entry">
Shatz, Itamar. 2024. <span>â€œAssumption-Checking Rather Than (Just)
Testing: <span>The</span> Importance of Visualization and Effect Size in
Statistical Diagnostics.â€</span> <em>Behav Res</em> 56 (2): 826â€“45. <a href="https://doi.org/10.3758/s13428-023-02072-x" class="external-link">https://doi.org/10.3758/s13428-023-02072-x</a>.
</div>
<div id="ref-Sidak:1967" class="csl-entry">
Å idÃ¡k, ZbynÄ›k. 1967. <span>â€œRectangular Confidence Regions for the Means
of Multivariate Normal Distributions.â€</span> <em>Journal of the
American Statistical Association</em> 62 (318): 626â€“33. <a href="https://doi.org/10.1080/01621459.1967.10482935" class="external-link">https://doi.org/10.1080/01621459.1967.10482935</a>.
</div>
<div id="ref-Welch:1947" class="csl-entry">
Welch, B. L. 1947. <span>â€œThe Generalization of <span>â€˜Studentâ€™sâ€™</span>
Problem When Several Different Population Variances Are
Involved.â€</span> <em>Biometrika</em> 34 (1â€“2): 28â€“35. <a href="https://doi.org/10.1093/biomet/34.1-2.28" class="external-link">https://doi.org/10.1093/biomet/34.1-2.28</a>.
</div>
<div id="ref-Welch:1951" class="csl-entry">
â€”â€”â€”. 1951. <span>â€œOn the <span>Comparison</span> of <span>Several Mean
Values</span>: <span>An Alternative Approach</span>.â€</span>
<em>Biometrika</em> 38 (3/4): 330â€“36. <a href="https://doi.org/10.2307/2332579" class="external-link">https://doi.org/10.2307/2332579</a>.
</div>
<div id="ref-Wilcox:2021" class="csl-entry">
Wilcox, Rand R. 2021. <em>Introduction to <span>Robust Estimation</span>
and <span>Hypothesis Testing</span></em>.
</div>
<div id="ref-Yap:2011" class="csl-entry">
Yap, B. W., and C. H. Sim. 2011. <span>â€œComparisons of Various Types of
Normality Tests.â€</span> <em>Journal of Statistical Computation and
Simulation</em> 81 (12): 2141â€“55. <a href="https://doi.org/10.1080/00949655.2010.520163" class="external-link">https://doi.org/10.1080/00949655.2010.520163</a>.
</div>
<div id="ref-Yates:1934" class="csl-entry">
Yates, F. 1934. <span>â€œContingency <span>Tables Involving Small
Numbers</span> and the
<span><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>X</mi><annotation encoding="application/x-tex">X</annotation></semantics></math></span>2
<span>Test</span>.â€</span> <em>Journal of the Royal Statistical Society
Series B: Statistical Methodology</em> 1 (2): 217â€“35. <a href="https://doi.org/10.2307/2983604" class="external-link">https://doi.org/10.2307/2983604</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Sabine Schilling.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

      </footer>
</div>






  </body>
</html>
