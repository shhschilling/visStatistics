<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<title>visStatistics: The right test, visualised ‚Ä¢ visStatistics</title>
<!-- jquery --><script src="https://cdnjs.cloudflare.com/ajax/libs/jquery/3.7.1/jquery.min.js" integrity="sha512-v2CJ7UaYy4JwqLDIrZUI/4hqeoQieOmAZNXBeQyjo21dadnwR+8ZaIJVT8EE2iyI61OV8e6M8PP2/4hpQINQ/g==" crossorigin="anonymous" referrerpolicy="no-referrer"></script><!-- Bootstrap --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/css/bootstrap.min.css" integrity="sha256-bZLfwXAP04zRMK2BjiO8iu9pf4FbLqX6zitd+tIvLhE=" crossorigin="anonymous">
<script src="https://cdnjs.cloudflare.com/ajax/libs/twitter-bootstrap/3.4.1/js/bootstrap.min.js" integrity="sha256-nuL8/2cJ5NDSSwnKD8VqreErSWHtnEP9E7AySL+1ev4=" crossorigin="anonymous"></script><!-- bootstrap-toc --><link rel="stylesheet" href="../bootstrap-toc.css">
<script src="../bootstrap-toc.js"></script><!-- Font Awesome icons --><link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/all.min.css" integrity="sha256-mmgLkCYLUQbXn0B1SRqzHar6dCnv9oZFPEC1g1cwlkk=" crossorigin="anonymous">
<link rel="stylesheet" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/5.12.1/css/v4-shims.min.css" integrity="sha256-wZjR52fzng1pJHwx4aV2AO3yyTOXrcDW7jBpJtTwVxw=" crossorigin="anonymous">
<!-- clipboard.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/clipboard.js/2.0.6/clipboard.min.js" integrity="sha256-inc5kl9MA1hkeYUt+EC3BhlIgyp/2jDIyBLS6k3UxPI=" crossorigin="anonymous"></script><!-- headroom.js --><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/headroom.min.js" integrity="sha256-AsUX4SJE1+yuDu5+mAVzJbuYNPHj/WroHuZ8Ir/CkE0=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/headroom/0.11.0/jQuery.headroom.min.js" integrity="sha256-ZX/yNShbjqsohH1k95liqY9Gd8uOiE1S4vZc+9KQ1K4=" crossorigin="anonymous"></script><!-- pkgdown --><link href="../pkgdown.css" rel="stylesheet">
<script src="../pkgdown.js"></script><meta property="og:title" content="visStatistics: The right test, visualised">
<!-- mathjax --><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js" integrity="sha256-nvJJv9wWKEm88qvoQl9ekL2J+k/RWIsaSScxxlsrv8k=" crossorigin="anonymous"></script><script src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/config/TeX-AMS-MML_HTMLorMML.js" integrity="sha256-84DKXVJXs0/F8OTMzX4UR909+jtl4G7SPypPavF+GfA=" crossorigin="anonymous"></script><!--[if lt IE 9]>
<script src="https://oss.maxcdn.com/html5shiv/3.7.3/html5shiv.min.js"></script>
<script src="https://oss.maxcdn.com/respond/1.4.2/respond.min.js"></script>
<![endif]-->
</head>
<body data-spy="scroll" data-target="#toc">


    <div class="container template-article">
      <header><div class="navbar navbar-default navbar-fixed-top" role="navigation">
  <div class="container">
    <div class="navbar-header">
      <button type="button" class="navbar-toggle collapsed" data-toggle="collapse" data-target="#navbar" aria-expanded="false">
        <span class="sr-only">Toggle navigation</span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
        <span class="icon-bar"></span>
      </button>
      <span class="navbar-brand">
        <a class="navbar-link" href="../index.html">visStatistics</a>
        <span class="version label label-default" data-toggle="tooltip" data-placement="bottom" title="">0.1.8</span>
      </span>
    </div>

    <div id="navbar" class="navbar-collapse collapse">
      <ul class="nav navbar-nav">
<li>
  <a href="../reference/index.html">Reference</a>
</li>
<li>
  <a href="../articles/visStatistics.html">Vignette</a>
</li>
<li>
  <a href="../articles/index.html">Articles</a>
</li>
      </ul>
<ul class="nav navbar-nav navbar-right">
<li>
  <a href="https://github.com/shhschilling/visStatistics/" class="external-link">
    <span class="fab fa-github fa-lg"></span>

  </a>
</li>
      </ul>
</div>
<!--/.nav-collapse -->
  </div>
<!--/.container -->
</div>
<!--/.navbar -->



      </header><div class="row">
  <div class="col-md-9 contents">
    <div class="page-header toc-ignore">
      <h1 data-toc-skip>visStatistics: The right test, visualised</h1>
                        <h4 data-toc-skip class="author">Sabine
Schilling</h4>
            <address class="author_afil">
      Institute of Tourism and Mobility, Lucerne University of Applied
Sciences and
Arts<br><a class="author_email" href="mailto:#"></a><a href="mailto:sabine.schilling@protonmail.com" class="email">sabine.schilling@protonmail.com</a>
      </address>
                  
            <h4 data-toc-skip class="date">2025-12-27</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/shhschilling/visStatistics/blob/master/vignettes/visStatistics.Rmd" class="external-link"><code>vignettes/visStatistics.Rmd</code></a></small>
      <div class="hidden name"><code>visStatistics.Rmd</code></div>

    </div>

    
    
<div class="section level2">
<h2 id="abstract">Abstract<a class="anchor" aria-label="anchor" href="#abstract"></a>
</h2>
<p><code>visStatistics</code> automatically selects and visualises
appropriate statistical hypothesis tests between two column vectors of
type of class <code>"numeric"</code>, <code>"integer"</code>, or
<code>"factor"</code>. The choice of test depends on the
<code>class</code>, distribution, and sample size of the vectors, as
well as the user-defined ‚Äòconf.level‚Äô. The main function
<code><a href="../reference/visstat.html">visstat()</a></code> visualises the selected test with appropriate
graphs (box plots, bar charts, regression lines with confidence bands,
mosaic plots, residual plots, Q-Q plots), annotated with the main test
results, including any assumption checks and post-hoc analyses. This
scripted workflow is particularly suited for browser-based interfaces
that rely on server-side R applications connected to secure databases,
where users have no direct access, or for quick data visualisations and
test selection, e.g., in statistical consulting projects.</p>
</div>
<div class="section level2">
<h2 id="introduction">Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<!-- Introductory statistics courses typically cover a fixed set of hypothesis tests, but selecting the correct method in practice can be error-prone.  -->
<!-- `visStatistics` addresses this by applying a deterministic, transparent test selection logic.  -->
<p>While numerous R packages provide statistical testing functionality,
few are designed with pedagogical accessibility as a primary concern.
The visStatistics package addresses this challenge by automating test
selection using deterministic decision logic, removing the burden of
manual test choice. This automation enables users to focus directly on
interpreting statistical outcomes rather than navigating test
selection.</p>
<p>The tailored visual outputs‚Äîannotated with test results and, where
appropriate, assumption checks and post-hoc analyses‚Äîfurther support
comprehension and help ensure valid conclusions from the outset. The
package is particularly valuable in statistical consulting for student
research projects, where time constraints demand streamlined,
assumption-aware output that prioritises interpretation over technical
execution. The implemented tests cover the typical content of an
introductory undergraduate course in statistics.</p>
<p>The package also suits server-based applications where users have
limited interaction: they provide only two input vector, and the
software returns valid, interpretable results without requiring further
statistical knowledge. This supports reproducibility and correct
inference even in such constrained environments.</p>
<p>The remainder of this vignette is organised as follows:</p>
<ul>
<li><p>Section 3 focuses on the installation and the main function
call.</p></li>
<li><p>Section 4 briefly introduces the General Linear Model and the
testing of its assumptions in <code>visstatistic</code>.</p></li>
<li><p>Section 5 summarises the decision logic used to select a
statistical test.</p></li>
<li><p>Section 6 provides background on the implemented tests and
illustrates the decision logic using examples. Function names in
parentheses in the headings indicate the corresponding statistical
hypothesis test function in R.</p></li>
<li><p>Section 7 outlines the main limitations of the package.</p></li>
<li><p>Section 8 provides an overview of the implemented tests.</p></li>
</ul>
</div>
<div class="section level2">
<h2 id="getting-started">Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<div class="section level5">
<h5 class="unnumbered" id="install-the-latest-development-version-from-github">1. Install the latest development version from
GITHUB<a class="anchor" aria-label="anchor" href="#install-the-latest-development-version-from-github"></a>
</h5>
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">install_github</span><span class="op">(</span><span class="st">"shhschilling/visStatistics"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 class="unnumbered" id="load-the-package">2. Load the package<a class="anchor" aria-label="anchor" href="#load-the-package"></a>
</h5>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/shhschilling/visStatistics" class="external-link">visStatistics</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 class="unnumbered" id="minimal-function-call">3. Minimal function call<a class="anchor" aria-label="anchor" href="#minimal-function-call"></a>
</h5>
<p>The function <code><a href="../reference/visstat.html">visstat()</a></code> accepts input in three ways:</p>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Standardised form (recommended):</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Formula interface:</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">y</span> <span class="op">~</span> <span class="va">x</span>, data <span class="op">=</span> <span class="va">dataframe</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Backward-compatible form:</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">dataframe</span>, <span class="st">"namey"</span>, <span class="st">"namex"</span><span class="op">)</span></span></code></pre></div>
<p><code>x</code> and <code>y</code> must be vectors of class
<code>"numeric"</code>, <code>"integer"</code>, or
<code>"factor"</code>.</p>
<p>In the formula interface, <code>y ~ x</code> specifies the
relationship, where <code>y</code> is the response variable and
<code>x</code> is the predictor, with both being column names in
<code>dataframe</code>.</p>
<p>In the backward-compatible form, <code>"namex"</code> and
<code>"namey"</code> must be character strings naming columns in
<code>dataframe</code>, which must themselves be of class
<code>"numeric"</code>, <code>"integer"</code>, or
<code>"factor"</code>. This is equivalent to writing:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">dataframe</span><span class="op">[[</span><span class="st">"namex"</span><span class="op">]</span><span class="op">]</span>, <span class="va">dataframe</span><span class="op">[[</span><span class="st">"namey"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2">
<h2 id="general-linear-model-glm">General Linear Model (GLM)<a class="anchor" aria-label="anchor" href="#general-linear-model-glm"></a>
</h2>
<p>General Linear Models (GLM) <span class="citation">(<a href="#ref-Searle:1971">Searle 1971</a>)</span> provide a unified
mathematical framework underlying many common statistical tests like
Student‚Äôs t-test, Fisher ANOVA, and simple linear regression. These
tests are not distinct statistical methods, but rather different
implementations of the same underlying mathematical framework.</p>
<p>The GLM can be expressed as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><msub><mi>x</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><mi>‚ãØ</mi><mo>+</mo><msub><mi>Œ≤</mi><mi>k</mi></msub><msub><mi>x</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>+</mo><msub><mi>Œµ</mi><mi>i</mi></msub><mspace width="1.0em"></mspace><mrow><mtext mathvariant="normal">for </mtext><mspace width="0.333em"></mspace></mrow><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>n</mi><mo>,</mo><mspace width="1.0em"></mspace><msub><mi>Œµ</mi><mi>i</mi></msub><mo>‚àº</mo><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mn>0</mn><mo>,</mo><msup><mi>œÉ</mi><mn>2</mn></msup><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">Y_i = \beta_0 + \beta_1 x_{i1} + \cdots + \beta_k x_{ik} + \varepsilon_i \quad \text{for } i = 1, \ldots, n, \quad \varepsilon_i \sim {N}(0, \sigma^2)</annotation></semantics></math></p>
<p>where:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
are the number of observations,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
the number of predictors,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">Y_i</annotation></semantics></math>
the model random variables,</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">x_{ij}</annotation></semantics></math>
the predictor values</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>,</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Œ≤</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\beta_0, \beta_1, \ldots, \beta_k</annotation></semantics></math>
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><annotation encoding="application/x-tex">(k+1)</annotation></semantics></math>
parameters and</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œµ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\varepsilon_i</annotation></semantics></math>
the normally distributed error terms with constant, unknown variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œÉ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></p></li>
</ul>
<p>In a more compact matrix form this can be written as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ùêò</mi><mo>=</mo><mi>ùêó</mi><mi>ùõÉ</mi><mo>+</mo><mi>ùõÜ</mi></mrow><annotation encoding="application/x-tex">\mathbf{Y} = \mathbf{X}\boldsymbol{\beta} + \boldsymbol{\varepsilon}</annotation></semantics></math></p>
<p>where:</p>
<ul>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêò</mi><annotation encoding="application/x-tex">\mathbf{Y}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
(response vector for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
observations)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùêó</mi><annotation encoding="application/x-tex">\mathbf{X}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">n \times (k+1)</annotation></semantics></math>
(design matrix:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
observations,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>+</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k+1</annotation></semantics></math>
parameters)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùõÉ</mi><annotation encoding="application/x-tex">\boldsymbol{\beta}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>√ó</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">(k+1) \times 1</annotation></semantics></math>
(parameter vector:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
predictors + intercept)</p></li>
<li><p><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ùõÜ</mi><annotation encoding="application/x-tex">\boldsymbol{\varepsilon}</annotation></semantics></math>
is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>√ó</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">n \times 1</annotation></semantics></math>
(error vector for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
observations)</p></li>
</ul>
<p>Both the parameter and error vector have to be estimated from
data:</p>
<p>We observe realisations
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mi>i</mi></msub><annotation encoding="application/x-tex">y_i</annotation></semantics></math>
of the response, and estimate the unknown parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Œ≤</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\beta_0, \dots, \beta_k</annotation></semantics></math>
by minimising the residual sum of squares:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msup><mrow><mo stretchy="true" form="prefix">[</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚àí</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><mi>‚ãØ</mi><mo>+</mo><msub><mi>b</mi><mi>k</mi></msub><msub><mi>x</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">]</mo></mrow><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">\sum_{i=1}^{n} e_i^2 = \sum_{i=1}^{n} \left[y_i - (b_0 + b_1 x_{i1} + \cdots + b_k x_{ik})\right]^2</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>b</mi><mn>0</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>b</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">b_0, \dots, b_k</annotation></semantics></math>
are the least-squares <strong>estimators</strong> of the true parameters
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><msub><mi>Œ≤</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">\beta_0, \dots, \beta_k</annotation></semantics></math>.</p>
<!-- Collectively, they form the estimate vector \( \mathbf{b} \)  $${\boldsymbol{b}} = (\mathbf{X}^T\mathbf{X})^{-1}\mathbf{X}^T\mathbf{Y}$$ -->
<p>The fitted value for observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>y</mi><mo accent="true">ÃÇ</mo></mover><mi>i</mi></msub><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><msub><mi>x</mi><mrow><mi>i</mi><mn>1</mn></mrow></msub><mo>+</mo><mi>‚Ä¶</mi><mo>+</mo><msub><mi>b</mi><mi>k</mi></msub><msub><mi>x</mi><mrow><mi>i</mi><mi>k</mi></mrow></msub><mo>,</mo></mrow><annotation encoding="application/x-tex">
\hat{y}_i = b_0 + b_1 x_{i1} + \dots + b_k x_{ik},
</annotation></semantics></math></p>
<p>and the residual can thus be written as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>e</mi><mi>i</mi></msub><mo>=</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚àí</mo><msub><mover><mi>y</mi><mo accent="true">ÃÇ</mo></mover><mi>i</mi></msub><mi>.</mi></mrow><annotation encoding="application/x-tex">
e_i = y_i - \hat{y}_i.
</annotation></semantics></math></p>
<p>The residuals are our estimators for the unknown on error terms
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œµ</mi><mi>i</mi></msub><annotation encoding="application/x-tex">\epsilon_i</annotation></semantics></math>.
We estimate the unknown variance of the error term by the square
standard error .</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mi>E</mi><mn>2</mn></msubsup><mo>=</mo><mn>1</mn><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚ãÖ</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></munderover><msubsup><mi>e</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">s_E^2=1/(n-k-1)\cdot \sum_{i=1}^n e_i^2 </annotation></semantics></math></p>
<div class="section level3">
<h3 id="glm-assumptions">GLM Assumptions<a class="anchor" aria-label="anchor" href="#glm-assumptions"></a>
</h3>
<p>The GLM assumptions are:</p>
<ul>
<li>Linearity: The mean structure
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>E</mi><mrow><mo stretchy="true" form="prefix">[</mo><mi>ùêò</mi><mo stretchy="true" form="postfix">]</mo></mrow><mo>=</mo><mi>ùêó</mi><mi>ùõÉ</mi></mrow><annotation encoding="application/x-tex">E[\mathbf{Y}] = \mathbf{X}\boldsymbol{\beta}</annotation></semantics></math>
is linear in parameters; assessed by checking for systematic patterns in
residual plots.</li>
</ul>
<p>Error terms are:</p>
<ul>
<li><p>independent,</p></li>
<li><p>normally distributed and</p></li>
<li><p>homoscedastic: Constant error variance
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œÉ</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\sigma^2</annotation></semantics></math></p></li>
</ul>
</div>
<div class="section level3">
<h3 id="glm-assumption-testing-in-visstatistics-vis_glm_assumptions">GLM assumption testing in visStatistics:
<code>vis_glm_assumptions()</code><a class="anchor" aria-label="anchor" href="#glm-assumption-testing-in-visstatistics-vis_glm_assumptions"></a>
</h3>
<p>The function <code>vis_glm_assumptions()</code> provides unified
diagnostic tools for all GLM variants. Since t-tests, ANOVA, and linear
regression share identical distributional assumptions, the same
residual-based diagnostics apply universally.</p>
<p><strong>Normality Assessment</strong>: The function evaluates
residual normality using both the Shapiro-Wilk test
(<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>) and Anderson-Darling test
(<code>ad.test()</code>) <span class="citation">(<a href="#ref-Gross:2015">Gross and Ligges 2015</a>)</span>. These tests
offer complementary strengths: Shapiro-Wilk generally exhibits greater
power across non-normal distributions in small samples, while
Anderson-Darling is highly sensitive to tail deviations in larger
samples <span class="citation">(<a href="#ref-Razali:2011">Razali and
Wah 2011</a>; <a href="#ref-Yap:2011">Yap and Sim 2011</a>)</span>.</p>
<p><strong>Homoscedasticity Assessment</strong>: Variance equality is
tested using the Levene-Brown-Forsythe test (<code>levene.test()</code>)
<span class="citation">(<a href="#ref-Brown:1974">Brown and Forsythe
1974</a>)</span> and Bartlett‚Äôs test (<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>).
<!-- The Levene-Brown-Forsythe test uses absolute deviations from group medians, making it robust to non-normality, while Bartlett's test has greater power when normality holds [@Allingham:2012]. --></p>
<p><strong><em>The Levene‚ÄìBrown‚ÄìForsythe test
<code>levene.test()</code></em></strong> The Levene‚ÄìBrown‚ÄìForsythe test
improves upon Levene‚Äôs original test <span class="citation">(<a href="#ref-Levene:1960">Levene 1960</a>)</span> by using the median
instead of the mean to centre the data. This makes it more robust to
skewed data or data with outliers providing more reliable results in
many practical situations <span class="citation">(<a href="#ref-Allingham:2012">Allingham and Rayner 2012</a>)</span>. Note
that <code>levene.test()</code> mimics the default behaviour of
<code>leveneTest()</code> in the <code>car</code> package <span class="citation">(<a href="#ref-Fox:2019">Fox and Weisberg
2019</a>)</span>.</p>
<p>The Levene‚ÄìBrown‚ÄìForsythe test evaluates the null hypothesis that all
groups have equal variances by testing whether the absolute deviations
from group medians are equal across groups.</p>
<p>For each observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">y_{ij}</annotation></semantics></math>
in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
it computes the absolute deviation from the group median:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><mover><msub><mi>y</mi><mi>i</mi></msub><mo accent="true">ÃÉ</mo></mover><mo stretchy="true" form="postfix">|</mo></mrow></mrow><annotation encoding="application/x-tex">z_{ij} = |y_{ij} - \tilde{y_i}|</annotation></semantics></math>,</p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><msub><mi>y</mi><mi>i</mi></msub><mo accent="true">ÃÉ</mo></mover><annotation encoding="application/x-tex">\tilde{y_i}</annotation></semantics></math>
is the median of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
<p>The test statistic is the F-statistic from a one-way ANOVA on the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">z_{ij}</annotation></semantics></math>
values:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mfrac><mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo>‚àí</mo><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>‚àí</mo><mi>k</mi></mrow></mfrac></mfrac><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>‚àí</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo>‚àí</mo><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow></mfrac></mrow><annotation encoding="application/x-tex">F = \frac{\frac{\sum_{i=1}^{k} n_i (\bar{z}_i - \bar{z})^2}{k-1}}{\frac{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (z_{ij} - \bar{z}_i)^2}{N-k}} = \frac{(N-k) \sum_{i=1}^{k} n_i (\bar{z}_i - \bar{z})^2}{(k-1) \sum_{i=1}^{k} \sum_{j=1}^{n_i} (z_{ij} - \bar{z}_i)^2}</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the total sample size,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\bar{z}_i</annotation></semantics></math>
is the mean of absolute deviations from the median in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>z</mi><mo accent="true">‚Äæ</mo></mover><annotation encoding="application/x-tex">\bar{z}</annotation></semantics></math>
is the overall mean of all absolute deviations.</p>
<p>Under the null hypothesis of equal variances, the test statistic
follows an F-distribution:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>‚àº</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo>,</mo><mi>N</mi><mo>‚àí</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F \sim F(k-1, N-k)</annotation></semantics></math>.</p>
<p><strong><em>Bartlett‚Äôs test
<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code></em></strong></p>
<p>Additionally, homoscedasticity is assessed via Bartlett‚Äôs test
(<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>), which has more power than the
Brown‚ÄìForsythe version of Levene‚Äôs test <span class="citation">(<a href="#ref-Brown:1974">Brown and Forsythe 1974</a>)</span> when the
normality assumption is met <span class="citation">(<a href="#ref-Allingham:2012">Allingham and Rayner 2012</a>)</span></p>
<p>Bartlett‚Äôs test evaluates whether sample variances are equal across
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
normally distributed groups. The test statistic is</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>K</mi><mn>2</mn></msup><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>‚àí</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><mo>‚àí</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mi>i</mi></msub><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>ln</mo><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup></mrow><mrow><mn>1</mn><mo>+</mo><mfrac><mn>1</mn><mrow><mn>3</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mrow><mo stretchy="true" form="prefix">(</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><mn>1</mn><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>‚àí</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>‚àí</mo><mi>k</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
K^2 = \frac{(N - k) \ln s_p^2 - \sum_{i=1}^k (n_i - 1) \ln s_i^2}{
            1 + \frac{1}{3(k - 1)} \left( \sum_{i=1}^k \frac{1}{n_i - 1} 
            - \frac{1}{N - k} \right)},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_i^2</annotation></semantics></math>
is the sample variance of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_p^2</annotation></semantics></math>
is the pooled variance:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mn>1</mn><mrow><mi>N</mi><mo>‚àí</mo><mi>k</mi></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mi>i</mi></msub><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup><mi>.</mi></mrow><annotation encoding="application/x-tex">
s_p^2 = \frac{1}{N - k} \sum_{i=1}^k (n_i - 1) s_i^2.
</annotation></semantics></math></p>
<p>Under the null hypothesis that all group variances are equal and the
data are normally distributed, the test statistic approximately follows
a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-distribution
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>
degrees of freedom <span class="citation">(<a href="#ref-Bartlett:1937">Bartlett 1937</a>)</span>.</p>
<p>Assessing assumptions solely through p-values can lead to both type I
errors (false positives) and type II errors (false negatives). In large
samples, even minor, random deviations from the null hypothesis‚Äîsuch as
the assumption of normality‚Äîcan result in statistically significant
p-values, leading to type I errors. Conversely, in small samples,
substantial violations of the assumption may not reach statistical
significance, resulting in type II errors <span class="citation">(<a href="#ref-Kozak:2018">Kozak and Piepho 2018</a>)</span>.</p>
<p>Thus, the robustness of statistical tests depends on both the sample
size and the shape of the underlying distribution. This is evident in
the limitations of the normality tests used: for instance, the
Shapiro‚ÄìWilk test is unreliable for large samples
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>&gt;</mo><mn>5000</mn></mrow><annotation encoding="application/x-tex">N &gt; 5000</annotation></semantics></math>),
while the Anderson‚ÄìDarling test requires at least 7 observations.</p>
<p>Moreover, assumption tests provide no information on the nature of
deviations from the expected distribution <span class="citation">(<a href="#ref-Shatz:2024">Shatz 2024</a>)</span>. Therefore, the assessment
of normality should not rely solely on p-values but should be
complemented by visual inspection.</p>
<p><strong>Diagnostic plots</strong></p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> produces diagnostic plots including: (1) a
histogram of the standardised residuals overlaid with the standard
normal distribution, (2) a scatter plot of the standardised residuals
versus the fitted values for each predictor level, and (3) a Q‚ÄìQ plot of
the residuals.</p>
<p>Since algorithmic logic cannot replace the combination of formal
tests and expert visual judgement, the function defaults to assuming
normality if the Shapiro‚ÄìWilk test yields a p-value greater than alpha.
Simulation studies suggest that Shapiro‚ÄìWilk has the highest power among
normality tests in small to moderate samples <span class="citation">(<a href="#ref-Razali:2011">Razali and Wah 2011</a>)</span>.</p>
<!-- ### Equal variances across groups (`levene.test() and bartlett.test()`) -->
<!-- Both `aov()` and `oneway.test()` assess whether two or more samples drawn from normal distributions have the same mean. -->
<!-- While `aov()` assumes homogeneity of variances across groups, `oneway.test()` does not require equal variances. -->
<!-- The decision logic of `visStatistics` assumes homogeneity of variances if the Levene‚ÄìBrown‚ÄìForsythe test (implemented as `levene.test()`) [@Brown:1974] yields a p-value greater than $\alpha$. -->
</div>
<div class="section level3">
<h3 id="deviation-from-classical-glm-in-automated-test-selection">Deviation from classical GLM in automated test selection<a class="anchor" aria-label="anchor" href="#deviation-from-classical-glm-in-automated-test-selection"></a>
</h3>
<p>In the automated test selection, <code>visStatistics</code> defaults
to Welch‚Äôs t-test and Welch‚Äôs ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) for
mean comparisons, which, in contrast to Student‚Äôs t-test and Fisher
One-way ANOVA (both representations of the GLM) do not require
homoscedasticity of the error terms. The Welch tests use separate
variance estimates for each group rather than pooling variance.</p>
<p>Welch methods avoid the inflated Type I error rates caused by
preliminary variance testing <span class="citation">(<a href="#ref-Zimmerman:2004">Zimmerman 2004</a>)</span> while maintaining
comparable power when variances are equal <span class="citation">(<a href="#ref-Delacre:2017">Delacre, Lakens, and Leys 2017</a>)</span>.</p>
</div>
<div class="section level3">
<h3 id="testing-normality-in-welch-methods">Testing normality in Welch methods<a class="anchor" aria-label="anchor" href="#testing-normality-in-welch-methods"></a>
</h3>
<p>To assess the assumption of normality for the Welch‚Äôs t-test and
ANOVA, standardized residuals are calculated for each group to account
for heteroscedasticity These residuals are then pooled and subjected to
a Shapiro-Wilk test. This approach ensures that differences in group
variances do not confound the assessment of the underlying
distributional shape.</p>
</div>
<div class="section level3">
<h3 id="two-approaches-to-assumption-testing-classical-glm-vs--pooled-normality">Two Approaches to Assumption Testing: Classical GLM vs.¬†Pooled
Normality<a class="anchor" aria-label="anchor" href="#two-approaches-to-assumption-testing-classical-glm-vs--pooled-normality"></a>
</h3>
<p><code>visStatistics</code> provides two complementary approaches to
assess statistical assumptions, each serving different purposes in the
workflow.</p>
<div class="section level4">
<h4 id="classical-glm-diagnostics-vis_glm_assumptions">Classical GLM Diagnostics: <code>vis_glm_assumptions()</code><a class="anchor" aria-label="anchor" href="#classical-glm-diagnostics-vis_glm_assumptions"></a>
</h4>
<p>The General Linear Model (GLM) provides an elegant unified framework
for Student‚Äôs t-test, Fisher‚Äôs ANOVA, and simple linear regression.
Under the GLM, these methods share identical mathematical foundations
and rely on three key assumptions: independent observations, normally
distributed residuals, and homogeneous variances across groups
(homoscedasticity).</p>
<p>The function <code>vis_glm_assumptions()</code> fits a classical GLM
model using <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and provides comprehensive diagnostics: -
Histogram of standardized residuals with normal overlay - Residuals
vs.¬†fitted values plot - Normal Q-Q plot - Shapiro-Wilk and
Anderson-Darling normality tests (p-values) - Levene-Brown-Forsythe and
Bartlett homoscedasticity tests (p-values)</p>
<p><strong>These diagnostics enable users to assess whether classical
GLM assumptions are met and manually override the automated test
selection</strong> to use Student‚Äôs t-test or Fisher‚Äôs ANOVA when
assumptions are demonstrably satisfied.</p>
</div>
<div class="section level4">
<h4 id="pooled-normality-testing-for-automated-selection">Pooled Normality Testing for Automated Selection<a class="anchor" aria-label="anchor" href="#pooled-normality-testing-for-automated-selection"></a>
</h4>
<p>However, automated test selection based on preliminary assumption
testing creates methodological problems. Testing homoscedasticity before
selecting a parametric test inflates Type I error rates <span class="citation">(<a href="#ref-Zimmerman:2004">Zimmerman
2004</a>)</span>, while testing normality separately in each group
compounds this issue through multiple testing. Additionally, normality
tests perform poorly at both small sample sizes (insufficient power) and
large sample sizes (rejecting trivial departures) <span class="citation">(<a href="#ref-Ghasemi:2012">Ghasemi and Zahediasl
2012</a>)</span>.</p>
<p>For automated test selection, <code>visStatistics</code> uses a
different approach that avoids multiple testing: <strong>pooled
standardized normality testing</strong>. Observations within each group
are standardized using that group‚Äôs own mean and standard deviation:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>z</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mover><mi>y</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>/</mi><msub><mi>s</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">z_{ij} = (y_{ij} - \bar{y}_i) / s_i</annotation></semantics></math>.
These standardized values are then pooled across all groups, and a
single normality test is applied to the pooled values. This tests the
global null hypothesis that all groups are normally distributed while
avoiding the multiple testing problem inherent in separate per-group
tests.</p>
<p>The function <code>vis_plot_pooled_normality()</code> visualizes this
assessment: - Q-Q plot of pooled standardized residuals - Histogram of
pooled standardized residuals with normal overlay - Boxplot of original
data by group - Test results panel showing the single normality test
decision</p>
<p>Based on this single pooled normality test, the algorithm selects: -
<strong>Parametric tests</strong> (Welch‚Äôs t-test / Welch‚Äôs ANOVA) if
normality holds - <strong>Non-parametric tests</strong> (Wilcoxon /
Kruskal-Wallis) if normality fails</p>
<p>This approach protects against invalid inference through multiple
testing while enabling statistically sound automation.</p>
</div>
</div>
</div>
<div class="section level2">
<h2 id="decision-logic">Decision logic<a class="anchor" aria-label="anchor" href="#decision-logic"></a>
</h2>
<p>Throughout the remainder, data of class <code>"numeric"</code> or
<code>"integer"</code> are referred to by their common <code>mode</code>
<code>numeric</code>, while data of class <code>"factor"</code> are
referred to as categorical. The significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
used throughout for hypothesis testing, is defined as
<code>1 - conf.level</code>, where <code>conf.level</code> is a
user-controllable argument (defaulting to <code>0.95</code>).</p>
<p>The choice of statistical tests performed by the function
<code><a href="../reference/visstat.html">visstat()</a></code> depends on whether the data are numeric or
categorical, the number of levels in the categorical variable, the
distribution of the data, as well as the user-defined ‚Äòconf.level‚Äô.</p>
<p>The function prioritizes interpretable visual output and tests that
remain valid under the following decision logic:</p>
<div class="section level3">
<h3 id="numeric-response-and-categorical-predictor-comparing-central-tendencies">Numeric response and categorical predictor: Comparing central
tendencies<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor-comparing-central-tendencies"></a>
</h3>
<p>When the response is numeric and the predictor is categorical, a
statistical hypothesis test comparing groups is selected.</p>
<ul>
<li><p>If the categorical predictor has exactly two levels, Welch‚Äôs
t-test (<code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>) is applied when both groups contain more
than 30 observations. This heuristic is based on the central limit
theorem, which ensures approximate normality of the sampling
distribution of the mean <span class="citation">(<a href="#ref-Rasch:2011">Rasch, Kubinger, and Moder 2011</a>; <a href="#ref-Lumley:2002">Lumley et al. 2002</a>; <a href="#ref-Kwak:2017">Kwak and Kim 2017</a>)</span>. Welch‚Äôs t-test is
the default method in R for comparing means and is generally preferred
over Student‚Äôs t-test because it does not assume equal variances. It
maintains comparable power even when variances are equal and outperforms
Student‚Äôs test when variances differ <span class="citation">(<a href="#ref-Moser:1992">Moser and Stevens 1992</a>; <a href="#ref-Fagerland:2009">Fagerland and Sandvik 2009</a>; <a href="#ref-Delacre:2017">Delacre, Lakens, and Leys
2017</a>)</span>.</p></li>
<li><p>For smaller samples, group-wise normality is assessed using the
Shapiro-Wilk test [<span class="citation">SHAPIRO and WILK (<a href="#ref-Shapiro:1965">1965</a>)</span> (<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>)
at a significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
Simulation studies show that the Shapiro-Wilk test is the most powerful
for detecting non-normality across most distributions, especially with
smaller sample sizes.<span class="citation">(<a href="#ref-Razali:2011">Razali and Wah 2011</a>; <a href="#ref-Ghasemi:2012">Ghasemi and Zahediasl 2012</a>)</span> If both
groups are found to be approximately normally distributed according to
the Shapiro‚ÄìWilk test, Welch‚Äôs t-test is applied; otherwise, the
Wilcoxon rank-sum test (<code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>) is used.</p></li>
<li><p>For predictors with more than two levels, a model of Fisher‚Äôs
one-way analysis of variables (ANOVA) (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) is initially
fitted.<!-- The normality of residuals is evaluated by the  the Shapiro--Wilk test (`shapiro.test()`) and the Anderson-Darling test (`ad.test()`); residuals are considered approximately normal if at least one of the two tests yields a result exceeding the significance threshold $\alpha$. -->
The normality of residuals is evaluated by the Shapiro-Wilk test
(<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>); residuals are considered approximately
normal if it yields a result exceeding the significance threshold
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
If this condition is met, the Levene‚ÄìBrown‚ÄìForsythe test (implemented as
<code>levene.test()</code>) <span class="citation">(<a href="#ref-Brown:1974">Brown and Forsythe 1974</a>)</span> assesses
homoscedasticity. When variances are homogeneous
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>),
Fisher‚Äôs one-way ANOVA (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) is applied with Tukey‚Äôs
Honestly Significant Differences (HSD) (<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>) for
post-hoc comparison. If variances differ significantly
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>‚â§</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">p \le \alpha</annotation></semantics></math>),
Welch‚Äôs heteroscedastic one-way ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) is
used, also followed by Tukey‚Äôs HSD. If residuals are not normally
distributed according to both tests
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>‚â§</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">p \le \alpha</annotation></semantics></math>),
the Kruskal‚ÄìWallis test (<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>) is selected,
followed by pairwise Wilcoxon tests
(<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code>). A graphical overview of the
decision logic used is provided in the figure below.</p></li>
</ul>
<div style="border: 1px solid #666; padding: 10px; display: inline-block; text-align: center;">
<img src="figures/decision_tree.png" width="100%" alt="Decision tree used to select the appropriate statistical test."><p style="font-style: italic; font-size: 90%; margin-top: 0.5em;">
Decision tree used to select the appropriate statistical test for a
categorical predictor and numeric response, based on the number of
factor levels, normality, and homoscedasticity.
</p>
</div>
</div>
<div class="section level3">
<h3 id="both-variables-numeric-simple-linear-regression-pearson-or-spearman">Both variables numeric: Simple linear regression, Pearson or
Spearman<a class="anchor" aria-label="anchor" href="#both-variables-numeric-simple-linear-regression-pearson-or-spearman"></a>
</h3>
<div class="section level4">
<h4 id="causal-relationship-simple-linear-regression-lm">Causal relationship: Simple linear regression (Lm())<a class="anchor" aria-label="anchor" href="#causal-relationship-simple-linear-regression-lm"></a>
</h4>
<p>By default, <code><a href="../reference/visstat.html">visstat()</a></code> assumes a causal, linear
relationship between a numeric response and predictor and simple linear
regression model (<code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code>) is fitted and analysed in detail,
including residual diagnostics, formal tests, and the plotting of fitted
values with confidence bands. Note that <strong>only one</strong>
predictor variable is allowed, as the function is designed for
two-dimensional visualisation.</p>
</div>
<div class="section level4">
<h4 id="non-causal-relationship-correlation-analysis-pearson-or-spearman">Non-causal relationship: Correlation analysis (Pearson or
Spearman)<a class="anchor" aria-label="anchor" href="#non-causal-relationship-correlation-analysis-pearson-or-spearman"></a>
</h4>
<p>When no directional relationship is assumed (by flag
<code>do_regression = FALSE</code>) between two numeric variables,
<code><a href="../reference/visstat.html">visstat()</a></code> performs correlation analysis using either
Pearson‚Äôs product-moment correlation
(<code>cor.test(method = "pearson")</code>) for bivariate normal data or
Spearman‚Äôs rank correlation (<code>cor.test(method = "spearman")</code>)
as a non-parametric alternative. Spearman correlation operates on the
ranks of the data rather than the original values, making it robust to
outliers and non-normal distributions while detecting monotonic
relationships. The choice between methods is determined by applying the
Shapiro-Wilk test to both variables separately, with Pearson correlation
selected only when both variables pass normality testing at the
specified significance level.</p>
</div>
</div>
<div class="section level3">
<h3 id="both-variables-categorical-comparing-proportions">Both variables categorical: Comparing proportions<a class="anchor" aria-label="anchor" href="#both-variables-categorical-comparing-proportions"></a>
</h3>
<p>When both variables are categorical, no direction is assumed; the
order of variables in the function call does not affect the test
statistic, but it does influence the graphical output. For consistency,
we continue referring to the variables as <em>predictor</em> and
<em>response</em>.</p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis that the variables
are independent using either Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
test (<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>) or Fisher‚Äôs exact test
(<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>), depending on expected cell counts. The
choice of test is based on Cochran‚Äôs rule <span class="citation">(<a href="#ref-Cochran:1954">Cochran 1954</a>)</span>, which advises that
the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
approximation is reliable only if no expected cell count is less than 1
and no more than 20 percent of cells have expected counts below 5.</p>
</div>
</div>
<div class="section level2">
<h2 id="background-on-selected-tests-and-examples">Background on selected tests and examples<a class="anchor" aria-label="anchor" href="#background-on-selected-tests-and-examples"></a>
</h2>
<div class="section level3">
<h3 id="numeric-response-and-categorical-predictor-comparing-central-tendencies-1">Numeric response and categorical predictor: Comparing central
tendencies<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor-comparing-central-tendencies-1"></a>
</h3>
<p>When the predictor consists of <code>class</code>
‚Äú<code>factor</code>‚Äù with two or more levels and the response is of
<code>class</code> ‚Äú<code>numeric</code>‚Äù or ‚Äú<code>integer</code>‚Äù
(both having mode ‚Äú<code>numeric</code>‚Äù), statistical tests are applied
to compare the central tendencies across groups. This section describes
the conditions under which parametric and non-parametric tests are
chosen, based on the response type, the number of factor levels, and the
underlying distributional assumptions.</p>
<div class="section level4">
<h4 id="categorical-predictor-with-two-levels-welchs-t-test-and-wilcoxon-rank-sum">Categorical predictor with two levels: Welch‚Äôs t-test and Wilcoxon
rank-sum<a class="anchor" aria-label="anchor" href="#categorical-predictor-with-two-levels-welchs-t-test-and-wilcoxon-rank-sum"></a>
</h4>
<p>When the predictor variable has exactly two levels, Welch‚Äôs t-test or
the Wilcoxon rank-sum test is applied.</p>
<div class="section level5">
<h5 id="students-t-test-t-testvar-equal-true">Student‚Äôs t-test (<code>t.test(var.equal = TRUE)</code>)<a class="anchor" aria-label="anchor" href="#students-t-test-t-testvar-equal-true"></a>
</h5>
<p>Student‚Äôs t-test can be formulated as a special case of the General
Linear Model framework by using an indicator variable to represent group
membership. The independent t-test model uses two means to predict
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Y</mi><annotation encoding="application/x-tex">Y</annotation></semantics></math>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Y</mi><mi>i</mi></msub><mo>=</mo><msub><mi>Œ≤</mi><mn>0</mn></msub><mo>+</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><msub><mi>x</mi><mi>i</mi></msub><mo>+</mo><msub><mi>œµ</mi><mi>i</mi></msub><mspace width="1.0em"></mspace><msub><mi>H</mi><mn>0</mn></msub><mo>:</mo><msub><mi>Œ≤</mi><mn>1</mn></msub><mo>=</mo><mn>0</mn></mrow><annotation encoding="application/x-tex">Y_i = \beta_0 + \beta_1 x_i +\epsilon_i\quad H_0: \beta_1 = 0</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mi>i</mi></msub><annotation encoding="application/x-tex">x_i</annotation></semantics></math>
is an indicator (0 or 1) saying whether data point
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
was sampled from one or the other group.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mn>0</mn></msub><annotation encoding="application/x-tex">\beta_0</annotation></semantics></math>
is the mean of group 1, whereas
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ≤</mi><mn>1</mn></msub><annotation encoding="application/x-tex">\beta_1</annotation></semantics></math>
ist the mean difference between the groups.</p>
<p>The test statistic for Student‚Äôs t-test is given by:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mfrac><mrow><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>1</mn></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>2</mn></msub></mrow><mrow><msub><mi>s</mi><mi>p</mi></msub><msqrt><mrow><mfrac><mn>1</mn><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><mn>1</mn><msub><mi>n</mi><mn>2</mn></msub></mfrac></mrow></msqrt></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
t = \frac{\bar{x}_1 - \bar{x}_2}{s_p \sqrt{\frac{1}{n_1} + \frac{1}{n_2}}},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\bar{x}_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>2</mn></msub><annotation encoding="application/x-tex">\bar{x}_2</annotation></semantics></math>
are the sample means,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>
are the sample sizes, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>s</mi><mi>p</mi></msub><annotation encoding="application/x-tex">s_p</annotation></semantics></math>
is the pooled standard deviation:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>p</mi></msub><mo>=</mo><msqrt><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>2</mn></msub><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup></mrow><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub><mo>‚àí</mo><mn>2</mn></mrow></mfrac></msqrt><mo>,</mo></mrow><annotation encoding="application/x-tex">
s_p = \sqrt{\frac{(n_1 - 1)s_1^2 + (n_2 - 1)s_2^2}{n_1 + n_2 - 2}},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_1^2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_2^2</annotation></semantics></math>
are the sample variances in the two groups. The test statistic follows a
t-distribution with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ŒΩ</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub><mo>‚àí</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">\nu = n_1 + n_2 - 2</annotation></semantics></math>
degrees of freedom.</p>
</div>
<div class="section level5">
<h5 id="welchs-t-test-t-test">Welch‚Äôs t-test (<code>t.test()</code>)<a class="anchor" aria-label="anchor" href="#welchs-t-test-t-test"></a>
</h5>
<p>Welch‚Äôs t-test relaxes the homoscedasticity assumption of the GLM
while maintaining the requirements for independent observations and
normally distributed residuals. It evaluates the null hypothesis that
the means of two groups are equal without assuming equal variances.</p>
<p>The test statistic is given by <span class="citation">(<a href="#ref-Welch:1947">Welch 1947</a>; <a href="#ref-Satterthwaite:1946">Satterthwaite 1946</a>)</span></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>t</mi><mo>=</mo><mfrac><mrow><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>1</mn></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>2</mn></msub></mrow><msqrt><mrow><mfrac><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>2</mn></msub></mfrac></mrow></msqrt></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>1</mn></msub><annotation encoding="application/x-tex">\bar{x}_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mn>2</mn></msub><annotation encoding="application/x-tex">\bar{x}_2</annotation></semantics></math>
are the sample means,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_1^2</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_2^2</annotation></semantics></math>
the sample variances, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>
the sample sizes in the two groups. The statistic follows a
<em>t</em>-distribution with degrees of freedom approximated by the
Welch-Satterthwaite equation:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ŒΩ</mi><mo>‚âà</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>1</mn></msub></mfrac><mo>+</mo><mfrac><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><msub><mi>n</mi><mn>2</mn></msub></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mi>/</mi><msub><mi>n</mi><mn>1</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mi>/</mi><msub><mi>n</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>n</mi><mn>2</mn></msub><mo>‚àí</mo><mn>1</mn></mrow></mfrac></mrow></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\nu \approx \frac{
\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2
}{
\frac{(s_1^2 / n_1)^2}{n_1 - 1} + \frac{(s_2^2 / n_2)^2}{n_2 - 1}
}.
</annotation></semantics></math></p>
<p>The resulting p-value is computed from the <em>t</em>-distribution
with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>ŒΩ</mi><annotation encoding="application/x-tex">\nu</annotation></semantics></math>
degrees of freedom.</p>
</div>
<div class="section level5">
<h5 id="mathematical-relationship-when-welchs-reduces-to-students-t-test">Mathematical relationship: When Welch‚Äôs Reduces to Student‚Äôs
t-test<a class="anchor" aria-label="anchor" href="#mathematical-relationship-when-welchs-reduces-to-students-t-test"></a>
</h5>
<p>When the assumption of equal variances holds
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>=</mo><msubsup><mi>s</mi><mn>2</mn><mn>2</mn></msubsup><mo>=</mo><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">s_1^2 = s_2^2 = s^2</annotation></semantics></math>)
and sample sizes are equal
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><msub><mi>n</mi><mn>2</mn></msub><mo>=</mo><mi>n</mi></mrow><annotation encoding="application/x-tex">n_1 = n_2 = n</annotation></semantics></math>),
Welch‚Äôs t-test reduces to Student‚Äôs t-test.</p>
<p>Under these conditions the pooled variance becomes:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mi>p</mi><mn>2</mn></msubsup><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>s</mi><mn>2</mn></msup><mo>+</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><msup><mi>s</mi><mn>2</mn></msup></mrow><mrow><mn>2</mn><mi>n</mi><mo>‚àí</mo><mn>2</mn></mrow></mfrac><mo>=</mo><msup><mi>s</mi><mn>2</mn></msup></mrow><annotation encoding="application/x-tex">s_p^2 = \frac{(n-1)s^2 + (n-1)s^2}{2n-2} = s^2</annotation></semantics></math>,
whereas Welch‚Äôs denominator simplifies to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msqrt><mrow><mfrac><msup><mi>s</mi><mn>2</mn></msup><mi>n</mi></mfrac><mo>+</mo><mfrac><msup><mi>s</mi><mn>2</mn></msup><mi>n</mi></mfrac></mrow></msqrt><mo>=</mo><mi>s</mi><msqrt><mfrac><mn>2</mn><mi>n</mi></mfrac></msqrt></mrow><annotation encoding="application/x-tex">\sqrt{\frac{s^2}{n} + \frac{s^2}{n}} = s\sqrt{\frac{2}{n}}</annotation></semantics></math>.
Student‚Äôs denominator becomes:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>s</mi><mi>p</mi></msub><msqrt><mrow><mfrac><mn>1</mn><mi>n</mi></mfrac><mo>+</mo><mfrac><mn>1</mn><mi>n</mi></mfrac></mrow></msqrt><mo>=</mo><mi>s</mi><msqrt><mfrac><mn>2</mn><mi>n</mi></mfrac></msqrt></mrow><annotation encoding="application/x-tex">s_p \sqrt{\frac{1}{n} + \frac{1}{n}} = s\sqrt{\frac{2}{n}}</annotation></semantics></math></p>
<p>The Welch-Satterthwaite degrees of freedom reduce to:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>ŒΩ</mi><mo>=</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mfrac><mrow><mn>2</mn><msup><mi>s</mi><mn>2</mn></msup></mrow><mi>n</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>s</mi><mn>2</mn></msup><mi>/</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mo>+</mo><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msup><mi>s</mi><mn>2</mn></msup><mi>/</mi><mi>n</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac></mrow></mfrac><mo>=</mo><mfrac><mfrac><mrow><mn>4</mn><msup><mi>s</mi><mn>4</mn></msup></mrow><msup><mi>n</mi><mn>2</mn></msup></mfrac><mfrac><mrow><mn>2</mn><msup><mi>s</mi><mn>4</mn></msup></mrow><mrow><msup><mi>n</mi><mn>2</mn></msup><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac></mfrac><mo>=</mo><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mn>2</mn><mi>n</mi><mo>‚àí</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">
\nu = \frac{\left(\frac{2s^2}{n}\right)^2}{\frac{(s^2/n)^2}{n-1} + \frac{(s^2/n)^2}{n-1}} = \frac{\frac{4s^4}{n^2}}{\frac{2s^4}{n^2(n-1)}} = 2(n-1) = 2n-2
</annotation></semantics></math></p>
<p>Welch‚Äôs t-test remains valid and exhibits only minimal loss of power
even when the assumptions of Student‚Äôs t-test ‚Äì namely, normality and
equal variances of the response variable across groups ‚Äì are satisfied
<span class="citation">(<a href="#ref-Moser:1992">Moser and Stevens
1992</a>; <a href="#ref-Zimmerman:2004">Zimmerman 2004</a>; <a href="#ref-Delacre:2017">Delacre, Lakens, and Leys 2017</a>)</span>. It
is therefore the default implementation of the t-test in R and the
default comparison of means between two groups in
<code>visStatistics</code>.</p>
</div>
<div class="section level5">
<h5 id="wilcoxon-rank-sum-test-wilcox-test">Wilcoxon rank-sum test (<code>wilcox.test()</code>)<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test-wilcox-test"></a>
</h5>
<p>The two-sample Wilcoxon rank-sum test (also known as the Mann-Whitney
test) is a non-parametric alternative that does not require the response
variable to be approximately normally distributed within each group. It
tests for a difference in location between two independent distributions
<span class="citation">(<a href="#ref-Mann:1947">Mann and Whitney
1947</a>)</span>. If the two groups have distributions that are
sufficiently similar in shape and scale, the Wilcoxon rank-sum test can
be interpreted as testing whether the medians of the two populations are
equal <span class="citation">(<a href="#ref-Hollander:2014">Hollander,
Chicken, and Wolfe 2014</a>)</span>.</p>
<p>The two-level factor variable <code>x</code> defines two groups, with
sample sizes
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>2</mn></msub><annotation encoding="application/x-tex">n_2</annotation></semantics></math>.
All
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>=</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><msub><mi>n</mi><mn>2</mn></msub></mrow><annotation encoding="application/x-tex">N=n_1 + n_2</annotation></semantics></math>
observations are pooled and assigned ranks from
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mn>1</mn><annotation encoding="application/x-tex">1</annotation></semantics></math>
to
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>.
Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>W</mi><mn>1</mn></msub><annotation encoding="application/x-tex">W_1</annotation></semantics></math>
denote the sum of the ranks assigned to the group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mn>1</mn></msub><annotation encoding="application/x-tex">x_1</annotation></semantics></math>
corresponding to the first level of <code>x</code> containing
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mn>1</mn></msub><annotation encoding="application/x-tex">n_1</annotation></semantics></math>
observations:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>W</mi><mn>1</mn></msub><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mn>1</mn></msub></munderover><mi>R</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
W_{1}= \sum_{i=1}^{n_1} R(x_{1,i})</annotation></semantics></math>,</p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">R(x_{1,i})</annotation></semantics></math>
is the rank of observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mn>1</mn><mo>,</mo><mi>i</mi></mrow></msub><annotation encoding="application/x-tex">x_{1,i}</annotation></semantics></math>
in the pooled sample.</p>
<p>The test statistic
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>W</mi><annotation encoding="application/x-tex">W</annotation></semantics></math>
returned by <code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code> is then computed as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>W</mi><mo>=</mo><msub><mi>U</mi><mn>1</mn></msub><mo>=</mo><msub><mi>W</mi><mn>1</mn></msub><mo>‚àí</mo><mfrac><mrow><msub><mi>n</mi><mn>1</mn></msub><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>n</mi><mn>1</mn></msub><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mn>2</mn></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
W =U_{1}=W_{1} - \frac{n_1(n_1 + 1)}{2}.
</annotation></semantics></math> It corresponds to the Mann-Whitney
<span class="citation">(<a href="#ref-Mann:1947">Mann and Whitney
1947</a>)</span>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>U</mi><annotation encoding="application/x-tex">U</annotation></semantics></math>
statistic of the first group.</p>
<p>If both groups contain fewer than 50 observations and the data
contain no ties, the <em>p</em>-value is computed exactly. Otherwise, a
normal approximation with continuity correction is used.</p>
</div>
<div class="section level5">
<h5 id="graphical-output">Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output"></a>
</h5>
<!-- `visstat()` selects between Welch's t-test and the Wilcoxon rank-sum test
as follows. If both groups contain more than 30 observations, Welch's t-test is
always applied, relying on the central limit theorem to justify its application
regardless of underlying normality [@Rasch:2011; @Lumley:2002]. -->
<!-- If either group contains fewer than 30 observations, the Shapiro--Wilk test
(`shapiro.test()`) is applied separately to each group. Welch's t-test is used
if both tests do not reject normality at the significance level $\alpha$;
otherwise, the Wilcoxon rank-sum test is applied. -->
<p>Both tests show two plot panes, the first for checking the assumption
of normality in both groups, the second the actual data with the chosen
test.</p>
<p>Welch‚Äôs t-test does not fit a regression model, so there are no
‚Äúresiduals‚Äù in the regression sense. Therefore the function
<code>vis_ttest_assumptions()</code>generates histograms overlaid with
the normal distribution for both groups acompanied by the p-values of
<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code> and <code>ad.tset()</code>.</p>
<p>The graphical output of the second pane consists of box plots
overlaid with jittered points to display individual observations. When
Welch‚Äôs t-test is applied, the function includes confidence intervals
based on the user-specified <code>conf.level</code>.</p>
<!-- The title is structured as follows: -->
<!-- -   First line: Test name and chosen significance level $\alpha$. -->
<!-- -   Second line: Null hypotheses automatically adapted based on the user-specified response and grouping variable. -->
<!-- -   Third line: Test statistic, p-value and automated comparison with $\alpha$ -->
<p>The function returns a list containing the results of the applied
test and the summary statistics used to construct the plot.</p>
</div>
</div>
<div class="section level4">
<h4 id="examples">Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="welchs-t-test">Welch‚Äôs t-test<a class="anchor" aria-label="anchor" href="#welchs-t-test"></a>
</h5>
<p>The <em>Motor Trend Car Road Tests</em> dataset (<code>mtcars</code>)
contains 32 observations, where <code>mpg</code> denotes miles per (US)
gallon, and <code>am</code> represents the transmission type
(<code>0</code> = automatic, <code>1</code> = manual).</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span><span class="op">)</span></span>
<span><span class="va">t_test_statistics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span>, <span class="va">mtcars</span><span class="op">$</span><span class="va">mpg</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-1-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-1-2.png" class="r-plt" alt="" width="100%"></p>
<p>Increasing the confidence level <code>conf.level</code> from the
default 0.95 to 0.99 results in wider confidence intervals, as a higher
confidence level requires more conservative bounds to ensure that the
interval includes the true parameter value with greater certainty.</p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="wilcoxon-rank-sum-test">Wilcoxon rank sum test<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test"></a>
</h5>
<p>The Wilcoxon rank sum test is exemplified on differences between the
central tendencies of grades of ‚Äúboys‚Äù and ‚Äúgirls‚Äù in a class:</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">grades_gender</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  sex <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="st">"girl"</span>, <span class="fl">21</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="st">"boy"</span>, <span class="fl">23</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  grade <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="fl">19.3</span>, <span class="fl">18.1</span>, <span class="fl">15.2</span>, <span class="fl">18.3</span>, <span class="fl">7.9</span>, <span class="fl">6.2</span>, <span class="fl">19.4</span>,</span>
<span>    <span class="fl">20.3</span>, <span class="fl">9.3</span>, <span class="fl">11.3</span>, <span class="fl">18.2</span>, <span class="fl">17.5</span>, <span class="fl">10.2</span>, <span class="fl">20.1</span>, <span class="fl">13.3</span>, <span class="fl">17.2</span>, <span class="fl">15.1</span>, <span class="fl">16.2</span>, <span class="fl">17.0</span>,</span>
<span>    <span class="fl">16.5</span>, <span class="fl">5.1</span>, <span class="fl">15.3</span>, <span class="fl">17.1</span>, <span class="fl">14.8</span>, <span class="fl">15.4</span>, <span class="fl">14.4</span>, <span class="fl">7.5</span>, <span class="fl">15.5</span>, <span class="fl">6.0</span>, <span class="fl">17.4</span>,</span>
<span>    <span class="fl">7.3</span>, <span class="fl">14.3</span>, <span class="fl">13.5</span>, <span class="fl">8.0</span>, <span class="fl">19.5</span>, <span class="fl">13.4</span>, <span class="fl">17.9</span>, <span class="fl">17.7</span>, <span class="fl">16.4</span>, <span class="fl">15.6</span>, <span class="fl">17.3</span>, <span class="fl">19.9</span>, <span class="fl">4.4</span>, <span class="fl">2.1</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">wilcoxon_statistics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">grades_gender</span><span class="op">$</span><span class="va">sex</span>, <span class="va">grades_gender</span><span class="op">$</span><span class="va">grade</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-3-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-3-2.png" class="r-plt" alt="" width="100%"></p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="categorical-predictor-with-more-than-two-levels">Categorical predictor with more than two levels<a class="anchor" aria-label="anchor" href="#categorical-predictor-with-more-than-two-levels"></a>
</h3>
<p>If the predictor is of <code>class</code> ‚Äú<code>factor</code>‚Äù with
<strong>more than two levels</strong> and the response is of
<code>mode</code> ‚Äú<code>numeric</code>‚Äù, <code><a href="../reference/visstat.html">visstat()</a></code> either
performs <!-- Fisher‚Äôs one- way ANOVA [@Fisher:1935] (`aov()`), -->
Welch‚Äôs heteroscedastic one-way ANOVA <span class="citation">(<a href="#ref-Welch:1951">Welch 1951</a>)</span>
(<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) or, as a non-parametric alternative, the
Kruskal -Wallis test <span class="citation">(<a href="#ref-Kruskal:1952">Kruskal and Wallis 1952</a>)</span>
(<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>).</p>
<p>In the remainder of this section, we briefly introduce the tests
themselves, the assumption checks, and the post-hoc procedures, and
illustrate each test with an example.</p>
<div class="section level4">
<h4 id="fishers-one-way-anova-aov">Fisher‚Äôs one-way ANOVA (<code>aov()</code>)<a class="anchor" aria-label="anchor" href="#fishers-one-way-anova-aov"></a>
</h4>
<p>Fisher‚Äôs one-way ANOVA (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) tests the null hypothesis
that the means of multiple groups are equal. It assumes independent
observations, normally distributed residuals, and
<strong>homogeneous</strong> variances across groups. The test statistic
is the ratio of the variance explained by differences among group means
(between-group variance) to the unexplained variance within groups <span class="citation">(<a href="#ref-Fisher:1990">Ronald A. Fisher and Yates
1990</a>)</span></p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>=</mo><mfrac><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow></mfrac><mo>=</mo><mfrac><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>‚àí</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><mo>=</mo><mo>=</mo><mfrac><mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo>‚àí</mo><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><msub><mi>n</mi><mi>i</mi></msub></munderover><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup></mrow><mrow><mi>N</mi><mo>‚àí</mo><mi>k</mi></mrow></mfrac></mfrac></mrow><annotation encoding="application/x-tex">F  = \frac{MS_{between}}{MS_{within}}=
\frac{SS_{between}/(k-1)}{SS_{within}/(N-k)}== \frac{\frac{\sum_{i=1}^{k} n_i (\bar{x}_i - \bar{x})^2}{k - 1}}
{\frac{\sum_{i=1}^{k}\sum_{j=1}^{n_i}(x_{ij}-\bar{x}_i)^2}{N - k}}</annotation></semantics></math></p>
<p>where: - where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">MS_{between}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">MS_{within}</annotation></semantics></math>
are the mean square between groups and mean Square within groups
respectively. -
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>b</mi><mi>e</mi><mi>t</mi><mi>w</mi><mi>e</mi><mi>e</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{between}</annotation></semantics></math>
= Sum of Squares between groups (variance due to group differences) -
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>S</mi><msub><mi>S</mi><mrow><mi>w</mi><mi>i</mi><mi>t</mi><mi>h</mi><mi>i</mi><mi>n</mi></mrow></msub></mrow><annotation encoding="application/x-tex">SS_{within}</annotation></semantics></math>
= Sum of Squares within groups (error variance)<br>
-
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
= number of groups -
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
= total sample size</p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\bar{x}_i</annotation></semantics></math>
is the mean of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><annotation encoding="application/x-tex">\bar{x}</annotation></semantics></math>
is the overall mean,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>x</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">x_{ij}</annotation></semantics></math>
is the observation
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the total number of observations.</p>
<p>Under the null hypothesis, this statistic follows an F-distribution
with two parameters for degrees of freedom:
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>)
and
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>N</mi><mo>‚àí</mo><mi>k</mi></mrow><annotation encoding="application/x-tex">N - k</annotation></semantics></math>):
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>F</mi><mo>‚àº</mo><mi>F</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo>,</mo><mi>N</mi><mo>‚àí</mo><mi>k</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">F \sim F(k-1, N-k)</annotation></semantics></math>
The resulting p-value is computed from this distribution.</p>
</div>
<div class="section level4">
<h4 id="welchs-heteroscedastic-one-way-anova-oneway-test">Welch‚Äôs heteroscedastic one-way ANOVA
(<code>oneway.test()</code>)<a class="anchor" aria-label="anchor" href="#welchs-heteroscedastic-one-way-anova-oneway-test"></a>
</h4>
<p>When only the assumptions of independent observations and normally
distributed residuals are met, but <em>homogeneous variances</em> across
groups <em>cannot be assumed</em>, Welch‚Äôs heteroscedastic one-way ANOVA
(<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) <span class="citation">(<a href="#ref-Welch:1951">Welch 1951</a>)</span> provides an alternative to
<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>. It compares group means using weights based on
sample sizes and variances. The degrees of freedom are adjusted using a
Satterthwaite-type approximation <span class="citation">(<a href="#ref-Satterthwaite:1946">Satterthwaite 1946</a>)</span>, resulting
in an F-statistic with non-integer degrees of freedom. The Welch
F-statistic is calculated as <span class="citation">(<a href="#ref-Welch:1951">Welch 1951</a>)</span>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>F</mi><mi>W</mi></msub><mo>=</mo><mfrac><mrow><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>w</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>y</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo>‚àí</mo><msub><mover><mi>y</mi><mo accent="true">‚Äæ</mo></mover><mi>w</mi></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mi>/</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mn>1</mn><mo>+</mo><mfrac><mrow><mn>2</mn><mrow><mo stretchy="true" form="prefix">(</mo><mi>k</mi><mo>‚àí</mo><mn>2</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msup><mi>k</mi><mn>2</mn></msup><mo>‚àí</mo><mn>1</mn></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>w</mi><mi>i</mi></msub><mi>/</mi><mi>w</mi><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mrow><msub><mi>n</mi><mi>i</mi></msub><mo>‚àí</mo><mn>1</mn></mrow></mfrac></mrow></mfrac></mrow><annotation encoding="application/x-tex">
F_W = \frac{\sum_{i=1}^{k} w_i (\bar{y}_i - \bar{y}_w)^2 / (k-1)}{1 + \frac{2(k-2)}{k^2-1} \sum_{i=1}^{k} \frac{(1-w_i/w)^2}{n_i-1}}
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>w</mi><mi>i</mi></msub><mo>=</mo><msub><mi>n</mi><mi>i</mi></msub><mi>/</mi><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">w_i = n_i/s_i^2</annotation></semantics></math>
are the weights (inverse variances),
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>w</mi><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub></mrow><annotation encoding="application/x-tex">w = \sum_{i=1}^{k} w_i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mover><mi>y</mi><mo accent="true">‚Äæ</mo></mover><mi>w</mi></msub><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></msubsup><msub><mi>w</mi><mi>i</mi></msub><msub><mover><mi>y</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mi>/</mi><mi>w</mi></mrow><annotation encoding="application/x-tex">\bar{y}_w = \sum_{i=1}^{k} w_i \bar{y}_i / w</annotation></semantics></math>
is the weighted grand mean,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msubsup><mi>s</mi><mi>i</mi><mn>2</mn></msubsup><annotation encoding="application/x-tex">s_i^2</annotation></semantics></math>
is the variance of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>.</p>
<p>Simulation studies have demonstrated that Welch‚Äôs ANOVA maintains
appropriate Type I error rates when variances are unequal, outperforming
the classical ANOVA in such scenarios <span class="citation">(<a href="#ref-Delacre:2017">Delacre, Lakens, and Leys 2017</a>; <a href="#ref-Zimmerman:2004">Zimmerman 2004</a>)</span>.</p>
<p>Welch methods avoid the inflated Type I error rates caused by
preliminary variance testing <span class="citation">(<a href="#ref-Zimmerman:2004">Zimmerman 2004</a>)</span> while maintaining
comparable power even when variances are equal <span class="citation">(<a href="#ref-Delacre:2017">Delacre, Lakens, and Leys
2017</a>)</span>. It is therefore the default implementation of one-way
ANOVA in <code><a href="../reference/visstat.html">visstat()</a></code> .</p>
<div class="section level5">
<h5 id="connection-to-students-one-way-anova">Connection to Student‚Äôs one-way ANOVA<a class="anchor" aria-label="anchor" href="#connection-to-students-one-way-anova"></a>
</h5>
<p>In the two-sample setting, Welch‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test
reduces exactly to Student‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>t</mi><annotation encoding="application/x-tex">t</annotation></semantics></math>-test
when variances and sample sizes are equal. This exact equivalence does
not extend to the multi-group case: Even under equal variances
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>s</mi><mn>1</mn><mn>2</mn></msubsup><mo>=</mo><mi>‚ãØ</mi><mo>=</mo><msubsup><mi>s</mi><mi>k</mi><mn>2</mn></msubsup></mrow><annotation encoding="application/x-tex">s_1^2 = \cdots = s_k^2</annotation></semantics></math>)
and equal sample sizes
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>n</mi><mn>1</mn></msub><mo>=</mo><mi>‚ãØ</mi><mo>=</mo><msub><mi>n</mi><mi>k</mi></msub></mrow><annotation encoding="application/x-tex">n_1 = \cdots = n_k</annotation></semantics></math>),
the resulting test statistic is not algebraically identical to the
classical ANOVA
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>-statistic.
Nevertheless, under these conditions the Welch statistic converges to
the classical
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>-statistic,
and any numerical differences become negligible in practice. <span class="citation">(<a href="#ref-Welch:1951">Welch 1951</a>)</span>.</p>
</div>
</div>
<div class="section level4">
<h4 id="kruskalwallis-test-kruskal-test">Kruskal‚ÄìWallis test (<code>kruskal.test()</code>)<a class="anchor" aria-label="anchor" href="#kruskalwallis-test-kruskal-test"></a>
</h4>
<p>When the assumption of normality is not met, the Kruskal‚ÄìWallis test
provides a non-parametric alternative. It compares group distributions
based on ranked values and tests the null hypothesis that the groups
come from the same population ‚Äî specifically, that the distributions
have the same location <span class="citation">(<a href="#ref-Kruskal:1952">Kruskal and Wallis 1952</a>)</span>. If the
group distributions are sufficiently similar in shape and scale, then
the Kruskal‚ÄìWallis test can be interpreted as testing for equality of
medians across groups <span class="citation">(<a href="#ref-Hollander:2014">Hollander, Chicken, and Wolfe
2014</a>)</span>.</p>
<p>The test statistic is defined as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>H</mi><mo>=</mo><mfrac><mn>12</mn><mrow><mi>N</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>N</mi><mo>+</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow></mfrac><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>k</mi></munderover><msub><mi>n</mi><mi>i</mi></msub><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><mo>‚àí</mo><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><mo>,</mo></mrow><annotation encoding="application/x-tex">
H = \frac{12}{N(N+1)} \sum_{i=1}^{k} n_i \left(\bar{R}_i - \bar{R} \right)^2,
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>n</mi><mi>i</mi></msub><annotation encoding="application/x-tex">n_i</annotation></semantics></math>
is the sample size in group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>k</mi><annotation encoding="application/x-tex">k</annotation></semantics></math>
is the number of groups,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mi>i</mi></msub><annotation encoding="application/x-tex">\bar{R}_i</annotation></semantics></math>
is the average rank of group
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>N</mi><annotation encoding="application/x-tex">N</annotation></semantics></math>
is the total sample size, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mover><mi>R</mi><mo accent="true">‚Äæ</mo></mover><mo>=</mo><mfrac><mrow><mi>N</mi><mo>+</mo><mn>1</mn></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">\bar{R} = \frac{N+1}{2}</annotation></semantics></math>
is the average of all ranks. Under the null hypothesis,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>H</mi><annotation encoding="application/x-tex">H</annotation></semantics></math>
approximately follows a
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
distribution with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>k</mi><mo>‚àí</mo><mn>1</mn></mrow><annotation encoding="application/x-tex">k - 1</annotation></semantics></math>
degrees of freedom.</p>
</div>
<div class="section level4">
<h4 id="testing-the-assumptions-vis_glm_assumptions">Testing the assumptions (<code>vis_glm_assumptions()</code>)<a class="anchor" aria-label="anchor" href="#testing-the-assumptions-vis_glm_assumptions"></a>
</h4>
<p>The test logic for <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>
follows from their respective assumptions. <code><a href="../reference/visstat.html">visstat()</a></code>
initially models the data using <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and analyses the
residuals.</p>
<p>If both of the following conditions are met: (1) the standardised
residuals follow the standard normal distribution, and (2) the residuals
exhibit homoscedasticity (equal variances across groups), then the test
statistic from <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> is returned.</p>
<p>If only the normality assumption is satisfied, <code><a href="../reference/visstat.html">visstat()</a></code>
applies <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>. If the normality assumption is
violated, <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code> is used instead.</p>
<p>These assumptions are tested using the
<code>vis_glm_assumptions()</code> function.</p>
</div>
<div class="section level4">
<h4 id="controlling-the-family-wise-error-rate">Controlling the family-wise error rate<a class="anchor" aria-label="anchor" href="#controlling-the-family-wise-error-rate"></a>
</h4>
<p>ANOVA is an omnibus test that evaluates a single null hypothesis: all
group means are equal. If the null hypothesis gets rejected, we would
like to identify which specific groups differ significantly from each
other. However, simple pairwise comparisons of group means following an
ANOVA increases the probability of incorrectly declaring a significant
difference when, in fact, there is none.</p>
<p>This error is quantified by the family-wise error rate ‚Äúalpha per
family of tests‚Äù
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PF}</annotation></semantics></math>,
which refers to the probability of making at least one Type I error,
that is, falsely rejecting the null hypothesis across all pairwise
comparisons.</p>
<p>Given
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>n</mi><annotation encoding="application/x-tex">n</annotation></semantics></math>
levels of the categorical variable, there are</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>M</mi><mo>=</mo><mfrac><mrow><mi>n</mi><mo>‚ãÖ</mo><mrow><mo stretchy="true" form="prefix">(</mo><mi>n</mi><mo>‚àí</mo><mn>1</mn><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mn>2</mn></mfrac></mrow><annotation encoding="application/x-tex">
M = \frac{n \cdot (n - 1)}{2}
</annotation></semantics></math></p>
<p>pairwise comparisons possible, defining a <em>family of tests</em>
<span class="citation">(<a href="#ref-Abdi:2007">Abdi 2007</a>)</span>.
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
corresponds to the number of null hypotheses in the post hoc tests, each
testing whether the means of two specific groups are equal.</p>
</div>
<div class="section level4">
<h4 id="post-hoc-analysis">Post-hoc analysis<a class="anchor" aria-label="anchor" href="#post-hoc-analysis"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="≈°id√°k-correction">≈†id√°k correction<a class="anchor" aria-label="anchor" href="#%C5%A1id%C3%A1k-correction"></a>
</h5>
<p>If
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PT}</annotation></semantics></math>
(‚Äúalpha per test‚Äù) is the probability of making a Type I error in one
comparison, then
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>1</mn><mo>‚àí</mo><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub></mrow><annotation encoding="application/x-tex">1 - \alpha_{PT}</annotation></semantics></math>
is the probability of not making a Type I error in one comparison.</p>
<p>If all
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>M</mi><annotation encoding="application/x-tex">M</annotation></semantics></math>
comparisons are <strong>independent</strong> of each other, the
probability of making no Type I error across the entire family of
pairwise comparisons is
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>M</mi></msup><annotation encoding="application/x-tex">(1 -
\alpha_{PT})^M</annotation></semantics></math>. The family-wise error
rate is then given by its complement <span class="citation">(<a href="#ref-Abdi:2007">Abdi 2007</a>)</span>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>‚àí</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mi>M</mi></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\alpha_{PF} = 1 - (1 - \alpha_{PT})^M.
</annotation></semantics></math></p>
<!-- Let us illustrate the inflation of the family-wise error rate with increasing group number $n$ (equal to the number of `levels` in the categorical variable)) by the following examples: With $\alpha_{PT}=0.05$ pairwise comparison of  $n=3$ groups results in a family-wise error rate of $\alpha_{PF} \approx 14\%$, whereas pairwise comparing of $n=6$ groups (as in the examples below) already leads to the probability of at least one time falsely rejecting the null hypothesis of $\alpha_{PF} \approx 54\%$. -->
<p>Solving the last equation defining
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PF}</annotation></semantics></math>
for
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PT}</annotation></semantics></math>
yields the ≈†id√°k equation <span class="citation">(<a href="#ref-Sidak:1967">≈†id√°k 1967</a>)</span>:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>‚àí</mo><msup><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mn>1</mn><mi>/</mi><mi>M</mi></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">
\alpha_{PT}=1-(1-{\alpha_{PF}})^{1/M}.
</annotation></semantics></math> This shows that, in order to achieve a
given family-wise error rate, the corresponding per-test significance
level must be reduced when there are more than two groups.
<!-- ###### ≈†id√°k  correction in `visstat()` {.unnumbered} --></p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> sets
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>F</mi></mrow></msub><annotation encoding="application/x-tex">\alpha_{PF}</annotation></semantics></math>
to the user-defined
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>=</mo><mn>1</mn><mo>‚àí</mo></mrow><annotation encoding="application/x-tex">\alpha = 1 -</annotation></semantics></math><code>conf.level</code>, resulting in</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>1</mn><mo>‚àí</mo><msup><mtext mathvariant="monospace">ùöåùöòùöóùöè.ùöïùöéùöüùöéùöï</mtext><mrow><mn>1</mn><mi>/</mi><mi>M</mi></mrow></msup><mi>.</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 1 - \texttt{conf.level}^{1 / M}.
</annotation></semantics></math></p>
<p>With the default setting <code>conf.level = 0.95</code>, this leads
to:</p>
<ul>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>3</mn></mrow><annotation encoding="application/x-tex">n = 3</annotation></semantics></math>
groups:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>1.70</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 1.70\%</annotation></semantics></math>
</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>6</mn></mrow><annotation encoding="application/x-tex">n = 6</annotation></semantics></math>
groups:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>0.34</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 0.34\%</annotation></semantics></math>
</li>
<li>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mn>10</mn></mrow><annotation encoding="application/x-tex">n = 10</annotation></semantics></math>
groups:
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo>=</mo><mn>0.11</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\alpha_{PT} = 0.11\%</annotation></semantics></math>
</li>
</ul>
<p>These examples illustrate that the ≈†id√°k approach becomes
increasingly conservative as the number of comparisons grows. Moreover,
since the method assumes independence among tests, it may be overly
conservative when this assumption is violated.</p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="post-hoc-test-following-an-aov-tukeyhsd">Post-hoc test following an aov():
<code>TukeyHSD()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-an-aov-tukeyhsd"></a>
</h5>
<p>In contrast to the general-purpose ≈†id√°k correction, Tukey‚Äôs Honestly
Significant Differences procedure (<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>) is
specifically designed for pairwise mean comparisons following an ANOVA
(<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>). It controls the family-wise error rate using a
critical value from the studentised range distribution, which properly
accounts for the correlated nature of pairwise comparisons sharing a
common residual variance <span class="citation">(<a href="#ref-Hochberg:1987">Hochberg and Tamhane 1987</a>)</span>.</p>
<p>Based on the user-specified confidence level
(<code>conf.level</code>), <code><a href="../reference/visstat.html">visstat()</a></code> constructs confidence
intervals for all pairwise <strong>differences</strong> between factor
level means. A significant difference between two means is indicated
when the corresponding confidence interval does not include zero.
<code><a href="../reference/visstat.html">visstat()</a></code> returns both the HSD-adjusted p-values and the
associated confidence intervals for all pairwise comparisons.</p>
<p>For graphical display, <code><a href="../reference/visstat.html">visstat()</a></code> uses a dual approach:
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> provides the statistical test results and
significance determinations, while ≈†id√°k-corrected confidence intervals
around individual group means are shown for visualisation purposes. This
separation allows for optimal statistical testing while maintaining
clear, interpretable graphics.</p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="post-hoc-test-following-oneway-test-games-howell-test-games_howell_test">Post-hoc test following oneway.test():
Games-Howell-Test <code>games_howell_test ()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-oneway-test-games-howell-test-games_howell_test"></a>
</h5>
<p>In contrast to the general-purpose ≈†id√°k correction, the
Games-Howell-test procedure (<code>games_howel_test()</code>) is
specifically designed for pairwise mean comparisons following an
Welch-ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>). It controls the family-wise
error rate using a critical value from the studentised range
distribution, which properly accounts for the correlated nature of
pairwise comparisons. Unlike Tukey‚Äôs HSD, Games-Howell uses separate
variance estimates for each pairwise comparison and Welch-adjusted
degrees of freedom, making it appropriate when variances are unequal
<span class="citation">(<a href="#ref-Games:1976">Games and Howell
1976</a>)</span>.</p>
<p>Based on the user-specified confidence level
(<code>conf.level</code>), <code><a href="../reference/visstat.html">visstat()</a></code> constructs confidence
intervals for all pairwise <strong>differences</strong> between factor
level means. A significant difference between two means is indicated
when the corresponding confidence interval does not include zero.
<code><a href="../reference/visstat.html">visstat()</a></code> returns both the Games-Howell-adjusted p-values
and the associated confidence intervals for all pairwise
comparisons.</p>
<p>For graphical display, <code><a href="../reference/visstat.html">visstat()</a></code> uses a dual approach:
<code>games_howell_test()</code> provides the statistical test results
and significance determinations, while ≈†id√°k-corrected confidence
intervals around individual group means are shown for visualisation
purposes.
<!-- This separation allows for optimal statistical testing while maintaining clear, interpretable graphics. --></p>
</div>
<div class="section level5">
<h5 class="unnumbered" id="post-hoc-test-following-the-kruskalwallis-rank-sum-test-pairwise-wilcox-test">Post-hoc test following the Kruskal‚ÄìWallis rank
sum test: <code>pairwise.wilcox.test()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-the-kruskalwallis-rank-sum-test-pairwise-wilcox-test"></a>
</h5>
<p>As a post-hoc analysis following the Kruskal‚ÄìWallis test,
<code><a href="../reference/visstat.html">visstat()</a></code> applies the pairwise Wilcoxon rank sum test using
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> to compare each pair of factor
levels (see section ‚ÄúWilcoxon rank-sum test
(<code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>)‚Äù).</p>
<p>The resulting p-values from all pairwise comparisons are then
adjusted for multiple testing using Holm‚Äôs method <span class="citation">(<a href="#ref-Holm:1979">Holm 1979</a>)</span>: The
p-values are first sorted from smallest to largest and tested against
thresholds that become less strict as their rank increases. This
stepwise adjustment does not assume independence among tests and is
typically less conservative than the ≈†id√°k method, while still ensuring
strong control of the family-wise error rate.</p>
</div>
</div>
<div class="section level4">
<h4 id="graphical-output-1">Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output-1"></a>
</h4>
<p>The graphical output for all tests based on a numeric response and a
categorical predictor with more than two levels consists of two panels:
the first focuses on the residual analysis, the second on the actual
test chosen by the decision logic.</p>
<p>The residual panel addresses the assumption of normality, both
graphically and through formal tests. It displays a scatter plot of the
standardised residuals versus the predicted values, as well as a normal
Q‚ÄìQ plot comparing the sample quantiles to the theoretical quantiles. If
the residuals are normally distributed, no more than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>5</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">5\%</annotation></semantics></math>
of the standardised residuals should exceed approximately
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo stretchy="true" form="prefix">|</mo><mn>2</mn><mo stretchy="true" form="postfix">|</mo></mrow><annotation encoding="application/x-tex">|2|</annotation></semantics></math>;
in the Q‚ÄìQ plot the data points should approximately follow the red
straight line.</p>
<p>The p-values of the formal tests for normality (Shapiro‚ÄìWilk and
Anderson‚ÄìDarling) as well as the tests for homoscedasticity (Bartlett‚Äôs
and Levene Brown‚ÄìForsythe) are given in the title.
<!-- To assume normality of residuals, the formal tests for normality (Shapiro--Wilk and Anderson--Darling) should result in p-values greater than the user-defined $\alpha$. --></p>
<!-- If normality of the residuals can be assumed, Bartlett's test can be assesed to check for homogenity of variances. Otherwise, the Levene Brown‚ÄìForsythe is a more robust test for homoscedasticity given deviations from the normality assumption of residuals  The p-values of the beforementioned tests are displayed in the title of the residual panel. -->
<p><code><a href="../reference/visstat.html">visstat()</a></code> then illustrates, in the subsequent graph,
either the <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>, the <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>,
or <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> result (see also Section ‚ÄúDecision logic‚Äù).</p>
<p>If neither normality of the residuals nor homogeneity of variances is
given, the <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code> is executed. The result is
illustrated using box plots alongside jittered data points, with the
title displaying the p-value from <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>.</p>
<p>Above each box plot, the number of observations per level is shown.
Different green letters below a pair of box plots indicate that the two
groups are considered significantly different based on Holm‚Äôs-adjusted
pairwise Wilcoxon rank sum test p-values smaller than
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>.
The letters are generated with the help of the function
<code>multcompLetters()</code> from the <code>multcompView</code>
package <span class="citation">(<a href="#ref-Graves:2024">Graves,
Piepho, and with help from Sundar Dorai-Raj 2024</a>)</span>).</p>
<p>If normality of the residuals can be assumed, a parametric test is
chosen: either <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>, if homoscedasticity is also assumed,
or <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> otherwise. <code><a href="../reference/visstat.html">visstat()</a></code> displays
the name of the test and the corresponding
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>F</mi><annotation encoding="application/x-tex">F</annotation></semantics></math>
and p-value in the title.</p>
<p>The graph shows both the <code>conf.level</code>
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mo>‚ãÖ</mo><mspace width="0.167em"></mspace><mn>100</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">\cdot\,100\%</annotation></semantics></math>
confidence intervals corresponding to the null hypothesis of ANOVA (all
group means are equal) and the ≈†id√°k-corrected
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mn>1</mn><mo>‚àí</mo><msub><mi>Œ±</mi><mrow><mi>P</mi><mi>T</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>‚ãÖ</mo><mn>100</mn><mi>%</mi></mrow><annotation encoding="application/x-tex">(1 - \alpha_{PT}) \cdot 100\%</annotation></semantics></math>
confidence intervals used in the post hoc analysis.</p>
<p>In <code><a href="../reference/visstat.html">visstat()</a></code>, the ≈†id√°k intervals are used only for
visualisation, as the underlying method assumes independent comparisons
and can become overly conservative when this assumption is violated.</p>
<p>The actual post hoc analysis should be based on
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>. A significant test result between two groups is
graphically represented by different green letters below a pair of group
means.</p>
<p>Besides the graphical output, <code><a href="../reference/visstat.html">visstat()</a></code> returns a list
containing the relevant test statistics along with the corresponding
post-hoc-adjusted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>p</mi><annotation encoding="application/x-tex">p</annotation></semantics></math>-values
for all pairwise comparisons.</p>
</div>
<div class="section level4">
<h4 id="examples-1">Examples<a class="anchor" aria-label="anchor" href="#examples-1"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="anova">ANOVA<a class="anchor" aria-label="anchor" href="#anova"></a>
</h5>
<p>The <code>npk</code> dataset reports the yield of peas (in pounds per
block) from an agricultural experiment conducted on six blocks. In this
experiment, the application of three different fertilisers ‚Äì nitrogen
(N), phosphate (P), and potassium (K) ‚Äì was varied systematically. Each
block received either none, one, two, or all three of the
fertilisers,</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">anova_npk</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">npk</span><span class="op">$</span><span class="va">block</span>,<span class="va">npk</span><span class="op">$</span><span class="va">yield</span>,conf.level<span class="op">=</span><span class="fl">0.90</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-4-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-4-2.png" class="r-plt" alt="" width="100%"></p>
<p>Normality of residuals is supported by graphical diagnostics
(histogram, scatter plot of standardised residuals, Q-Q plot) and formal
tests (Shapiro‚ÄìWilk and Anderson- Darling, both with
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>).homogeneity
of variances is not supported at the given confidence level by
<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>, but by <code>levene.test()</code>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>&gt;</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">p &gt; \alpha</annotation></semantics></math>).
The decision logic is based on <code>levene.test()</code> and triggers
<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>. Post-hoc analysis with <code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> shows
no significant yield differences between blocks, as all share the same
group label (e.g., all green letters).</p>
<!-- {.unnumbered} -->
<!-- The `InsectSprays` dataset reports insect counts from agricultural experimental units treated with six different insecticides. To stabilise the variance in counts, we apply a square root transformation to the response variable. -->
<!-- ```{r} -->
<!-- insect_sprays_tr <- InsectSprays -->
<!-- insect_sprays_tr$count_sqrt <- sqrt(InsectSprays$count) -->
<!-- test_statistic_anova=visstat(insect_sprays_tr$spray, insect_sprays_tr$count_sqrt) -->
<!-- # test_statistic_anova  -->
<!-- ``` -->
<!-- After the transformation, the homogeneity of variances can be assumed ($p> -->
<!-- \alpha$ as calculated with the `bartlett.test()`), and the test statistic and p-value of Fisher's one-way ANOVA`aov()` is displayed. -->
<!-- <!-- The `ToothGrowth` data set studies the effect of vitamin C dosage C (0.5, 1, and 2 mg/day) on the the tooth growth of 60 guinea pigs. -->
<!-- <!-- ```{r} -->
<!-- <!-- visstat(ToothGrowth$dose, ToothGrowth$len) -->
<!-- <!-- ``` -->
<!-- <!-- Again, the homogeneity of variances can be assumed ($p> -->
<!-- <!-- \alpha$ as calculated with the `bartlett.test()`) and the test statistic and p-value of Fisher's one-way ANOVA`aov()` is displayed. -->
</div>
<div class="section level5">
<h5 class="unnumbered" id="kruskalwallis-rank-sum-test">Kruskal‚ÄìWallis rank sum test<a class="anchor" aria-label="anchor" href="#kruskalwallis-rank-sum-test"></a>
</h5>
<p>The <code>iris</code> dataset contains petal width measurements (in
cm) for three different iris species.</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span>, <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-5-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-5-2.png" class="r-plt" alt="" width="100%"></p>
<p>In this example, scatter plots of the standardised residuals and the
Q-Q plot suggest that the residuals are not normally distributed. This
is confirmed by very small p-values from both the Shapiro‚ÄìWilk and
Anderson-Darling tests.</p>
<p>If both p-values are below the significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>Œ±</mi><annotation encoding="application/x-tex">\alpha</annotation></semantics></math>,
<code><a href="../reference/visstat.html">visstat()</a></code> switches to the non-parametric
<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>. Post-hoc analysis using
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> shows significant differences in
petal width between all three species, as indicated by distinct group
labels (all green letters differ).</p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="both-variables-numeric">Both variables numeric<a class="anchor" aria-label="anchor" href="#both-variables-numeric"></a>
</h3>
<div class="section level4">
<h4 id="simple-linear-regression-lm">Simple linear regression (<code>lm()</code>)<a class="anchor" aria-label="anchor" href="#simple-linear-regression-lm"></a>
</h4>
<p>If both the predictor and the response are numeric and contain only
one level each, <code><a href="../reference/visstat.html">visstat()</a></code> performs a simple linear
regression.</p>
<p>The resulting regression plot displays the point estimate of the
regression line</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>y</mi><mo>=</mo><msub><mi>b</mi><mn>0</mn></msub><mo>+</mo><msub><mi>b</mi><mn>1</mn></msub><mo>‚ãÖ</mo><mi>x</mi><mo>,</mo></mrow><annotation encoding="application/x-tex">
y = b_0 + b_1 \cdot x,
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>
is the response variable,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
is the predictor variable,
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mn>0</mn></msub><annotation encoding="application/x-tex">b_0</annotation></semantics></math>
is the intercept, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>b</mi><mn>1</mn></msub><annotation encoding="application/x-tex">b_1</annotation></semantics></math>
is the slope of the regression line.</p>
<div class="section level5">
<h5 id="residual-analysis">Residual analysis<a class="anchor" aria-label="anchor" href="#residual-analysis"></a>
</h5>
<p><code><a href="../reference/visstat.html">visstat()</a></code> checks the normality of the standardised
residuals from <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> both with diagnostic plots and using
the Shapiro‚ÄìWilk and Anderson-Darling tests. (via
<code>visAnovassumptions()</code>)</p>
<!-- If the p-values for the null hypothesis of normally distributed residuals from both tests are smaller than $1 -$`conf.int`, the title of the residual plot will display the message: "Requirement of normally distributed residuals not met". -->
<p>Note, that regardless of the result of the residual analysis,
<code><a href="../reference/visstat.html">visstat()</a></code> proceeds to perform the regression. The title of
the graphical output indicates the chosen confidence level
(<code>conf.level</code>), the estimated regression parameters with
their confidence intervals and p-values, and the adjusted
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>R</mi><mn>2</mn></msup><annotation encoding="application/x-tex">R^2</annotation></semantics></math>.
The plot displays the raw data, the fitted regression line, and both the
confidence and prediction bands corresponding to the specified
<code>conf.level</code>.</p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> returns a list containing the regression test
statistics, the p-values from the normality tests of the standardised
residuals, and the pointwise estimates of the confidence and prediction
bands.</p>
</div>
</div>
<div class="section level4">
<h4 id="examples-2">Examples<a class="anchor" aria-label="anchor" href="#examples-2"></a>
</h4>
<div class="section level5">
<h5 class="unnumbered" id="dataset-trees">dataset: `trees``<a class="anchor" aria-label="anchor" href="#dataset-trees"></a>
</h5>
<p>The <code>trees</code> data set contains the diameter
<code>Girth</code> in inches and <code>Volume</code> in cubic ft of 31
black cherry trees.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linreg_trees</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">trees</span><span class="op">$</span><span class="va">Girth</span>, <span class="va">trees</span><span class="op">$</span><span class="va">Volume</span>,conf.level<span class="op">=</span><span class="fl">0.9</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-6-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-6-2.png" class="r-plt" alt="" width="100%"></p>
<p>p-values greater than <code>conf.level</code> in both the
Anderson-Darling normality test and the Shapiro‚ÄìWilk test of the
standardised residuals indicate that the normality assumption of the
residuals underlying the linear regression is met.</p>
<p>Increasing the confidence level <code>conf.level</code> from the
default 0.9 to 0.99 results in wider confidence intervals of the
regression parameters as well as wider confidence and prediction
bands</p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linreg_trees_99</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">trees</span><span class="op">$</span><span class="va">Girth</span>, <span class="va">trees</span><span class="op">$</span><span class="va">Volume</span>,conf.level <span class="op">=</span> <span class="fl">0.99</span><span class="op">)</span></span></code></pre></div>
<p>The <code>visStatistics</code> plot allows to display only the second
generated plot (the assumption plot is unchanged):</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">linreg_trees_99</span>,which<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-8-1.png" class="r-plt" alt="" width="100%"></p>
</div>
</div>
<div class="section level4">
<h4 id="pearson-and-spearman-correlation-cor">Pearson and Spearman correlation (<code>cor()</code>)<a class="anchor" aria-label="anchor" href="#pearson-and-spearman-correlation-cor"></a>
</h4>
<p>If both variables are numeric and contain only one level each,
<code><a href="../reference/visstat.html">visstat()</a></code> computes correlation coefficients to measure the
strength and direction of their linear or monotonic relationship. The
Pearson correlation coefficient
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>r</mi><mo>=</mo><mfrac><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><msub><mi>œÉ</mi><mi>x</mi></msub><mo>‚ãÖ</mo><msub><mi>œÉ</mi><mi>y</mi></msub></mrow></mfrac></mrow><annotation encoding="application/x-tex">
r = \frac{\text{Cov}(x,y)}{\sigma_x \cdot \sigma_y}
</annotation></semantics></math> quantifies the linear association
between variables
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mtext mathvariant="normal">Cov</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo>,</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mn>1</mn><mrow><mi>n</mi><mo>‚àí</mo><mn>1</mn></mrow></mfrac><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>n</mi></msubsup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>x</mi><mi>i</mi></msub><mo>‚àí</mo><mover><mi>x</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>y</mi><mi>i</mi></msub><mo>‚àí</mo><mover><mi>y</mi><mo accent="true">‚Äæ</mo></mover><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">\text{Cov}(x,y) = \frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})(y_i - \bar{y})</annotation></semantics></math>
is the sample covariance measuring how the variables vary together, and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÉ</mi><mi>x</mi></msub><annotation encoding="application/x-tex">\sigma_x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>œÉ</mi><mi>y</mi></msub><annotation encoding="application/x-tex">\sigma_y</annotation></semantics></math>
are the standard deviations representing the spread of each variable
individually, while the Spearman correlation coefficient
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>œÅ</mi><mo>=</mo><mi>r</mi><mrow><mo stretchy="true" form="prefix">(</mo><mtext mathvariant="normal">rank</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>x</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo>,</mo><mtext mathvariant="normal">rank</mtext><mrow><mo stretchy="true" form="prefix">(</mo><mi>y</mi><mo stretchy="true" form="postfix">)</mo></mrow><mo stretchy="true" form="postfix">)</mo></mrow></mrow><annotation encoding="application/x-tex">
\rho = r(\text{rank}(x), \text{rank}(y))
</annotation></semantics></math> applies the Pearson correlation formula
to the ranked values of
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>x</mi><annotation encoding="application/x-tex">x</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>y</mi><annotation encoding="application/x-tex">y</annotation></semantics></math>,
measuring monotonic relationships by calculating the linear correlation
between the rank-transformed variables rather than the original data
values.</p>
</div>
<div class="section level4">
<h4 id="examples-3">Examples<a class="anchor" aria-label="anchor" href="#examples-3"></a>
</h4>
<p>Here, we use the <code>swiss</code> data set, standardised fertility
measure and socioeconomic indicators for each of 47 French-speaking
provinces of Switzerland at about 1888. Both fertility and education
level are given in percentage points.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">result_swiss1</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">swiss</span><span class="op">$</span><span class="va">Fertility</span>,</span>
<span>                             <span class="va">swiss</span><span class="op">$</span><span class="va">Education</span>,do_regression<span class="op">=</span><span class="cn">FALSE</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-9-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-9-2.png" class="r-plt" alt="" width="100%"></p>
<!-- #### dataset: `cars`{.unnumbered} -->
<!-- The `cars` dataset reports the speed of 50 cars in miles per hour (`speed`) and the stopping distance in feet (`dist`) recorded in  the 1920s. In this example the assuptions of normally distributed residuals is not met. -->
<!-- ```{r} -->
<!-- linreg_cars <- visstat(cars$speed, cars$dist) -->
<!-- ``` -->
</div>
</div>
<div class="section level3">
<h3 id="both-variables-categorical-comparing-proportions-1">Both variables categorical: Comparing proportions<a class="anchor" aria-label="anchor" href="#both-variables-categorical-comparing-proportions-1"></a>
</h3>
<p>When both variables are categorical (i.e., of class
<code>factor</code>), <code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis
that the two variables are independent. Observed frequencies are
typically arranged in a contingency table, where rows index the levels
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
of the response variable and columns index the levels
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
of the predictor variable.</p>
<div class="section level4">
<h4 id="pearsons-residuals-and-mosaic-plots">Pearson‚Äôs residuals and mosaic plots<a class="anchor" aria-label="anchor" href="#pearsons-residuals-and-mosaic-plots"></a>
</h4>
<p>Mosaic plots provide a graphical representation of contingency
tables, where the area of each tile is proportional to the observed cell
frequency. To aid interpretation, tiles are coloured based on Pearson
residuals from a chi-squared test of independence. These residuals
measure the standardised deviation of observed from expected counts
under the null hypothesis of independence.</p>
<p>Let
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">O_{ij}</annotation></semantics></math>
and
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><annotation encoding="application/x-tex">E_{ij}</annotation></semantics></math>
denote the observed and expected frequencies in row
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>i</mi><annotation encoding="application/x-tex">i</annotation></semantics></math>
and column
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>j</mi><annotation encoding="application/x-tex">j</annotation></semantics></math>
of an
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>√ó</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">R \times C</annotation></semantics></math>
contingency table. The Pearson residual for each cell is defined as</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>r</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>=</mo><mfrac><mrow><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><msqrt><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></msqrt></mfrac><mo>,</mo><mspace width="1.0em"></mspace><mi>i</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>R</mi><mo>,</mo><mspace width="1.0em"></mspace><mi>j</mi><mo>=</mo><mn>1</mn><mo>,</mo><mi>‚Ä¶</mi><mo>,</mo><mi>C</mi><mi>.</mi></mrow><annotation encoding="application/x-tex">
r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}}}, \quad i = 1, \ldots, R,\quad
j = 1, \ldots, C.
</annotation></semantics></math> Positive residuals (shaded in blue)
indicate observed counts greater than expected, while negative values
suggest under-representation (shaded in red). Colour shading thus
highlights which combinations of categorical levels contribute most to
the overall association.</p>
</div>
<div class="section level4">
<h4 id="pearsons-chi2-test-chisq-test">Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-test
(<code>chisq.test()</code>)<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-chisq-test"></a>
</h4>
<p>The test statistic of Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-test
<span class="citation">(<a href="#ref-Pearson:1900">Pearson
1900</a>)</span> is the sum of squared Pearson residuals:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msup><mi>œá</mi><mn>2</mn></msup><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><msubsup><mi>r</mi><mrow><mi>i</mi><mi>j</mi></mrow><mn>2</mn></msubsup><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></munderover><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\chi^2 = \sum_{i=1}^{R} \sum_{j=1}^{C} r_{ij}^2 =
\sum_{i=1}^{R} \sum_{j=1}^{C} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}.
</annotation></semantics></math></p>
<p>The test statistic is compared to the chi-squared distribution with $
(R - 1)(C - 1)$ degrees of freedom. The resulting p-value corresponds to
the upper tail probability ‚Äî that is, the probability of observing a
value greater than or equal to the test statistic under the null
hypothesis.</p>
</div>
<div class="section level4">
<h4 id="pearsons-chi2-test-with-yates-continuity-correction">Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
test with Yates‚Äô continuity correction<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-with-yates-continuity-correction"></a>
</h4>
<p>Yates‚Äô correction is applied to the Pearson
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
statistic in
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>√ó</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math>
contingency tables (with one degree of freedom). In this case, the
approximation of the discrete sampling distribution by the continuous
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
distribution tends to overestimate the significance level of the test.
To correct for this, Yates proposed subtracting 0.5 from each absolute
difference between observed and expected counts <span class="citation">(<a href="#ref-Yates:1934">Yates 1934</a>)</span>,
resulting in a smaller test statistic:
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msubsup><mi>œá</mi><mtext mathvariant="normal">Yates</mtext><mn>2</mn></msubsup><mo>=</mo><munderover><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><munderover><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mn>2</mn></munderover><mfrac><msup><mrow><mo stretchy="true" form="prefix">(</mo><mrow><mo stretchy="true" form="prefix">|</mo><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo>‚àí</mo><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub><mo stretchy="true" form="postfix">|</mo></mrow><mo>‚àí</mo><mn>0.5</mn><mo stretchy="true" form="postfix">)</mo></mrow><mn>2</mn></msup><msub><mi>E</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mfrac><mi>.</mi></mrow><annotation encoding="application/x-tex">
\chi^2_{\text{Yates}} = \sum_{i=1}^{2} \sum_{j=1}^{2}
\frac{(|O_{ij} - E_{ij}| - 0.5)^2}{E_{ij}}.
</annotation></semantics></math></p>
<p>This reduced test statistic yields a larger p-value, thereby lowering
the risk of a Type I error.</p>
<p>Yates‚Äô continuity correction is applied by default by the underlying
routine <code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>.</p>
</div>
<div class="section level4">
<h4 id="fishers-exact-test-fisher-test">Fisher‚Äôs exact test (<code>fisher.test()</code>)<a class="anchor" aria-label="anchor" href="#fishers-exact-test-fisher-test"></a>
</h4>
<p>The
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
approximation is considered reliable only if no expected cell count is
less than 1 and no more than 20 percent of cells have expected counts
below 5 <span class="citation">(<a href="#ref-Cochran:1954">Cochran
1954</a>)</span>). If this condition is not met, Fisher‚Äôs exact test
<span class="citation">(<a href="#ref-Fisher:1970">Ronald Aylmer Fisher
1970</a>)</span> (<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>) is applied instead, as it
is a non-parametric method that does not rely on large-sample
approximations. The test calculates an exact p-value for testing
independence by conditioning on the observed margins: the row totals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>R</mi><mi>i</mi></msub><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>j</mi><mo>=</mo><mn>1</mn></mrow><mi>C</mi></msubsup><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">R_i = \sum_{j=1}^C O_{ij}</annotation></semantics></math>
and the column totals
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><msub><mi>C</mi><mi>j</mi></msub><mo>=</mo><msubsup><mo>‚àë</mo><mrow><mi>i</mi><mo>=</mo><mn>1</mn></mrow><mi>R</mi></msubsup><msub><mi>O</mi><mrow><mi>i</mi><mi>j</mi></mrow></msub></mrow><annotation encoding="application/x-tex">C_j = \sum_{i=1}^R O_{ij}</annotation></semantics></math>,
defining the structure of the contingency table.</p>
<p>In the
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>√ó</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math>
case, the observed table can be written as:</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mtable><mtr><mtd columnalign="center" style="text-align: center"></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>C</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><msub><mi>C</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">Row sums</mtext></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mn>1</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi><mo>+</mo><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><msub><mi>R</mi><mn>2</mn></msub></mtd><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>c</mi><mo>+</mo><mi>d</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mtext mathvariant="normal">Column sums</mtext></mtd><mtd columnalign="center" style="text-align: center"><mi>a</mi><mo>+</mo><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi><mo>+</mo><mi>d</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>n</mi></mtd></mtr></mtable><annotation encoding="application/x-tex">
\begin{array}{c|cc|c}
&amp; C_1 &amp; C_2 &amp; \text{Row sums} \\\\
\hline
R_1 &amp; a &amp; b &amp; a + b \\\\
R_2 &amp; c &amp; d &amp; c + d \\\\
\hline
\text{Column sums} &amp; a + c &amp; b + d &amp; n
\end{array}
</annotation></semantics></math></p>
<p>Let
<math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>O</mi><mo>=</mo><mrow><mo stretchy="true" form="prefix">[</mo><mtable><mtr><mtd columnalign="center" style="text-align: center"><mi>a</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>b</mi></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"></mtd></mtr><mtr><mtd columnalign="center" style="text-align: center"><mi>c</mi></mtd><mtd columnalign="center" style="text-align: center"><mi>d</mi></mtd></mtr></mtable><mo stretchy="true" form="postfix">]</mo></mrow></mrow><annotation encoding="application/x-tex">
O = \begin{bmatrix} a &amp; b \\\\ c &amp; d \end{bmatrix}
</annotation></semantics></math> denote the above observed
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mn>2</mn><mo>√ó</mo><mn>2</mn></mrow><annotation encoding="application/x-tex">2 \times 2</annotation></semantics></math>
contingency table. The exact probability of observing this table under
the null hypothesis of independence, given the fixed margins, is given
by the hypergeometric probability mass function (PMF)</p>
<p><math display="block" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>‚Ñô</mi><mrow><mo stretchy="true" form="prefix">(</mo><mi>O</mi><mo>‚à£</mo><msub><mi>R</mi><mn>1</mn></msub><mo>,</mo><msub><mi>R</mi><mn>2</mn></msub><mo>,</mo><msub><mi>C</mi><mn>1</mn></msub><mo>,</mo><msub><mi>C</mi><mn>2</mn></msub><mo stretchy="true" form="postfix">)</mo></mrow><mo>=</mo><mfrac><mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mi>a</mi><mo>+</mo><mi>b</mi></mrow><mi>a</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mrow><mi>c</mi><mo>+</mo><mi>d</mi></mrow><mi>c</mi></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mrow><mrow><mo stretchy="true" form="prefix">(</mo><mfrac linethickness="0"><mi>n</mi><mrow><mi>a</mi><mo>+</mo><mi>c</mi></mrow></mfrac><mo stretchy="true" form="postfix">)</mo></mrow></mfrac><mo>,</mo></mrow><annotation encoding="application/x-tex">
\mathbb{P}(O \mid
R_1, R_2, C_1, C_2) =
\frac{\binom{a + b}{a} \binom{c + d}{c}}{\binom{n}{a + c}},
</annotation></semantics></math></p>
<p>where
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>=</mo><mi>a</mi><mo>+</mo><mi>b</mi><mo>+</mo><mi>c</mi><mo>+</mo><mi>d</mi></mrow><annotation encoding="application/x-tex">n = a + b + c + d</annotation></semantics></math>
is the total sample size.</p>
<!-- Because all other cell values are determined by $a$ and the fixed margins,
-->
<!-- this is often written more simply as: -->
<!-- $$ -->
<!-- P(a \mid \text{margins}) = -->
<!-- \frac{\binom{a + b}{a} \binom{c + d}{c}}{\binom{n}{a + c}}. -->
<!-- $$ -->
<p>The p-value is computed by summing the probabilities of all tables
with the same margins whose probabilities under the null are less than
or equal to that of the observed table.</p>
<p>For general
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>R</mi><mo>√ó</mo><mi>C</mi></mrow><annotation encoding="application/x-tex">R \times C</annotation></semantics></math>
tables, <code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code> generalises this approach using the
multivariate hypergeometric distribution.</p>
</div>
<div class="section level4">
<h4 id="test-choice-and-graphical-output">Test choice and graphical output<a class="anchor" aria-label="anchor" href="#test-choice-and-graphical-output"></a>
</h4>
<p>If the expected frequencies are sufficiently large - specifically, if
at least 80% of the cells have expected counts greater than 5 and no
expected count is smaller than 1, the function uses Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
(<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>).</p>
<p>Otherwise, it switches to Fisher‚Äôs exact test
(<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>) <span class="citation">(<a href="#ref-Cochran:1954">Cochran 1954</a>)</span>.</p>
<p>For 2-by-2 contingency tables, Yates‚Äô continuity correction <span class="citation">(<a href="#ref-Yates:1934">Yates 1934</a>)</span> is
always applied to Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test.</p>
<p>For all tests of independence <code><a href="../reference/visstat.html">visstat()</a></code> displays a
grouped column plot that includes the respective test‚Äôs p-value in the
title, as well as a mosaic plot showing colour-coded Pearson residuals
and the p-value of Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>-test.</p>
</div>
<div class="section level4">
<h4 id="transforming-a-contingency-table-to-a-data-frame">Transforming a contingency table to a data frame<a class="anchor" aria-label="anchor" href="#transforming-a-contingency-table-to-a-data-frame"></a>
</h4>
<p>The following examples for tests of categorical predictor and
response are all based on the <code>HairEyeColor</code> contingency
table.</p>
<p>Contingency tables must be converted to the required column-based
<code>data.frame</code> using the helper function
<code><a href="../reference/counts_to_cases.html">counts_to_cases()</a></code>. The function transforms the contingency
table <code>HairEyeColor</code> into <code>data.frame</code> named
<code>HairEyeColourDataFrame</code>.</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">HairEyeColourDataFrame</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">HairEyeColor</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level4">
<h4 id="examples-4">Examples<a class="anchor" aria-label="anchor" href="#examples-4"></a>
</h4>
<p>In all examples of this section, we will test the null hypothesis
that hair colour (‚ÄúHair‚Äù) and eye colour (‚ÄúEye‚Äù) are independent of each
other.</p>
<div class="section level5">
<h5 id="pearsons-chi2-test-chisq-test-1">Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
(<code>chisq.test()</code>)<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-chisq-test-1"></a>
</h5>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_eye_colour_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">HairEyeColor</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">hair_eye_colour_df</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">hair_eye_colour_df</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-11-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-11-2.png" class="r-plt" alt="" width="100%"></p>
<p>The graphical output shows that the null hypothesis of Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">\chi^2</annotation></semantics></math>
test ‚Äì namely, that hair colour and eye colour are independent ‚Äì must be
rejected at the default significance level
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>Œ±</mi><mo>=</mo><mn>0.05</mn></mrow><annotation encoding="application/x-tex">\alpha=0.05</annotation></semantics></math>
(<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>p</mi><mo>=</mo><mn>2.33</mn><mo>‚ãÖ</mo><msup><mn>10</mn><mrow><mo>‚àí</mo><mn>25</mn></mrow></msup><mo>&lt;</mo><mi>Œ±</mi></mrow><annotation encoding="application/x-tex">p = 2.33 \cdot 10^{-25} &lt;
\alpha</annotation></semantics></math>). The mosaic plot indicates that
the strongest deviations are due to over-representation of individuals
with black hair and brown eyes, and of those with blond hair and blue
eyes. In contrast, individuals with blond hair and brown eyes are the
most under-represented.</p>
</div>
<div class="section level5">
<h5 id="pearsons-chi2-test-with-yates-continuity-correction-1">Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
with Yate‚Äôs continuity correction<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-with-yates-continuity-correction-1"></a>
</h5>
<p>In the following example, we restrict the data to participants with
either black or brown hair and either brown or blue eyes, resulting in a
2-by-2 contingency table.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_black_brown_eyes_brown_blue</span> <span class="op">&lt;-</span> <span class="va">HairEyeColor</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="op">]</span></span>
<span><span class="co"># Transform to data frame</span></span>
<span><span class="va">hair_black_brown_eyes_brown_blue_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">hair_black_brown_eyes_brown_blue</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Chi-squared test</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">hair_black_brown_eyes_brown_blue_df</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">hair_black_brown_eyes_brown_blue_df</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-12-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-12-2.png" class="r-plt" alt="" width="100%"></p>
<p>Also in this reduced dataset we reject the null hypothesis of
independence of the hair colors ‚Äúbrown‚Äù and ‚Äúblack‚Äù from the eye colours
‚Äúbrown‚Äù and ‚Äù blue‚Äù. The mosaic plot shows that blue-eyed persons with
black hair are under- represented. Note the higher p-value of Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
with Yate‚Äôs continuity correction (p = 0.00354) compared to the p-value
of Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>-test
(p = 0.00229) shown in the mosaic plot.</p>
</div>
<div class="section level5">
<h5 id="fishers-exact-test-fisher-test-1">Fisher‚Äôs exact test (<code>fisher.test()</code>)<a class="anchor" aria-label="anchor" href="#fishers-exact-test-fisher-test-1"></a>
</h5>
<p>Again, we extract a 2-by-2 contingency table from the full dataset,
this time keeping only male participants with black or brown hair and
hazel or green eyes.</p>
<p>Pearson‚Äôs
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><msup><mi>œá</mi><mn>2</mn></msup><annotation encoding="application/x-tex">{\chi}^2</annotation></semantics></math>
test applied to this table would yield an expected frequency less than 5
in one of the four cells (25% of all cells), which violates the
requirement that at least 80% of the expected frequencies must be 5 or
greater <span class="citation">(<a href="#ref-Cochran:1954">Cochran
1954</a>)</span>.</p>
<p>Therefore, <code><a href="../reference/visstat.html">visstat()</a></code> automatically selects Fisher‚Äôs
exact test instead.</p>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_eye_colour_male</span> <span class="op">&lt;-</span> <span class="va">HairEyeColor</span><span class="op">[</span>, , <span class="fl">1</span><span class="op">]</span></span>
<span><span class="co"># Slice out a 2 by 2 contingency table</span></span>
<span><span class="va">black_brown_hazel_green_male</span> <span class="op">&lt;-</span> <span class="va">hair_eye_colour_male</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span>
<span><span class="co"># Transform to data frame</span></span>
<span><span class="va">black_brown_hazel_green_male</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Fisher test</span></span>
<span><span class="va">fisher_stats</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/fisher-data-prep-1.png" class="r-plt" alt="" width="100%"><img src="visStatistics_files/figure-html/fisher-data-prep-2.png" class="r-plt" alt="" width="100%"></p>
</div>
</div>
</div>
<div class="section level3">
<h3 id="saving-the-graphical-output">Saving the graphical output<a class="anchor" aria-label="anchor" href="#saving-the-graphical-output"></a>
</h3>
<p>All generated graphics can be saved in any file format supported by
<code>Cairo()</code>, including ‚Äúpng‚Äù, ‚Äújpeg‚Äù, ‚Äúpdf‚Äù, ‚Äúsvg‚Äù, ‚Äúps‚Äù, and
‚Äútiff‚Äù in the user specified <code>plotDirectory</code>.</p>
<p>If the optional argument <code>plotName</code> is not given, the
naming of the output follows the pattern
<code>"testname_namey_namex."</code>, where <code>"testname"</code>
specifies the selected test and <code>"namey"</code> and
<code>"namex"</code> are character strings naming the selected data
vectors <code>y</code> and <code>x</code>, respectively. The suffix
corresponding to the chosen <code>graphicsoutput</code> (e.g.,
<code>"pdf"</code>, <code>"png"</code>) is then concatenated to form the
complete output file name.</p>
<p>In the following example, we store the graphics in <code>png</code>
format in the <code>plotDirectory</code> <code><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir()</a></code> with the
default naming convention:</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Graphical output written to plotDirectory: In this example </span></span>
<span><span class="co"># a bar chart to visualise the Chi-squared test and mosaic plot showing</span></span>
<span><span class="co"># Pearson's residuals.</span></span>
<span><span class="co">#chi_squared_or_fisher_Hair_Eye.png and mosaic_complete_Hair_Eye.png</span></span>
<span><span class="va">save_fisher</span> <span class="op">=</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Hair</span>,</span>
<span>        graphicsoutput <span class="op">=</span> <span class="st">"png"</span>, plotDirectory <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The full file path of the generated graphics are stored as the
attribute <code>"plot_paths"</code> on the returned object of class
<code>"visstat"</code>.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">paths</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">save_fisher</span>, <span class="st">"plot_paths"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">paths</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] "/tmp/Rtmpbdgm9V/chi_squared_or_fisher_Hair_Eye.png"</span></span>
<span><span class="co">## [2] "/tmp/Rtmpbdgm9V/mosaic_complete_Hair_Eye.png"</span></span></code></pre>
<p>Remove the graphical output from <code>plotDirectory</code>:</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/files.html" class="external-link">file.remove</a></span><span class="op">(</span><span class="va">paths</span><span class="op">)</span></span></code></pre></div>
<pre><code><span><span class="co">## [1] TRUE TRUE</span></span></code></pre>
<p>When assumptions plots (residual and Q-Q plot) are generated, the
corresponding plot has the prefix <code>"assumption_</code>.</p>
</div>
</div>
<div class="section level2">
<h2 id="limitations">Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a>
</h2>
<div class="section level3">
<h3 id="limitations-by-default-settings">Limitations by default settings<a class="anchor" aria-label="anchor" href="#limitations-by-default-settings"></a>
</h3>
<p>The main purpose of this package is a decision-logic based automatic
visualisation of statistical test results. Therefore, except for the
user-adjustable <code>conf.level</code> parameter, all statistical tests
are applied using their default settings from the corresponding base R
functions. As a consequence, paired tests are currently not supported
and <code><a href="../reference/visstat.html">visstat()</a></code> does not allow to study interactions terms
between the different levels of an independent variable in an analysis
of variance. Focusing on the graphical representation of tests, only
simple linear regression is implemented, as multiple linear regressions
cannot be visualised.</p>
</div>
<div class="section level3">
<h3 id="limitations-of-decision-logic-based-on-p-values-of-hypothesis-tests">Limitations of decision-logic based on p-values of hypothesis
tests<a class="anchor" aria-label="anchor" href="#limitations-of-decision-logic-based-on-p-values-of-hypothesis-tests"></a>
</h3>
<p>This package uses the Shapiro-Wilk test to select between parametric
(ANOVA/Welch‚Äôs t-test) and non-parametric (Kruskal-Wallis/Wilcoxon)
methods.</p>
<!-- It furthermore applies the  Levene-Brown-Forsythe test for variance homogeneity to select between the Welch's heteroscedastic one-way ANOVA (oneway.test())  and Fisher's one/way ANOVA aov(). -->
<p>However, no single test maintains optimal Type I error rates and
statistical power across all distributions <span class="citation">(<a href="#ref-Olejnik:1987">Olejnik and Algina 1987</a>)</span>, and
p-values obtained from these tests may be unreliable if their
assumptions are violated.</p>
</div>
<div class="section level3">
<h3 id="limitations-of-defaulting-to-welch-tests">Limitations of defaulting to Welch tests<a class="anchor" aria-label="anchor" href="#limitations-of-defaulting-to-welch-tests"></a>
</h3>
<p>This package uses Welch‚Äôs t-test and Welch‚Äôs ANOVA (oneway.test) by
default to avoid preliminary testing for homoscedasticity, which
inflates Type I error rates <span class="citation">(<a href="#ref-Zimmerman:2004">Zimmerman 2004</a>)</span>. When variances
are equal, Welch versions have minimal power loss compared to classical
tests (Delacre et al., 2017; Delacre et al., 2019). For N &gt; 2 groups
with n &lt; 30, normality must be assessed before selecting parametric
versus non-parametric tests.</p>
<p>However, Welch‚Äôs ANOVA does not produce residuals suitable for
normality testing. Testing normality separately for each group inflates
Type I error through multiple testing. This package instead extracts
residuals from classical ANOVA (aov), tests their normality, then uses
that result to select Welch‚Äôs ANOVA‚Äîa pragmatic but conceptually
inconsistent approach. For
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>‚â•</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">n \ge 30</annotation></semantics></math>,
normality testing is omitted, assuming parametric tests are robust to
moderate non-normality (Knief &amp; Forstmeier, 2021; Lumley et al.,
2002).</p>
<p>Heteroscedasticity violates a key assumption of the classical linear
model: OLS estimators remain unbiased but the standard errors, test
statistics and therefore all associated inference become invalid when
error variances differ (Gelman and Hill 2007; Fox 2016). Importantly,
this affects only inference, not the definition of residuals, which
remain simply the observed deviations from the fitted group means and
thus provide a useful empirical summary of within-group departures from
normality. For this reason, residual-based normality checks can be
employed even when subsequent inference uses heteroscedasticity-robust
methods such as Welch-type tests.</p>
<div class="section level4">
<h4 id="limitations-of-normality-testing">Limitations of normality testing<a class="anchor" aria-label="anchor" href="#limitations-of-normality-testing"></a>
</h4>
<p>Normality tests behave poorly at both ends of the sample-size range:
with small samples they fail to detect non-normality, and with large
samples they flag negligible departures from normality as significant
<span class="citation">(<a href="#ref-Ghasemi:2012">Ghasemi and
Zahediasl 2012</a>; <a href="#ref-Fagerland:2012">Fagerland 2012</a>; <a href="#ref-Franc:2025">Franc 2025</a>)</span>. Since visual Q‚ÄìQ
inspection cannot be implemented algorithmically, <code><a href="../reference/visstat.html">visstat()</a></code>
cannot automate a more robust alternative. This package uses
<math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mrow><mi>n</mi><mo>‚â•</mo><mn>30</mn></mrow><annotation encoding="application/x-tex">n \ge 30</annotation></semantics></math>
per group as a rule of thumb threshold to omit normality testing,
assuming parametric tests become robust to non-normality at larger
sample sizes based on the central limit theorem. In fact, the actual
sample size needed for parametric test validity depends on the unknown
shape of the underlying distribution <span class="citation">(<a href="#ref-Knief:2021">Knief and Forstmeier 2021</a>)</span>, with
heavily skewed data potentially requiring much larger samples.</p>
</div>
<div class="section level4">
<h4 id="combining-tests-inflates-the-overall-type-i-error-rate">Combining tests inflates the overall Type I error rate<a class="anchor" aria-label="anchor" href="#combining-tests-inflates-the-overall-type-i-error-rate"></a>
</h4>
<p>Combining multiple tests with differing assumptions using simple
majority voting inflates the overall Type I error rate.</p>
<p>Therefore, automated test selection based solely on p-values cannot
replace the visual inspection of sample distributions provided by
<code><a href="../reference/visstat.html">visstat()</a></code>. Based on the provided diagnostic plots, it may
be necessary to override the automated choice of test in individual
cases.</p>
</div>
</div>
<div class="section level3">
<h3 id="bootstrapping-as-modern-alternative-to-hypothesis-testing">Bootstrapping as modern alternative to hypothesis testing<a class="anchor" aria-label="anchor" href="#bootstrapping-as-modern-alternative-to-hypothesis-testing"></a>
</h3>
<p>Bootstrapping methods <span class="citation">(<a href="#ref-Wilcox:2021">Wilcox 2021</a>)</span> make minimal
distributional assumptions and can provide confidence intervals for
nearly any statistic. However, bootstrapping is computationally
intensive, often requiring thousands of resamples, and may perform
poorly with very small sample sizes.
<!-- Additionally, the empirical literature lacks sufficient evidence to establish the bootstrap as a universally reliable approach for uncertainty quantification across diverse statistical contexts.[@Zrimsek:2024] --></p>
<p>The computational intensity of bootstrap and runs counter to the
purpose of the <code>visStatistics</code> package, which is designed to
offer a rapid overview of the data, laying the groundwork for deeper
analysis in subsequent steps.</p>
<p>The package targets users with basic statistical literacy, such as
non-specialist professionals and students in applied fields. It covers
topics typically included in an undergraduate course on applied
statistics, intentionally excluding more advanced methods like
bootstrapping to keep the focus on foundational concepts.</p>
</div>
</div>
<div class="section level2">
<h2 id="overview-of-implemented-tests">Overview of implemented tests<a class="anchor" aria-label="anchor" href="#overview-of-implemented-tests"></a>
</h2>
<div class="section level3">
<h3 id="numeric-response-and-categorical-predictor">Numeric response and categorical predictor<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor"></a>
</h3>
<p>When the response is numeric and the predictor is categorical, a test
of central tendency is selected:</p>
<p><code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>, <code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>,
<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>,
<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>,<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code></p>
<div class="section level4">
<h4 id="normality-assumption-check">Normality assumption check<a class="anchor" aria-label="anchor" href="#normality-assumption-check"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code> and <code>ad.test()</code> <span class="citation">(<a href="#ref-Gross:2015">Gross and Ligges
2015</a>)</span></p>
</div>
<div class="section level4">
<h4 id="homoscedasticity-assumption-check">Homoscedasticity assumption check<a class="anchor" aria-label="anchor" href="#homoscedasticity-assumption-check"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code> and <code>levene.test()</code></p>
</div>
<div class="section level4">
<h4 id="post-hoc-tests">Post-hoc tests<a class="anchor" aria-label="anchor" href="#post-hoc-tests"></a>
</h4>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> (for <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>and
<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>)</li>
<li>
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> (for
<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>)</li>
</ul>
</div>
</div>
<div class="section level3">
<h3 id="numeric-response-and-numeric-predictor">Numeric response and numeric predictor<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor"></a>
</h3>
<p>The default behaviour is to fit a simple linear regression model with
a numeric response and predictor variables.</p>
<p><code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code></p>
<p>Changing the flag <code>do_regression</code> to FALSE performs</p>
<p>‚Äôcor.test()`</p>
</div>
<div class="section level3">
<h3 id="both-variables-categorical">Both variables categorical<a class="anchor" aria-label="anchor" href="#both-variables-categorical"></a>
</h3>
<p>When both variables are categorical, <code><a href="../reference/visstat.html">visstat()</a></code> tests the
null hypothesis of independence using one of the following:</p>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code> (default for larger samples)</li>
<li>
<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code> (used for small expected cell counts
based on Cochran‚Äôs rule)</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 class="unnumbered" id="bibliography">Bibliography<a class="anchor" aria-label="anchor" href="#bibliography"></a>
</h2>
<div id="refs" class="references csl-bib-body hanging-indent" entry-spacing="0">
<div id="ref-Abdi:2007" class="csl-entry">
Abdi, Herv√©. 2007. <span>‚ÄúThe <span>Bonferonni</span> and <span>≈†id√°k
Corrections</span> for <span>Multiple Comparisons</span>.‚Äù</span>
<em>Encyclopediav of Measurement and Statistics</em>.
</div>
<div id="ref-Allingham:2012" class="csl-entry">
Allingham, David, and J. C. W. Rayner. 2012. <span>‚ÄúTesting
<span>Equality</span> of <span>Variances</span> for <span>Multiple
Univariate Normal Populations</span>.‚Äù</span> <em>Journal of Statistical
Theory and Practice</em> 6 (3): 524‚Äì35. <a href="https://doi.org/10.1080/15598608.2012.695703" class="external-link">https://doi.org/10.1080/15598608.2012.695703</a>.
</div>
<div id="ref-Bartlett:1937" class="csl-entry">
Bartlett, M. S. 1937. <span>‚ÄúProperties of Sufficiency and Statistical
Tests.‚Äù</span> <em>Proceedings of the Royal Society of London. Series A,
Mathematical and Physical Sciences</em> 160 (901): 268‚Äì82. <a href="https://doi.org/10.1098/rspa.1937.0109" class="external-link">https://doi.org/10.1098/rspa.1937.0109</a>.
</div>
<div id="ref-Brown:1974" class="csl-entry">
Brown, Morton B., and Alan B. Forsythe. 1974. <span>‚ÄúRobust
<span>Tests</span> for the <span>Equality</span> of
<span>Variances</span>.‚Äù</span> <em>Journal of the American Statistical
Association</em> 69 (346): 364‚Äì67. <a href="https://doi.org/10.1080/01621459.1974.10482955" class="external-link">https://doi.org/10.1080/01621459.1974.10482955</a>.
</div>
<div id="ref-Cochran:1954" class="csl-entry">
Cochran, William G. 1954. <span>‚ÄúThe <span>Combination</span> of
<span>Estimates</span> from <span>Different Experiments</span>.‚Äù</span>
<em>Biometrics</em> 10 (1): 101. <a href="https://doi.org/10.2307/3001666" class="external-link">https://doi.org/10.2307/3001666</a>.
</div>
<div id="ref-Delacre:2017" class="csl-entry">
Delacre, Marie, Dani√´l Lakens, and Christophe Leys. 2017. <span>‚ÄúWhy
Psychologists Should by Default Use <span>Welch</span>‚Äôs t-Test Instead
of <span>Student</span>‚Äôs t-Test.‚Äù</span> <em>International Review of
Social Psychology</em> 30 (1): 92‚Äì101. <a href="https://doi.org/10.5334/irsp.82" class="external-link">https://doi.org/10.5334/irsp.82</a>.
</div>
<div id="ref-Fagerland:2012" class="csl-entry">
Fagerland, Morten W. 2012. <span>‚ÄúT-Tests, Non-Parametric Tests, and
Large Studies‚Äîa Paradox of Statistical Practice?‚Äù</span> <em>BMC Medical
Research Methodology</em> 12 (1): 78. <a href="https://doi.org/10.1186/1471-2288-12-78" class="external-link">https://doi.org/10.1186/1471-2288-12-78</a>.
</div>
<div id="ref-Fagerland:2009" class="csl-entry">
Fagerland, Morten W., and Leiv Sandvik. 2009. <span>‚ÄúPerformance of Five
Two-Sample Location Tests for Skewed Distributions with Unequal
Variances.‚Äù</span> <em>Contemporary Clinical Trials</em> 30 (5): 490‚Äì96.
<a href="https://doi.org/10.1016/j.cct.2009.06.007" class="external-link">https://doi.org/10.1016/j.cct.2009.06.007</a>.
</div>
<div id="ref-Fisher:1990" class="csl-entry">
Fisher, Ronald A., and F Yates. 1990. <em>Statistical
<span>Methods</span>, <span>Experimental Design</span>, and
<span>Scientific Inference</span>: <span class="nocase">A
Re-issue</span> of <span>Statistical Methods</span> for <span>Research
Workers</span>, the <span>Design</span> of <span>Experiments</span> and
<span>Statistical Methods</span> and <span>Scientific
Inference</span></em>. Edited by J H Bennett. Oxford University
PressOxford. <a href="https://doi.org/10.1093/oso/9780198522294.001.0001" class="external-link">https://doi.org/10.1093/oso/9780198522294.001.0001</a>.
</div>
<div id="ref-Fisher:1970" class="csl-entry">
Fisher, Ronald Aylmer. 1970. <em>Statistical Methods for Research
Workers</em>. 14th ed., revised and enlarged. Edinburgh: <span>Oliver
and Boyd</span>.
</div>
<div id="ref-Fox:2019" class="csl-entry">
Fox, John, and Sanford Weisberg. 2019. <em>An <span>R</span> Companion
to Applied Regression</em>. 3rd ed. Thousand Oaks CA: Sage.
</div>
<div id="ref-Franc:2025" class="csl-entry">
Franc, Jeffrey Michael. 2025. <span>‚ÄúThe <span>Misuse</span> of
<span>Normality Tests</span> as <span>Gatekeepers</span> for
<span>Research</span> in <span>Prehospital</span> and <span>Disaster
Medicine</span>.‚Äù</span> <em>Prehospital and Disaster Medicine</em> 40
(5): 241‚Äì42. <a href="https://doi.org/10.1017/S1049023X25101465" class="external-link">https://doi.org/10.1017/S1049023X25101465</a>.
</div>
<div id="ref-Games:1976" class="csl-entry">
Games, Paul A., and John F. Howell. 1976. <span>‚ÄúPairwise Multiple
Comparison Procedures with Unequal <span>N</span>‚Äôs and/or Variances:
<span>A Monte Carlo</span> Study.‚Äù</span> <em>Journal of Educational
Statistics</em> 1 (2): 113‚Äì25. <a href="https://doi.org/10.2307/1164979" class="external-link">https://doi.org/10.2307/1164979</a>.
</div>
<div id="ref-Ghasemi:2012" class="csl-entry">
Ghasemi, Asghar, and Saleh Zahediasl. 2012. <span>‚ÄúNormality
<span>Tests</span> for <span>Statistical Analysis</span>: <span>A
Guide</span> for <span>Non-Statisticians</span>.‚Äù</span> <em>Int J
Endocrinol Metab</em> 10 (2): 486‚Äì89. <a href="https://doi.org/10.5812/ijem.3505" class="external-link">https://doi.org/10.5812/ijem.3505</a>.
</div>
<div id="ref-Graves:2024" class="csl-entry">
Graves, Spencer, Hans-Peter Piepho, and Luciano Selzer with help from
Sundar Dorai-Raj. 2024. <em><span class="nocase">multcompView</span>:
<span>Visualizations</span> of Paired Comparisons</em>. Manual. <a href="https://doi.org/10.32614/CRAN.package.multcompView" class="external-link">https://doi.org/10.32614/CRAN.package.multcompView</a>.
</div>
<div id="ref-Gross:2015" class="csl-entry">
Gross, Juergen, and Uwe Ligges. 2015. <em>Nortest: <span>Tests</span>
for Normality</em>. Manual. <a href="https://doi.org/10.32614/CRAN.package.nortest" class="external-link">https://doi.org/10.32614/CRAN.package.nortest</a>.
</div>
<div id="ref-Hochberg:1987" class="csl-entry">
Hochberg, Yosef, and Ajit C. Tamhane. 1987. <em>Multiple
<span>Comparison Procedures</span></em>. 1st ed. Wiley
<span>Series</span> in <span>Probability</span> and
<span>Statistics</span>. Wiley. <a href="https://doi.org/10.1002/9780470316672" class="external-link">https://doi.org/10.1002/9780470316672</a>.
</div>
<div id="ref-Hollander:2014" class="csl-entry">
Hollander, Myles, Eric Chicken, and Douglas A. Wolfe. 2014.
<em>Nonparametric Statistical Methods</em>. Third edition. Wiley Series
in Probability and Statistics. Hoboken, New Jersey: John Wiley &amp;
Sons, Inc.
</div>
<div id="ref-Holm:1979" class="csl-entry">
Holm, Sture. 1979. <span>‚ÄúA <span>Simple Sequentially Rejective Multiple
Test Procedure</span>.‚Äù</span> <em>Scandinavian Journal of
Statistics</em> 6 (2): 65‚Äì70. <a href="https://www.jstor.org/stable/4615733" class="external-link">https://www.jstor.org/stable/4615733</a>.
</div>
<div id="ref-Knief:2021" class="csl-entry">
Knief, Ulrich, and Wolfgang Forstmeier. 2021. <span>‚ÄúViolating the
Normality Assumption May Be the Lesser of Two Evils.‚Äù</span> <em>Behav
Res Methods</em> 53 (6): 2576‚Äì90. <a href="https://doi.org/10.3758/s13428-021-01587-5" class="external-link">https://doi.org/10.3758/s13428-021-01587-5</a>.
</div>
<div id="ref-Kozak:2018" class="csl-entry">
Kozak, M., and H.-P. Piepho. 2018. <span>‚ÄúWhat‚Äôs Normal Anyway?
<span>Residual</span> Plots Are More Telling Than Significance Tests
When Checking <span><span class="smallcaps">ANOVA</span></span>
Assumptions.‚Äù</span> <em>J Agronomy Crop Science</em> 204 (1): 86‚Äì98. <a href="https://doi.org/10.1111/jac.12220" class="external-link">https://doi.org/10.1111/jac.12220</a>.
</div>
<div id="ref-Kruskal:1952" class="csl-entry">
Kruskal, William H., and W. Allen Wallis. 1952. <span>‚ÄúUse of
<span>Ranks</span> in <span>One-Criterion Variance
Analysis</span>.‚Äù</span> <em>Journal of the American Statistical
Association</em> 47 (260): 583‚Äì621. <a href="https://doi.org/10.2307/2280779" class="external-link">https://doi.org/10.2307/2280779</a>.
</div>
<div id="ref-Kwak:2017" class="csl-entry">
Kwak, Sang Gyu, and Jong Hae Kim. 2017. <span>‚ÄúCentral Limit Theorem:
The Cornerstone of Modern Statistics.‚Äù</span> <em>Korean J
Anesthesiol</em> 70 (2): 144‚Äì56. <a href="https://doi.org/10.4097/kjae.2017.70.2.144" class="external-link">https://doi.org/10.4097/kjae.2017.70.2.144</a>.
</div>
<div id="ref-Levene:1960" class="csl-entry">
Levene, Howard. 1960. <span>‚ÄúRobust Tests for Equality of
Variances.‚Äù</span> In <em>Contributions to Probability and Statistics:
<span>Essays</span> in Honor of Harold Hotelling</em>, edited by Ingram
Olkin, 278‚Äì92. Stanford, CA: Stanford University Press.
</div>
<div id="ref-Lumley:2002" class="csl-entry">
Lumley, Thomas, Paula Diehr, Scott Emerson, and Lu Chen. 2002.
<span>‚ÄúThe <span>Importance</span> of the <span>Normality
Assumption</span> in <span>Large Public Health Data Sets</span>.‚Äù</span>
<em>Annu. Rev. Public Health</em> 23 (1): 151‚Äì69. <a href="https://doi.org/10.1146/annurev.publhealth.23.100901.140546" class="external-link">https://doi.org/10.1146/annurev.publhealth.23.100901.140546</a>.
</div>
<div id="ref-Mann:1947" class="csl-entry">
Mann, Henry B., and Donald R. Whitney. 1947. <span>‚ÄúOn a Test of Whether
One of Two Random Variables Is Stochastically Larger Than the
Other.‚Äù</span> <em>The Annals of Mathematical Statistics</em> 18 (1):
50‚Äì60. <a href="https://doi.org/10.1214/aoms/1177730491" class="external-link">https://doi.org/10.1214/aoms/1177730491</a>.
</div>
<div id="ref-Moser:1992" class="csl-entry">
Moser, B K, and G. R. Stevens. 1992. <span>‚ÄúHomogeneity of Variance in
the Two-Sample Means Test.‚Äù</span> <em>The American Statistician</em>,
February, 19‚Äì21. <a href="https://doi.org/10.1080/00031305.1992.10475839" class="external-link">https://doi.org/10.1080/00031305.1992.10475839</a>.
</div>
<div id="ref-Olejnik:1987" class="csl-entry">
Olejnik, Stephen F., and James Algina. 1987. <span>‚ÄúType <span>I Error
Rates</span> and <span>Power Estimates</span> of <span>Selected
Parametric</span> and <span>Nonparametric Tests</span> of
<span>Scale</span>.‚Äù</span> <em>Journal of Educational Statistics</em>
12 (1): 45. <a href="https://doi.org/10.2307/1164627" class="external-link">https://doi.org/10.2307/1164627</a>.
</div>
<div id="ref-Pearson:1900" class="csl-entry">
Pearson, Karl. 1900. <span>‚ÄúOn the Criterion That a Given System of
Deviations from the Probable in the Case of a Correlated System of
Variables Is Such That It Can Be Reasonably Supposed to Have Arisen from
Random Sampling.‚Äù</span> <em>The London, Edinburgh, and Dublin
Philosophical Magazine and Journal of Science</em> 50 (302): 157‚Äì75. <a href="https://doi.org/10.1080/14786440009463897" class="external-link">https://doi.org/10.1080/14786440009463897</a>.
</div>
<div id="ref-Rasch:2011" class="csl-entry">
Rasch, Dieter, Klaus D. Kubinger, and Karl Moder. 2011. <span>‚ÄúThe
Two-Sample t Test: Pre-Testing Its Assumptions Does Not Pay Off.‚Äù</span>
<em>Stat Papers</em> 52 (1): 219‚Äì31. <a href="https://doi.org/10.1007/s00362-009-0224-x" class="external-link">https://doi.org/10.1007/s00362-009-0224-x</a>.
</div>
<div id="ref-Razali:2011" class="csl-entry">
Razali, Nornadiah Mohd, and Yap Bee Wah. 2011. <span>‚ÄúPower Comparisons
of <span>Shapiro-Wilk</span>, <span>Kolmogorov-Smirnov</span>,
<span>Lilliefors</span> and <span>Anderson-Darling</span> Tests.‚Äù</span>
<em>Journal of Statistical Modeling and Analytics</em> 2 (1): 21‚Äì33.
</div>
<div id="ref-Satterthwaite:1946" class="csl-entry">
Satterthwaite, F. E. 1946. <span>‚ÄúAn <span>Approximate
Distribution</span> of <span>Estimates</span> of <span>Variance
Components</span>.‚Äù</span> <em>Biometrics Bulletin</em> 2 (6): 110‚Äì14.
<a href="https://doi.org/10.2307/3002019" class="external-link">https://doi.org/10.2307/3002019</a>.
</div>
<div id="ref-Searle:1971" class="csl-entry">
Searle, Shayle R. 1971. <em>Linear Models</em>. John Wiley &amp; Sons.
</div>
<div id="ref-Shapiro:1965" class="csl-entry">
SHAPIRO, S. S., and M. B. WILK. 1965. <span>‚ÄúAn Analysis of Variance
Test for Normality (Complete Samples).‚Äù</span> <em>Biometrika</em> 52
(3-4): 591‚Äì611. <a href="https://doi.org/10.1093/biomet/52.3-4.591" class="external-link">https://doi.org/10.1093/biomet/52.3-4.591</a>.
</div>
<div id="ref-Shatz:2024" class="csl-entry">
Shatz, Itamar. 2024. <span>‚ÄúAssumption-Checking Rather Than (Just)
Testing: <span>The</span> Importance of Visualization and Effect Size in
Statistical Diagnostics.‚Äù</span> <em>Behav Res</em> 56 (2): 826‚Äì45. <a href="https://doi.org/10.3758/s13428-023-02072-x" class="external-link">https://doi.org/10.3758/s13428-023-02072-x</a>.
</div>
<div id="ref-Sidak:1967" class="csl-entry">
≈†id√°k, Zbynƒõk. 1967. <span>‚ÄúRectangular Confidence Regions for the Means
of Multivariate Normal Distributions.‚Äù</span> <em>Journal of the
American Statistical Association</em> 62 (318): 626‚Äì33. <a href="https://doi.org/10.1080/01621459.1967.10482935" class="external-link">https://doi.org/10.1080/01621459.1967.10482935</a>.
</div>
<div id="ref-Welch:1947" class="csl-entry">
Welch, B. L. 1947. <span>‚ÄúThe Generalization of <span>‚ÄòStudent‚Äôs‚Äô</span>
Problem When Several Different Population Variances Are
Involved.‚Äù</span> <em>Biometrika</em> 34 (1‚Äì2): 28‚Äì35. <a href="https://doi.org/10.1093/biomet/34.1-2.28" class="external-link">https://doi.org/10.1093/biomet/34.1-2.28</a>.
</div>
<div id="ref-Welch:1951" class="csl-entry">
‚Äî‚Äî‚Äî. 1951. <span>‚ÄúOn the <span>Comparison</span> of <span>Several Mean
Values</span>: <span>An Alternative Approach</span>.‚Äù</span>
<em>Biometrika</em> 38 (3/4): 330‚Äì36. <a href="https://doi.org/10.2307/2332579" class="external-link">https://doi.org/10.2307/2332579</a>.
</div>
<div id="ref-Wilcox:2021" class="csl-entry">
Wilcox, Rand R. 2021. <em>Introduction to <span>Robust Estimation</span>
and <span>Hypothesis Testing</span></em>.
</div>
<div id="ref-Yap:2011" class="csl-entry">
Yap, B. W., and C. H. Sim. 2011. <span>‚ÄúComparisons of Various Types of
Normality Tests.‚Äù</span> <em>Journal of Statistical Computation and
Simulation</em> 81 (12): 2141‚Äì55. <a href="https://doi.org/10.1080/00949655.2010.520163" class="external-link">https://doi.org/10.1080/00949655.2010.520163</a>.
</div>
<div id="ref-Yates:1934" class="csl-entry">
Yates, F. 1934. <span>‚ÄúContingency <span>Tables Involving Small
Numbers</span> and the
<span><math display="inline" xmlns="http://www.w3.org/1998/Math/MathML"><semantics><mi>œá</mi><annotation encoding="application/x-tex">\chi</annotation></semantics></math></span>2
<span>Test</span>.‚Äù</span> <em>Journal of the Royal Statistical Society
Series B: Statistical Methodology</em> 1 (2): 217‚Äì35. <a href="https://doi.org/10.2307/2983604" class="external-link">https://doi.org/10.2307/2983604</a>.
</div>
<div id="ref-Zimmerman:2004" class="csl-entry">
Zimmerman, Donald W. 2004. <span>‚ÄúA Note on Preliminary Tests of
Equality of Variances.‚Äù</span> <em>British Journal of Mathematical and
Statistical Psychology</em> 57 (1): 173‚Äì81. <a href="https://doi.org/10.1348/000711004849222" class="external-link">https://doi.org/10.1348/000711004849222</a>.
</div>
</div>
</div>
  </div>

  <div class="col-md-3 hidden-xs hidden-sm" id="pkgdown-sidebar">

      </div>

</div>



      <footer><div class="copyright">
  <p></p>
<p>Developed by Sabine Schilling.</p>
</div>

<div class="pkgdown">
  <p></p>
<p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.2.0.</p>
</div>

      </footer>
</div>






  </body>
</html>
