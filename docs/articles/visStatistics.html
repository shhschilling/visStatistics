<!DOCTYPE html>
<!-- Generated by pkgdown: do not edit by hand --><html lang="en">
<head>
<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
<meta charset="utf-8">
<meta http-equiv="X-UA-Compatible" content="IE=edge">
<meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<title>visStatistics: The right test, visualised • visStatistics</title>
<script src="../deps/jquery-3.6.0/jquery-3.6.0.min.js"></script><meta name="viewport" content="width=device-width, initial-scale=1, shrink-to-fit=no">
<link href="../deps/bootstrap-5.3.1/bootstrap.min.css" rel="stylesheet">
<script src="../deps/bootstrap-5.3.1/bootstrap.bundle.min.js"></script><link href="../deps/font-awesome-6.5.2/css/all.min.css" rel="stylesheet">
<link href="../deps/font-awesome-6.5.2/css/v4-shims.min.css" rel="stylesheet">
<script src="../deps/headroom-0.11.0/headroom.min.js"></script><script src="../deps/headroom-0.11.0/jQuery.headroom.min.js"></script><script src="../deps/bootstrap-toc-1.0.1/bootstrap-toc.min.js"></script><script src="../deps/clipboard.js-2.0.11/clipboard.min.js"></script><script src="../deps/search-1.0.0/autocomplete.jquery.min.js"></script><script src="../deps/search-1.0.0/fuse.min.js"></script><script src="../deps/search-1.0.0/mark.min.js"></script><!-- pkgdown --><script src="../pkgdown.js"></script><meta property="og:title" content="visStatistics: The right test, visualised">
</head>
<body>
    <a href="#main" class="visually-hidden-focusable">Skip to contents</a>


    <nav class="navbar navbar-expand-lg fixed-top bg-light" data-bs-theme="light" aria-label="Site navigation"><div class="container">

    <a class="navbar-brand me-2" href="../index.html">visStatistics</a>

    <small class="nav-text text-muted me-auto" data-bs-toggle="tooltip" data-bs-placement="bottom" title="">0.1.8</small>


    <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbar" aria-controls="navbar" aria-expanded="false" aria-label="Toggle navigation">
      <span class="navbar-toggler-icon"></span>
    </button>

    <div id="navbar" class="collapse navbar-collapse ms-3">
      <ul class="navbar-nav me-auto">
<li class="nav-item"><a class="nav-link" href="../reference/index.html">Reference</a></li>
<li class="active nav-item"><a class="nav-link" href="../articles/visStatistics.html">Vignette</a></li>
<li class="nav-item"><a class="nav-link" href="../articles/index.html">Articles</a></li>
      </ul>
<ul class="navbar-nav">
<li class="nav-item"><form class="form-inline" role="search">
 <input class="form-control" type="search" name="search-input" id="search-input" autocomplete="off" aria-label="Search site" placeholder="Search for" data-search-index="../search.json">
</form></li>
<li class="nav-item"><a class="external-link nav-link" href="https://github.com/shhschilling/visStatistics/" aria-label="GitHub"><span class="fa fab fa-github fa-lg"></span></a></li>
      </ul>
</div>


  </div>
</nav><div class="container template-article">


<div class="row">
  <main id="main" class="col-md-9"><div class="page-header">

      <h1>visStatistics: The right test, visualised</h1>
                        <h4 data-toc-skip class="author">Sabine Schilling</h4>
            <address class="author_afil">
      Institute of Tourism and Mobility, Lucerne University of Applied Sciences and Arts<br><a class="author_email" href="mailto:#"></a><a href="mailto:sabine.schilling@protonmail.com" class="email">sabine.schilling@protonmail.com</a>
      </address>
                  
            <h4 data-toc-skip class="date">2025-06-24</h4>
      
      <small class="dont-index">Source: <a href="https://github.com/shhschilling/visStatistics/blob/HEAD/vignettes/visStatistics.Rmd" class="external-link"><code>vignettes/visStatistics.Rmd</code></a></small>
      <div class="d-none name"><code>visStatistics.Rmd</code></div>
    </div>

    
    
<div class="sourceCode" id="cb1"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/shhschilling/visStatistics" class="external-link">visStatistics</a></span><span class="op">)</span></span></code></pre></div>
<div class="section level2" number="1">
<h2 id="abstract">
<span class="header-section-number">1</span> Abstract<a class="anchor" aria-label="anchor" href="#abstract"></a>
</h2>
<p><code>visStatistics</code> automatically selects and visualises appropriate statistical hypothesis tests between two column vectors of type of class <code>"numeric"</code>, <code>"integer"</code>, or <code>"factor"</code>.
The choice of test depends on the <code>class</code>, distribution, and sample size of the vectors, as well as the user-defined ‘conf.level’.
The main function <code><a href="../reference/visstat.html">visstat()</a></code> visualises the selected test with appropriate graphs (box plots, bar charts, regression lines with confidence bands, mosaic plots, residual plots, Q-Q plots), annotated with the main test results, including any assumption checks and post-hoc analyses.
This scripted workflow is particularly suited for browser-based interfaces that rely on server-side R applications connected to secure databases, where users have no direct access, or for quick data visualisations and test selection, e.g., in statistical consulting projects.</p>
</div>
<div class="section level2" number="2">
<h2 id="introduction">
<span class="header-section-number">2</span> Introduction<a class="anchor" aria-label="anchor" href="#introduction"></a>
</h2>
<!-- Introductory statistics courses typically cover a fixed set of hypothesis tests, but selecting the correct method in practice can be error-prone.  -->
<!-- `visStatistics` addresses this by applying a deterministic, transparent test selection logic.  -->
<p>While numerous R packages provide statistical testing functionality, few are designed with pedagogical accessibility as a primary concern.
The visStatistics package addresses this challenge by automating test selection using deterministic decision logic, removing the burden of manual test choice.
This automation enables users to focus directly on interpreting statistical outcomes rather than navigating test selection.</p>
<p>The tailored visual outputs—annotated with test results and, where appropriate, assumption checks and post-hoc analyses—further support comprehension and help ensure valid conclusions from the outset.
The package is particularly valuable in statistical consulting for student research projects, where time constraints demand streamlined, assumption-aware output that prioritises interpretation over technical execution.
The implemented tests cover the typical content of an introductory undergraduate course in statistics.</p>
<p>The package also suits server-based applications where users have limited interaction: they provide only two input vectors, and the software returns valid, interpretable results without requiring further statistical knowledge.
This supports reproducibility and correct inference even in such constrained environments.</p>
<p>The remainder of this vignette is organised as follows:</p>
<ul>
<li><p>Section 3 focuses on the installation and the main function call,</p></li>
<li><p>Section 4 summarises the decision logic used to select a statistical test.</p></li>
<li><p>Sections 5–7 provide background on the implemented tests and illustrate the decision logic using examples.
Function names in parentheses in the headings indicate the corresponding statistical hypothesis test function in R.</p></li>
<li><p>Section 8 outlines the main limitations of the package.</p></li>
<li><p>Section 9 provides an overview of the implemented tests.</p></li>
</ul>
</div>
<div class="section level2" number="3">
<h2 id="getting-started">
<span class="header-section-number">3</span> Getting started<a class="anchor" aria-label="anchor" href="#getting-started"></a>
</h2>
<div class="section level5">
<h5 id="install-the-latest-development-version-from-github">1. Install the latest development version from GITHUB<a class="anchor" aria-label="anchor" href="#install-the-latest-development-version-from-github"></a>
</h5>
<div class="sourceCode" id="cb2"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu">install_github</span><span class="op">(</span><span class="st">"shhschilling/visStatistics"</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="load-the-package">2. Load the package<a class="anchor" aria-label="anchor" href="#load-the-package"></a>
</h5>
<div class="sourceCode" id="cb3"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="kw"><a href="https://rdrr.io/r/base/library.html" class="external-link">library</a></span><span class="op">(</span><span class="va"><a href="https://github.com/shhschilling/visStatistics" class="external-link">visStatistics</a></span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level5">
<h5 id="minimal-function-call">3. Minimal function call<a class="anchor" aria-label="anchor" href="#minimal-function-call"></a>
</h5>
<p>The function <code><a href="../reference/visstat.html">visstat()</a></code> accepts input in two ways:</p>
<div class="sourceCode" id="cb4"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># Standardised form (recommended):</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">x</span>, <span class="va">y</span><span class="op">)</span></span>
<span></span>
<span><span class="co"># Backward-compatible form</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">dataframe</span>, <span class="st">"namey"</span>, <span class="st">"namex"</span><span class="op">)</span></span></code></pre></div>
<p>In the standardised form, <code>x</code> and <code>y</code> must be vectors of class <code>"numeric"</code>, <code>"integer"</code>, or <code>"factor"</code>.</p>
<p>In the backward-compatible form, <code>"namex"</code> and <code>"namey"</code> must be character strings naming columns in <code>dataframe</code>, which must themselves be of class <code>"numeric"</code>, <code>"integer"</code>, or <code>"factor"</code>.
This is equivalent to writing:</p>
<div class="sourceCode" id="cb5"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">dataframe</span><span class="op">[[</span><span class="st">"namex"</span><span class="op">]</span><span class="op">]</span>, <span class="va">dataframe</span><span class="op">[[</span><span class="st">"namey"</span><span class="op">]</span><span class="op">]</span><span class="op">)</span></span></code></pre></div>
</div>
</div>
<div class="section level2" number="4">
<h2 id="decision-logic">
<span class="header-section-number">4</span> Decision logic<a class="anchor" aria-label="anchor" href="#decision-logic"></a>
</h2>
<p>Throughout the remainder, data of class <code>"numeric"</code> or <code>"integer"</code> are referred to by their common <code>mode</code> <code>numeric</code>, while data of class <code>"factor"</code> are referred to as categorical.
The significance level <span class="math inline">\(\alpha\)</span>, used throughout for hypothesis testing, is defined as <code>1 - conf.level</code>, where <code>conf.level</code> is a user-controllable argument (defaulting to <code>0.95</code>).</p>
<p>The choice of statistical tests performed by the function <code><a href="../reference/visstat.html">visstat()</a></code> depends on whether the data are numeric or categorical, the number of levels in the categorical variable, the distribution of the data, as well as the user-defined ‘conf.level’.</p>
<p>The function prioritizes interpretable visual output and tests that remain valid under the following decision logic:</p>
<div class="section level3" number="4.1">
<h3 id="numeric-response-and-categorical-predictor-comparing-central-tendencies">
<span class="header-section-number">4.1</span> Numeric response and categorical predictor: Comparing central tendencies<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor-comparing-central-tendencies"></a>
</h3>
<p>When the response is numeric and the predictor is categorical, a statistical hypothesis test of central tendencies is selected.</p>
<ul>
<li><p>If the categorical predictor has exactly two levels, Welch’s t-test (<code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>) is applied when both groups contain more than 30 observations.
This heuristic is based on the central limit theorem, which ensures approximate normality of the sampling distribution of the mean <span class="citation">[<a href="#ref-Rasch:2011">1</a>–<a href="#ref-Kwak:2017">3</a>]</span>.
Welch’s t-test is the default method in R for comparing means and is generally preferred over Student’s t-test because it does not assume equal variances.
It maintains comparable power even when variances are equal and outperforms Student’s test when variances differ <span class="citation">[<a href="#ref-Moser:1992">4</a>–<a href="#ref-Delacre:2017">6</a>]</span>.</p></li>
<li>
<p>For smaller samples, group-wise normality is assessed using the Shapiro-Wilk test [<span class="citation">[<a href="#ref-Shapiro:1965">7</a>]</span> (<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>) at a significance level <span class="math inline">\(\alpha\)</span>.
Simulation studies consistently show that the Shapiro-Wilk test is the most powerful for detecting non-normality across most distributions, especially with smaller sample sizes.<span class="citation">[<a href="#ref-Razali:2011">8</a>, <a href="#ref-Ghasemi:2012">9</a>]</span></p>
<p>If both groups are found to be approximately normally distributed according to the Shapiro–Wilk test, Welch’s t-test is applied; otherwise, the Wilcoxon rank-sum test (<code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>) is used.</p>
</li>
<li><p>For predictors with more than two levels, a model of Fisher’s one-way analysis of variables (ANOVA) (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) is initially fitted.
<!-- The normality of residuals is evaluated by the  the Shapiro--Wilk test (`shapiro.test()`) and the Anderson-Darling test (`ad.test()`); residuals are considered approximately normal if at least one of the two tests yields a result exceeding the significance threshold $\alpha$. --> The normality of residuals is evaluated by the Shapiro-Wilk test (<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>); residuals are considered approximately normal if it yields a result exceeding the significance threshold <span class="math inline">\(\alpha\)</span>.
If this condition is met, the Levene–Brown–Forsythe test (implemented as <code><a href="../reference/levene.test.html">levene.test()</a></code>) <span class="citation">[<a href="#ref-Brown:1974">10</a>]</span> assesses homoscedasticity.
When variances are homogeneous (<span class="math inline">\(p &gt; \alpha\)</span>), Fisher’s one-way ANOVA (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) is applied with Tukey’s Honestly Significant Differences (HSD) (<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>) for post-hoc comparison.
If variances differ significantly (<span class="math inline">\(p \le \alpha\)</span>), Welch’s heteroscedastic one-way ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) is used, also followed by Tukey’s HSD. If residuals are not normally distributed according to both tests (<span class="math inline">\(p \le \alpha\)</span>), the Kruskal–Wallis test (<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>) is selected, followed by pairwise Wilcoxon tests (<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code>).
A graphical overview of the decision logic used is provided in the figure below.</p></li>
</ul>
<div style="border: 1px solid #666; padding: 10px; display: inline-block; text-align: center;">
<img src="figures/decision_tree.png" width="100%" alt="Decision tree used to select the appropriate statistical test."><p style="font-style: italic; font-size: 90%; margin-top: 0.5em;">
Decision tree used to select the appropriate statistical test for a categorical
predictor and numeric response, based on the number of factor levels, normality,
and homoscedasticity.
</p>
</div>
</div>
<div class="section level3" number="4.2">
<h3 id="numeric-response-and-numeric-predictor-simple-linear-regression">
<span class="header-section-number">4.2</span> Numeric response and numeric predictor: Simple linear regression<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor-simple-linear-regression"></a>
</h3>
<p>When both the response and predictor are numeric, a simple linear regression model (<code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code>) is fitted and analysed in detail, including residual diagnostics, formal tests, and the plotting of fitted values with confidence bands.
Note that <strong>only one</strong> predictor variable is allowed, as the function is designed for two-dimensional visualisation.</p>
</div>
<div class="section level3" number="4.3">
<h3 id="both-variables-categorical-comparing-proportions">
<span class="header-section-number">4.3</span> Both variables categorical: Comparing proportions<a class="anchor" aria-label="anchor" href="#both-variables-categorical-comparing-proportions"></a>
</h3>
<p>When both variables are categorical, no direction is assumed; the order of variables in the function call does not affect the test statistic, but it does influence the graphical output.
For consistency, we continue referring to the variables as <em>predictor</em> and <em>response</em>.</p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis that the variables are independent using either Pearson’s <span class="math inline">\(\chi^2\)</span> test (<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>) or Fisher’s exact test (<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>), depending on expected cell counts.
The choice of test is based on Cochran’s rule <span class="citation">[<a href="#ref-Cochran:1954">11</a>]</span>, which advises that the <span class="math inline">\(\chi^2\)</span> approximation is reliable only if no expected cell count is less than 1 and no more than 20 percent of cells have expected counts below 5.</p>
</div>
</div>
<div class="section level2" number="5">
<h2 id="numeric-response-and-categorical-predictor-comparing-central-tendencies-1">
<span class="header-section-number">5</span> Numeric response and categorical predictor: Comparing central tendencies<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor-comparing-central-tendencies-1"></a>
</h2>
<p>When the predictor consists of <code>class</code> “<code>factor</code>” with two or more levels and the response is of <code>class</code> “<code>numeric</code>” or “<code>integer</code>” (both having mode “<code>numeric</code>”), statistical tests are applied to compare the central tendencies across groups.
This section describes the conditions under which parametric and non-parametric tests are chosen, based on the response type, the number of factor levels, and the underlying distributional assumptions.</p>
<div class="section level3" number="5.1">
<h3 id="categorical-predictor-with-two-levels-welchs-t-test-and-wilcoxon-rank-sum">
<span class="header-section-number">5.1</span> Categorical predictor with two levels: Welch’s t-test and Wilcoxon rank-sum<a class="anchor" aria-label="anchor" href="#categorical-predictor-with-two-levels-welchs-t-test-and-wilcoxon-rank-sum"></a>
</h3>
<p>When the predictor variable has exactly two levels, Welch’s t-test or the Wilcoxon rank-sum test is applied.</p>
<div class="section level4" number="5.1.1">
<h4 id="welchs-t-test-t-test">
<span class="header-section-number">5.1.1</span> Welch’s t-test (<code>t.test()</code>)<a class="anchor" aria-label="anchor" href="#welchs-t-test-t-test"></a>
</h4>
<p>Welch’s t-test (<code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>) assumes that the observations are independent and that the response variable is approximately normally distributed within each group.
<!-- In contrast to Student's t-test, it does not require the assumption of equal variances (homoscedasticity) between groups. --> <!-- Welch's t-test remains valid and exhibits only minimal loss of efficiency even when the assumptions of Student's t-test -- namely, normality and equal variances of the response variable across groups -- are satisfied [@Moser:1992; @Delacre:2017]. --> <!-- Therefore, Student's t-test is not implemented. --></p>
<p>It evaluates the null hypothesis that the means of two groups are equal without assuming equal variances.
The test statistic is given by <span class="citation">[<a href="#ref-Welch:1947">12</a>, <a href="#ref-Satterthwaite:1946">13</a>]</span></p>
<p><span class="math display">\[
t = \frac{\bar{x}_1 - \bar{x}_2}{\sqrt{\frac{s_1^2}{n_1} + \frac{s_2^2}{n_2}}},
\]</span> where <span class="math inline">\(\bar{x}_1\)</span> and <span class="math inline">\(\bar{x}_2\)</span> are the sample means, <span class="math inline">\(s_1^2\)</span> and <span class="math inline">\(s_2^2\)</span> the sample variances, and <span class="math inline">\(n_1\)</span>, <span class="math inline">\(n_2\)</span> the sample sizes in the two groups.
The statistic follows a <em>t</em>-distribution with degrees of freedom approximated by the Welch-Satterthwaite equation:</p>
<p><span class="math display">\[
\nu \approx \frac{
\left( \frac{s_1^2}{n_1} + \frac{s_2^2}{n_2} \right)^2
}{
\frac{(s_1^2 / n_1)^2}{n_1 - 1} + \frac{(s_2^2 / n_2)^2}{n_2 - 1}
}.
\]</span></p>
<p>The resulting p-value is computed from the <em>t</em>-distribution with <span class="math inline">\(\nu\)</span> degrees of freedom.</p>
<p>Welch’s t-test remains valid and exhibits only minimal loss of power even when the assumptions of Student’s t-test – namely, normality and equal variances of the response variable across groups – are satisfied <span class="citation">[<a href="#ref-Moser:1992">4</a>, <a href="#ref-Delacre:2017">6</a>]</span>.
It is therefore the default implementation of the t-test in R.</p>
</div>
<div class="section level4" number="5.1.2">
<h4 id="wilcoxon-rank-sum-test-wilcox-test">
<span class="header-section-number">5.1.2</span> Wilcoxon rank-sum test (<code>wilcox.test()</code>)<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test-wilcox-test"></a>
</h4>
<p>The two-sample Wilcoxon rank-sum test (also known as the Mann-Whitney test) is a non-parametric alternative that does not require the response variable to be approximately normally distributed within each group.
It tests for a difference in location between two independent distributions <span class="citation">[<a href="#ref-Mann:1947">14</a>]</span>.
If the two groups have distributions that are sufficiently similar in shape and scale, the Wilcoxon rank-sum test can be interpreted as testing whether the medians of the two populations are equal <span class="citation">[<a href="#ref-Hollander:2014">15</a>]</span>.</p>
<p>The two-level factor variable <code>x</code> defines two groups, with sample sizes <span class="math inline">\(n_1\)</span> and <span class="math inline">\(n_2\)</span>.
All <span class="math inline">\(N=n_1 + n_2\)</span> observations are pooled and assigned ranks from <span class="math inline">\(1\)</span> to <span class="math inline">\(N\)</span>.
Let <span class="math inline">\(W_1\)</span> denote the sum of the ranks assigned to the group <span class="math inline">\(x_1\)</span> corresponding to the first level of <code>x</code> containing <span class="math inline">\(n_1\)</span> observations:</p>
<p><span class="math display">\[
W_{1}= \sum_{i=1}^{n_1} R(x_{1,i})\]</span>,</p>
<p>where <span class="math inline">\(R(x_{1,i})\)</span> is the rank of observation <span class="math inline">\(x_{1,i}\)</span> in the pooled sample.</p>
<p>The test statistic <span class="math inline">\(W\)</span> returned by <code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code> is then computed as</p>
<p><span class="math display">\[
W =U_{1}=W_{1} - \frac{n_1(n_1 + 1)}{2}.
\]</span> It corresponds to the Mann-Whitney <span class="citation">[<a href="#ref-Mann:1947">14</a>]</span> <span class="math inline">\(U\)</span> statistic of the first group.</p>
<p>If both groups contain fewer than 50 observations and the data contain no ties, the <em>p</em>-value is computed exactly.
Otherwise, a normal approximation with continuity correction is used.</p>
</div>
<div class="section level4" number="5.1.3">
<h4 id="graphical-output">
<span class="header-section-number">5.1.3</span> Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output"></a>
</h4>
<!-- `visstat()` selects between Welch's t-test and the Wilcoxon rank-sum test
as follows. If both groups contain more than 30 observations, Welch's t-test is
always applied, relying on the central limit theorem to justify its application
regardless of underlying normality [@Rasch:2011; @Lumley:2002]. -->
<!-- If either group contains fewer than 30 observations, the Shapiro--Wilk test
(`shapiro.test()`) is applied separately to each group. Welch's t-test is used
if both tests do not reject normality at the significance level $\alpha$;
otherwise, the Wilcoxon rank-sum test is applied. -->
<p>The graphical output consists of box plots overlaid with jittered points to display individual observations.
When Welch’s t-test is applied, the function includes confidence intervals based on the user-specified <code>conf.level</code>.</p>
<p>The title is structured as follows:</p>
<ul>
<li><p>First line: Test name and chosen significance level <span class="math inline">\(\alpha\)</span>.</p></li>
<li><p>Second line: Null hypotheses automatically adapted based on the user-specified response and grouping variable.</p></li>
<li><p>Third line: Test statistic, p-value and automated comparison with <span class="math inline">\(\alpha\)</span></p></li>
</ul>
<p>The function returns a list containing the results of the applied test and the summary statistics used to construct the plot.</p>
</div>
<div class="section level4" number="5.1.4">
<h4 id="examples">
<span class="header-section-number">5.1.4</span> Examples<a class="anchor" aria-label="anchor" href="#examples"></a>
</h4>
<div class="section level5">
<h5 id="welchs-t-test">Welch’s t-test<a class="anchor" aria-label="anchor" href="#welchs-t-test"></a>
</h5>
<p>The <em>Motor Trend Car Road Tests</em> dataset (<code>mtcars</code>) contains 32 observations, where <code>mpg</code> denotes miles per (US) gallon, and <code>am</code> represents the transmission type (<code>0</code> = automatic, <code>1</code> = manual).</p>
<div class="sourceCode" id="cb6"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span><span class="op">)</span></span>
<span><span class="va">t_test_statistics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">mtcars</span><span class="op">$</span><span class="va">am</span>, <span class="va">mtcars</span><span class="op">$</span><span class="va">mpg</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-2-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-2-2.png" width="100%"></p>
<p>Increasing the confidence level <code>conf.level</code> from the default 0.95 to 0.99 results in wider confidence intervals, as a higher confidence level requires more conservative bounds to ensure that the interval includes the true parameter value with greater certainty.</p>
</div>
<div class="section level5">
<h5 id="wilcoxon-rank-sum-test">Wilcoxon rank sum test<a class="anchor" aria-label="anchor" href="#wilcoxon-rank-sum-test"></a>
</h5>
<p>The Wilcoxon rank sum test is exemplified on differences between the central tendencies of grades of “boys” and “girls” in a class:</p>
<div class="sourceCode" id="cb7"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">grades_gender</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/data.frame.html" class="external-link">data.frame</a></span><span class="op">(</span></span>
<span>  sex <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/factor.html" class="external-link">as.factor</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="st">"girl"</span>, <span class="fl">21</span><span class="op">)</span>, <span class="fu"><a href="https://rdrr.io/r/base/rep.html" class="external-link">rep</a></span><span class="op">(</span><span class="st">"boy"</span>, <span class="fl">23</span><span class="op">)</span><span class="op">)</span><span class="op">)</span>,</span>
<span>  grade <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/c.html" class="external-link">c</a></span><span class="op">(</span></span>
<span>    <span class="fl">19.3</span>, <span class="fl">18.1</span>, <span class="fl">15.2</span>, <span class="fl">18.3</span>, <span class="fl">7.9</span>, <span class="fl">6.2</span>, <span class="fl">19.4</span>,</span>
<span>    <span class="fl">20.3</span>, <span class="fl">9.3</span>, <span class="fl">11.3</span>, <span class="fl">18.2</span>, <span class="fl">17.5</span>, <span class="fl">10.2</span>, <span class="fl">20.1</span>, <span class="fl">13.3</span>, <span class="fl">17.2</span>, <span class="fl">15.1</span>, <span class="fl">16.2</span>, <span class="fl">17.0</span>,</span>
<span>    <span class="fl">16.5</span>, <span class="fl">5.1</span>, <span class="fl">15.3</span>, <span class="fl">17.1</span>, <span class="fl">14.8</span>, <span class="fl">15.4</span>, <span class="fl">14.4</span>, <span class="fl">7.5</span>, <span class="fl">15.5</span>, <span class="fl">6.0</span>, <span class="fl">17.4</span>,</span>
<span>    <span class="fl">7.3</span>, <span class="fl">14.3</span>, <span class="fl">13.5</span>, <span class="fl">8.0</span>, <span class="fl">19.5</span>, <span class="fl">13.4</span>, <span class="fl">17.9</span>, <span class="fl">17.7</span>, <span class="fl">16.4</span>, <span class="fl">15.6</span>, <span class="fl">17.3</span>, <span class="fl">19.9</span>, <span class="fl">4.4</span>, <span class="fl">2.1</span></span>
<span>  <span class="op">)</span></span>
<span><span class="op">)</span></span>
<span></span>
<span><span class="va">wilcoxon_statistics</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">grades_gender</span><span class="op">$</span><span class="va">sex</span>, <span class="va">grades_gender</span><span class="op">$</span><span class="va">grade</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-4-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-4-2.png" width="100%"></p>
</div>
</div>
</div>
<div class="section level3" number="5.2">
<h3 id="categorical-predictor-with-more-than-two-levels">
<span class="header-section-number">5.2</span> Categorical predictor with more than two levels<a class="anchor" aria-label="anchor" href="#categorical-predictor-with-more-than-two-levels"></a>
</h3>
<p>If the predictor is of <code>class</code> “<code>factor</code>” with <strong>more than two levels</strong> and the response is of <code>mode</code> “<code>numeric</code>”, <code><a href="../reference/visstat.html">visstat()</a></code> either performs Fisher’s one- way ANOVA <span class="citation">[<a href="#ref-Fisher:1935">16</a>]</span> (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>), Welch’s heteroscedastic one-way ANOVA <span class="citation">[<a href="#ref-Welch:1951">17</a>]</span> (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) or, as a non-parametric alternative, the Kruskal -Wallis test <span class="citation">[<a href="#ref-Kruskal:1952">18</a>]</span> (<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>).</p>
<p>In the remainder of this section, we briefly introduce the tests themselves, the assumption checks, and the post-hoc procedures, and illustrate each test with an example.</p>
<div class="section level4" number="5.2.1">
<h4 id="fishers-one-way-anova-aov">
<span class="header-section-number">5.2.1</span> Fisher’s one-way ANOVA (<code>aov()</code>)<a class="anchor" aria-label="anchor" href="#fishers-one-way-anova-aov"></a>
</h4>
<p>Fisher’s one-way ANOVA (<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>) tests the null hypothesis that the means of multiple groups are equal.
It assumes independent observations, normally distributed residuals, and <strong>homogeneous</strong> variances across groups.
The test statistic is the ratio of the variance explained by differences among group means (between-group variance) to the unexplained variance within groups <span class="citation">[<a href="#ref-Fisher:1990">19</a>]</span></p>
<p><span class="math display">\[
F = \frac{\text{between-group variance}}{\text{within-group variance}}
= \frac{\frac{\sum_{i=1}^{k} n_i (\bar{x}_i - \bar{x})^2}{k - 1}}
{\frac{\sum_{i=1}^{k}\sum_{j=1}^{n_i}(x_{ij}-\bar{x}_i)^2}{N - k}},
\]</span></p>
<p>where <span class="math inline">\(\bar{x}_i\)</span> is the mean of group <span class="math inline">\(i\)</span>, <span class="math inline">\(\bar{x}\)</span> is the overall mean, <span class="math inline">\(x_{ij}\)</span> is the observation <span class="math inline">\(j\)</span> in group <span class="math inline">\(i\)</span>, <span class="math inline">\(n_i\)</span> is the sample size in group <span class="math inline">\(i\)</span>, <span class="math inline">\(k\)</span> is the number of groups, and <span class="math inline">\(N\)</span> is the total number of observations.</p>
<p>Under the null hypothesis, this statistic follows an F-distribution with two parameters for degrees of freedom: (<span class="math inline">\(k - 1\)</span>) and (<span class="math inline">\(N - k\)</span>): <span class="math inline">\(F \sim F(k-1, N-k)\)</span> The resulting p-value is computed from this distribution.</p>
</div>
<div class="section level4" number="5.2.2">
<h4 id="welchs-heteroscedastic-one-way-anova-oneway-test">
<span class="header-section-number">5.2.2</span> Welch’s heteroscedastic one-way ANOVA (<code>oneway.test()</code>)<a class="anchor" aria-label="anchor" href="#welchs-heteroscedastic-one-way-anova-oneway-test"></a>
</h4>
<p>When only the assumptions of independent observations and normally distributed residuals are met, but <em>homogeneous variances</em> across groups <em>cannot be assumed</em> , Welch’s heteroscedastic one-way ANOVA (<code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>) <span class="citation">[<a href="#ref-Welch:1951">17</a>]</span> provides an alternative to <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>.
It compares group means using weights based on sample sizes and variances.
The degrees of freedom are adjusted using a Satterthwaite-type approximation <span class="citation">[<a href="#ref-Satterthwaite:1946">13</a>]</span>, resulting in an F-statistic with non-integer degrees of freedom.</p>
</div>
<div class="section level4" number="5.2.3">
<h4 id="kruskalwallis-test-kruskal-test">
<span class="header-section-number">5.2.3</span> Kruskal–Wallis test (<code>kruskal.test()</code>)<a class="anchor" aria-label="anchor" href="#kruskalwallis-test-kruskal-test"></a>
</h4>
<p>When the assumption of normality is not met, the Kruskal–Wallis test provides a non-parametric alternative.
It compares group distributions based on ranked values and tests the null hypothesis that the groups come from the same population — specifically, that the distributions have the same location <span class="citation">[<a href="#ref-Kruskal:1952">18</a>]</span>.
If the group distributions are sufficiently similar in shape and scale, then the Kruskal–Wallis test can be interpreted as testing for equality of medians across groups <span class="citation">[<a href="#ref-Hollander:2014">15</a>]</span>.</p>
<p>The test statistic is defined as:</p>
<p><span class="math display">\[
H = \frac{12}{N(N+1)} \sum_{i=1}^{k} n_i \left(\bar{R}_i - \bar{R} \right)^2,
\]</span></p>
<p>where <span class="math inline">\(n_i\)</span> is the sample size in group <span class="math inline">\(i\)</span>, <span class="math inline">\(k\)</span> is the number of groups, <span class="math inline">\(\bar{R}_i\)</span> is the average rank of group <span class="math inline">\(i\)</span>, <span class="math inline">\(N\)</span> is the total sample size, and <span class="math inline">\(\bar{R} = \frac{N+1}{2}\)</span> is the average of all ranks.
Under the null hypothesis, <span class="math inline">\(H\)</span> approximately follows a <span class="math inline">\(\chi^2\)</span> distribution with <span class="math inline">\(k - 1\)</span> degrees of freedom.</p>
</div>
<div class="section level4" number="5.2.4">
<h4 id="test-choice">
<span class="header-section-number">5.2.4</span> Test choice<a class="anchor" aria-label="anchor" href="#test-choice"></a>
</h4>
</div>
<div class="section level4" number="5.2.5">
<h4 id="testing-the-assumptions-visanovaassumptions">
<span class="header-section-number">5.2.5</span> Testing the assumptions (<code>visAnovaAssumptions()</code>)<a class="anchor" aria-label="anchor" href="#testing-the-assumptions-visanovaassumptions"></a>
</h4>
<p>The test logic for <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> follows from their respective assumptions.
<code><a href="../reference/visstat.html">visstat()</a></code> initially models the data using <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and analyses the residuals.</p>
<p>If both of the following conditions are met: (1) the standardised residuals follow the standard normal distribution, and (2) the residuals exhibit homoscedasticity (equal variances across groups), then the test statistic from <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> is returned.</p>
<p>If only the normality assumption is satisfied, <code><a href="../reference/visstat.html">visstat()</a></code> applies <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>.
If the normality assumption is violated, <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code> is used instead.</p>
<p>These assumptions are tested using the <code>visAnovaAssumptions()</code> function.</p>
<div class="section level5" number="5.2.5.1">
<h5 id="normality-of-residuals-shapiro-test-and-ad-test">
<span class="header-section-number">5.2.5.1</span> Normality of residuals (<code>shapiro.test()</code> and <code>ad.test()</code>)<a class="anchor" aria-label="anchor" href="#normality-of-residuals-shapiro-test-and-ad-test"></a>
</h5>
<p>The <code>visAnovaAssumptions()</code> function evaluates the normality of standardised residuals from the ANOVA model using both the Shapiro–Wilk test (<code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code>) and the Anderson–Darling test (<code>ad.test()</code>)<span class="citation">[<a href="#ref-Gross:2015">20</a>]</span>.
These tests offer complementary strengths: Shapiro–Wilk generally exhibits greater power across a range of non-normal distributions in small samples, whereas Anderson–Darling is highly sensitive to tail deviations and performs reliably in larger samples <span class="citation">[<a href="#ref-Razali:2011">8</a>, <a href="#ref-Yap:2011">21</a>]</span>.</p>
<p>Assessing assumptions solely through p-values can lead to both type I errors (false positives) and type II errors (false negatives).
In large samples, even minor, random deviations from the null hypothesis—such as the assumption of normality—can result in statistically significant p-values, leading to type I errors.
Conversely, in small samples, substantial violations of the assumption may not reach statistical significance, resulting in type II errors <span class="citation">[<a href="#ref-Kozak:2018">22</a>]</span>.</p>
<p>Thus, the robustness of statistical tests depends on both the sample size and the shape of the underlying distribution.
This is evident in the limitations of the normality tests used: for instance, the Shapiro–Wilk test is unreliable for large samples (<span class="math inline">\(N &gt; 5000\)</span>), while the Anderson–Darling test requires at least 7 observations.</p>
<p>Moreover, assumption tests provide no information on the nature of deviations from the expected distribution <span class="citation">[<a href="#ref-Shatz:2024">23</a>]</span>.
Therefore, the assessment of normality should not rely solely on p-values but should be complemented by visual inspection.
<code><a href="../reference/visstat.html">visstat()</a></code> produces diagnostic plots including: (1) a histogram of the standardised residuals overlaid with the standard normal distribution, (2) a scatter plot of the standardised residuals versus the fitted values for each predictor level, and (3) a Q–Q plot of the residuals.</p>
<p>Since algorithmic logic cannot replace the combination of formal tests and expert visual judgement, the function defaults to assuming normality if the Shapiro–Wilk test yields a p-value greater than alpha.
Simulation studies suggest that Shapiro–Wilk has the highest power among normality tests in small to moderate samples <span class="citation">[<a href="#ref-Razali:2011">8</a>]</span>.</p>
</div>
<div class="section level5" number="5.2.5.2">
<h5 id="equal-variances-across-groups-levene-test-and-bartlett-test">
<span class="header-section-number">5.2.5.2</span> Equal variances across groups (<code>levene.test() and bartlett.test()</code>)<a class="anchor" aria-label="anchor" href="#equal-variances-across-groups-levene-test-and-bartlett-test"></a>
</h5>
<p>Both <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> and <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> assess whether two or more samples drawn from normal distributions have the same mean.
While <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> assumes homogeneity of variances across groups, <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> does not require equal variances.</p>
<p>The decision logic of <code>visStatistics</code> assumes homogeneity of variances if the Levene–Brown–Forsythe test (implemented as <code><a href="../reference/levene.test.html">levene.test()</a></code>) <span class="citation">[<a href="#ref-Brown:1974">10</a>]</span> yields a p-value greater than <span class="math inline">\(\alpha\)</span>.</p>
<p>The Levene–Brown–Forsythe test evaluates the null hypothesis that all groups have equal variances by testing whether the absolute deviations from group medians are equal across groups.</p>
<p>For each observation <span class="math inline">\(y_{ij}\)</span> in group <span class="math inline">\(i\)</span>, it computes the absolute deviation from the group median:</p>
<p><span class="math display">\[z_{ij} = |y_{ij} - \tilde{y_i}|\]</span>,</p>
<p>where <span class="math inline">\(\tilde{y_i}\)</span> is the median of group <span class="math inline">\(i\)</span>.</p>
<p>The test statistic is the F-statistic from a one-way ANOVA on the <span class="math inline">\(z_{ij}\)</span> values:</p>
<p><span class="math display">\[F = \frac{\frac{\sum_{i=1}^{k} n_i (\bar{z}_i - \bar{z})^2}{k-1}}{\frac{\sum_{i=1}^{k} \sum_{j=1}^{n_i} (z_{ij} - \bar{z}_i)^2}{N-k}}\]</span></p>
<p>where <span class="math inline">\(k\)</span> is the number of groups, <span class="math inline">\(N\)</span> is the total sample size, <span class="math inline">\(n_i\)</span> is the sample size of group <span class="math inline">\(i\)</span>, <span class="math inline">\(\bar{z}_i\)</span> is the mean of absolute deviations from the median in group <span class="math inline">\(i\)</span>, and <span class="math inline">\(\bar{z}\)</span> is the overall mean of all absolute deviations.</p>
<p>Under the null hypothesis of equal variances, the test statistic follows an F-distribution: <span class="math inline">\(F \sim F(k-1, N-k)\)</span>.</p>
<p>The Levene–Brown–Forsythe test improves upon Levene’s original test <span class="citation">[<a href="#ref-Levene:1960">24</a>]</span> by using the median instead of the mean to centre the data.
This makes it more robust to non-normal data, providing more reliable results in many practical situations.
Note that <code><a href="../reference/levene.test.html">levene.test()</a></code> mimics the default behaviour of <code>leveneTest()</code> in the <code>car</code> package <span class="citation">[<a href="#ref-Fox:2019">25</a>]</span>.</p>
<p>Additionally, homoscedasticity is assessed via Bartlett’s test (<code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>), which has more power than the Brown–Forsythe version of Levene’s test <span class="citation">[<a href="#ref-Brown:1974">10</a>]</span> when the normality assumption is met,<br>
but is not robust to deviations from normality.</p>
</div>
</div>
<div class="section level4" number="5.2.6">
<h4 id="controlling-the-family-wise-error-rate">
<span class="header-section-number">5.2.6</span> Controlling the family-wise error rate<a class="anchor" aria-label="anchor" href="#controlling-the-family-wise-error-rate"></a>
</h4>
<p>ANOVA is an omnibus test that evaluates a single null hypothesis: all group means are equal.
If the null hypothesis gets rejected, we would like to identify which specific groups differ significantly from each other.
However, simple pairwise comparisons of group means following an ANOVA increases the probability of incorrectly declaring a significant difference when, in fact, there is none.</p>
<p>This error is quantified by the family-wise error rate “alpha per family of tests” <span class="math inline">\(\alpha_{PF}\)</span>, which refers to the probability of making at least one Type I error, that is, falsely rejecting the null hypothesis across all pairwise comparisons.</p>
<p>Given <span class="math inline">\(n\)</span> levels of the categorical variable, there are</p>
<p><span class="math display">\[
M = \frac{n \cdot (n - 1)}{2}
\]</span></p>
<p>pairwise comparisons possible, defining a <em>family of tests</em> <span class="citation">[<a href="#ref-Abdi:2007">26</a>]</span>.
<span class="math inline">\(M\)</span> corresponds to the number of null hypotheses in the post hoc tests, each testing whether the means of two specific groups are equal.</p>
<div class="section level5">
<h5 id="šidák-correction">Šidák correction<a class="anchor" aria-label="anchor" href="#%C5%A1id%C3%A1k-correction"></a>
</h5>
<p>If <span class="math inline">\(\alpha_{PT}\)</span> (“alpha per test”) is the probability of making a Type I error in one comparison, then <span class="math inline">\(1 - \alpha_{PT}\)</span> is the probability of not making a Type I error in one comparison.</p>
<p>If all <span class="math inline">\(M\)</span> comparisons are <strong>independent</strong> of each other, the probability of making no Type I error across the entire family of pairwise comparisons is <span class="math inline">\((1 -
\alpha_{PT})^M\)</span>.
The family-wise error rate is then given by its complement <span class="citation">[<a href="#ref-Abdi:2007">26</a>]</span>:</p>
<p><span class="math display">\[
\alpha_{PF} = 1 - (1 - \alpha_{PT})^M.
\]</span></p>
<!-- Let us illustrate the inflation of the family-wise error rate with increasing group number $n$ (equal to the number of `levels` in the categorical variable)) by the following examples: With $\alpha_{PT}=0.05$ pairwise comparison of  $n=3$ groups results in a family-wise error rate of $\alpha_{PF} \approx 14\%$, whereas pairwise comparing of $n=6$ groups (as in the examples below) already leads to the probability of at least one time falsely rejecting the null hypothesis of $\alpha_{PF} \approx 54\%$. -->
<p>Solving the last equation defining <span class="math inline">\(\alpha_{PF}\)</span> for <span class="math inline">\(\alpha_{PT}\)</span> yields the Šidák equation <span class="citation">[<a href="#ref-Sidak:1967">27</a>]</span>:</p>
<p><span class="math display">\[
\alpha_{PT}=1-(1-{\alpha_{PF}})^{1/M}.
\]</span> This shows that, in order to achieve a given family-wise error rate, the corresponding per-test significance level must be reduced when there are more than two groups.
<!-- ###### Šidák  correction in `visstat()` {.unnumbered} --></p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> sets <span class="math inline">\(\alpha_{PF}\)</span> to the user-defined <span class="math inline">\(\alpha = 1 -\)</span> <code>conf.level</code>, resulting in</p>
<p><span class="math display">\[\alpha_{PT} = 1 - \texttt{conf.level}^{1 / M}.
\]</span></p>
<p>With the default setting <code>conf.level = 0.95</code>, this leads to:</p>
<ul>
<li>
<span class="math inline">\(n = 3\)</span> groups: <span class="math inline">\(\alpha_{PT} = 1.70\%\)</span>
</li>
<li>
<span class="math inline">\(n = 6\)</span> groups: <span class="math inline">\(\alpha_{PT} = 0.34\%\)</span>
</li>
<li>
<span class="math inline">\(n = 10\)</span> groups: <span class="math inline">\(\alpha_{PT} = 0.11\%\)</span>
</li>
</ul>
<p>These examples illustrate that the Šidák approach becomes increasingly conservative as the number of comparisons grows.
Moreover, since the method assumes independence among tests, it may be overly conservative when this assumption is violated.</p>
</div>
<div class="section level5">
<h5 id="post-hoc-test-following-an-anova-tukeyhsd">Post-hoc test following an ANOVA: <code>TukeyHSD()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-an-anova-tukeyhsd"></a>
</h5>
<p>In contrast to the general-purpose Šidák correction, Tukey’s Honestly Significant Differences procedure (<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>) is specifically designed for pairwise mean comparisons following an ANOVA (either <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> or <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>).
It controls the family-wise error rate using a critical value from the studentised range distribution, which properly accounts for the correlated nature of pairwise comparisons sharing a common residual variance <span class="citation">[<a href="#ref-Hochberg:1987">28</a>]</span>.</p>
<p>Based on the user-specified confidence level (<code>conf.level</code>), <code><a href="../reference/visstat.html">visstat()</a></code> constructs confidence intervals for all pairwise <strong>differences</strong> between factor level means.
A significant difference between two means is indicated when the corresponding confidence interval does not include zero.
<code><a href="../reference/visstat.html">visstat()</a></code> returns both the HSD-adjusted p-values and the associated confidence intervals for all pairwise comparisons.</p>
<p>For graphical display, <code><a href="../reference/visstat.html">visstat()</a></code> uses a dual approach: <code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> provides the statistical test results and significance determinations, while Šidák-corrected confidence intervals around individual group means are shown for visualisation purposes.
This separation allows for optimal statistical testing while maintaining clear, interpretable graphics.</p>
</div>
<div class="section level5">
<h5 id="post-hoc-test-following-the-kruskalwallis-rank-sum-test-pairwise-wilcox-test">Post-hoc test following the Kruskal–Wallis rank sum test: <code>pairwise.wilcox.test()</code><a class="anchor" aria-label="anchor" href="#post-hoc-test-following-the-kruskalwallis-rank-sum-test-pairwise-wilcox-test"></a>
</h5>
<p>As a post-hoc analysis following the Kruskal–Wallis test, <code><a href="../reference/visstat.html">visstat()</a></code> applies the pairwise Wilcoxon rank sum test using <code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> to compare each pair of factor levels (see section “Wilcoxon rank-sum test (<code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>)”).</p>
<p>The resulting p-values from all pairwise comparisons are then adjusted for multiple testing using Holm’s method <span class="citation">[<a href="#ref-Holm:1979">29</a>]</span>: The p-values are first sorted from smallest to largest and tested against thresholds that become less strict as their rank increases.
This stepwise adjustment does not assume independence among tests and is typically less conservative than the Šidák method, while still ensuring strong control of the family-wise error rate.</p>
</div>
</div>
<div class="section level4" number="5.2.7">
<h4 id="graphical-output-1">
<span class="header-section-number">5.2.7</span> Graphical output<a class="anchor" aria-label="anchor" href="#graphical-output-1"></a>
</h4>
<p>The graphical output for all tests based on a numeric response and a categorical predictor with more than two levels consists of two panels: the first focuses on the residual analysis, the second on the actual test chosen by the decision logic.</p>
<p>The residual panel addresses the assumption of normality, both graphically and through formal tests.
It displays a scatter plot of the standardised residuals versus the predicted values, as well as a normal Q–Q plot comparing the sample quantiles to the theoretical quantiles.
If the residuals are normally distributed, no more than <span class="math inline">\(5\%\)</span> of the standardised residuals should exceed approximately <span class="math inline">\(|2|\)</span>; in the Q–Q plot the data points should approximately follow the red straight line.</p>
<p>The p-values of the formal tests for normality (Shapiro–Wilk and Anderson–Darling) as well as the tests for homoscedasticity (Bartlett’s and Levene Brown–Forsythe) are given in the title.
<!-- To assume normality of residuals, the formal tests for normality (Shapiro--Wilk and Anderson--Darling) should result in p-values greater than the user-defined $\alpha$. --></p>
<!-- If normality of the residuals can beassumed, Bartlett's test can be assesed to check for homogenity of variances. Otherwise, the Levene Brown–Forsythe is a more robust test for homoscedasticity given deviations from the normality assumption of residuals  The p-values of the beforementioned tests are displayed in the title of the residual panel. -->
<p><code><a href="../reference/visstat.html">visstat()</a></code> then illustrates, in the subsequent graph, either the <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>, the <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>, or <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> result (see also Section “Decision logic”).</p>
<p>If neither normality of the residuals nor homogeneity of variances is given, the <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code> is executed.
The result is illustrated using box plots alongside jittered data points, with the title displaying the p-value from <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>.</p>
<p>Above each box plot, the number of observations per level is shown.
Different green letters below a pair of box plots indicate that the two groups are considered significantly different based on Holm’s-adjusted pairwise Wilcoxon rank sum test p-values smaller than <span class="math inline">\(\alpha\)</span>.
The letters are generated with the help of the function <code>multcompLetters()</code> from the <code>multcompView</code> package <span class="citation">[<a href="#ref-Graves:2024">30</a>]</span>).</p>
<p>If normality of the residuals can be assumed, a parametric test is chosen: either <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>, if homoscedasticity is also assumed, or <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> otherwise.
<code><a href="../reference/visstat.html">visstat()</a></code> displays the name of the test and the corresponding <span class="math inline">\(F\)</span> and p-value in the title.</p>
<p>The graph shows both the <code>conf.level</code> <span class="math inline">\(\cdot\,100\%\)</span> confidence intervals corresponding to the null hypothesis of ANOVA (all group means are equal) and the Šidák-corrected <span class="math inline">\((1 - \alpha_{PT}) \cdot 100\%\)</span> confidence intervals used in the post hoc analysis.</p>
<p>In <code><a href="../reference/visstat.html">visstat()</a></code>, the Šidák intervals are used only for visualisation, as the underlying method assumes independent comparisons and can become overly conservative when this assumption is violated.</p>
<p>The actual post hoc analysis should be based on <code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code>.
A significant test result between two groups is graphically represented by different green letters below a pair of group means.</p>
<p>Besides the graphical output, <code><a href="../reference/visstat.html">visstat()</a></code> returns a list containing the relevant test statistics along with the corresponding post-hoc-adjusted <span class="math inline">\(p\)</span>-values for all pairwise comparisons.</p>
</div>
<div class="section level4" number="5.2.8">
<h4 id="examples-1">
<span class="header-section-number">5.2.8</span> Examples<a class="anchor" aria-label="anchor" href="#examples-1"></a>
</h4>
<div class="section level5">
<h5 id="one-way-test">One-way test<a class="anchor" aria-label="anchor" href="#one-way-test"></a>
</h5>
<p>The <code>npk</code> dataset reports the yield of peas (in pounds per block) from an agricultural experiment conducted on six blocks.
In this experiment, the application of three different fertilisers – nitrogen (N), phosphate (P), and potassium (K) – was varied systematically.
Each block received either none, one, two, or all three of the fertilisers,</p>
<div class="sourceCode" id="cb8"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">oneway_npk</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">npk</span><span class="op">$</span><span class="va">block</span>,<span class="va">npk</span><span class="op">$</span><span class="va">yield</span>,conf.level<span class="op">=</span><span class="fl">0.90</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-5-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-5-2.png" width="100%"></p>
<p>Normality of residuals is supported by graphical diagnostics (scatter plot of standardised residuals, Q-Q plot) and formal tests (Shapiro–Wilk and Anderson- Darling, both with <span class="math inline">\(p &gt; \alpha\)</span>).
However, homogeneity of variances is not supported at the given confidence level (<span class="math inline">\(p &lt; \alpha\)</span>, <code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>), so the p-value from the variance-robust <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code> is reported.
Post-hoc analysis with <code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> shows no significant yield differences between blocks, as all share the same group label (e.g., all green letters).</p>
</div>
<div class="section level5">
<h5 id="anova-example">ANOVA example<a class="anchor" aria-label="anchor" href="#anova-example"></a>
</h5>
<!-- {.unnumbered} -->
<p>The <code>InsectSprays</code> dataset reports insect counts from agricultural experimental units treated with six different insecticides.
To stabilise the variance in counts, we apply a square root transformation to the response variable.</p>
<div class="sourceCode" id="cb9"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">insect_sprays_tr</span> <span class="op">&lt;-</span> <span class="va">InsectSprays</span></span>
<span><span class="va">insect_sprays_tr</span><span class="op">$</span><span class="va">count_sqrt</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/MathFun.html" class="external-link">sqrt</a></span><span class="op">(</span><span class="va">InsectSprays</span><span class="op">$</span><span class="va">count</span><span class="op">)</span></span>
<span><span class="va">test_statistic_anova</span><span class="op">=</span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">insect_sprays_tr</span><span class="op">$</span><span class="va">spray</span>, <span class="va">insect_sprays_tr</span><span class="op">$</span><span class="va">count_sqrt</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-6-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-6-2.png" width="100%"></p>
<div class="sourceCode" id="cb10"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co"># test_statistic_anova </span></span></code></pre></div>
<p>After the transformation, the homogeneity of variances can be assumed (<span class="math inline">\(p&gt;
\alpha\)</span> as calculated with the <code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code>), and the test statistic and p-value of Fisher’s one-way ANOVA<code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code> is displayed.</p>
<!-- The `ToothGrowth` data set studies the effect of vitamin C dosage C (0.5, 1, and 2 mg/day) on the the tooth growth of 60 guinea pigs. -->
<!-- ```{r} -->
<!-- visstat(ToothGrowth$dose, ToothGrowth$len) -->
<!-- ``` -->
<!-- Again, the homogeneity of variances can be assumed ($p> -->
<!-- \alpha$ as calculated with the `bartlett.test()`) and the test statistic and p-value of Fisher's one-way ANOVA`aov()` is displayed. -->
</div>
<div class="section level5">
<h5 id="kruskalwallis-rank-sum-test">Kruskal–Wallis rank sum test<a class="anchor" aria-label="anchor" href="#kruskalwallis-rank-sum-test"></a>
</h5>
<p>The <code>iris</code> dataset contains petal width measurements (in cm) for three different iris species.</p>
<div class="sourceCode" id="cb11"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">iris</span><span class="op">$</span><span class="va">Species</span>, <span class="va">iris</span><span class="op">$</span><span class="va">Petal.Width</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-7-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-7-2.png" width="100%"></p>
<p>In this example, scatter plots of the standardised residuals and the Q-Q plot suggest that the residuals are not normally distributed.
This is confirmed by very small p-values from both the Shapiro–Wilk and Anderson-Darling tests.</p>
<p>If both p-values are below the significance level <span class="math inline">\(\alpha\)</span>, <code><a href="../reference/visstat.html">visstat()</a></code> switches to the non-parametric <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>.
Post-hoc analysis using <code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> shows significant differences in petal width between all three species, as indicated by distinct group labels (all green letters differ).</p>
</div>
</div>
</div>
</div>
<div class="section level2" number="6">
<h2 id="numeric-response-and-numeric-predictor-simple-linear-regression-1">
<span class="header-section-number">6</span> Numeric response and numeric predictor: Simple linear regression<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor-simple-linear-regression-1"></a>
</h2>
<div class="section level3" number="6.1">
<h3 id="simple-linear-regression-lm">
<span class="header-section-number">6.1</span> Simple linear regression (<code>lm()</code>)<a class="anchor" aria-label="anchor" href="#simple-linear-regression-lm"></a>
</h3>
<p>If both the predictor and the response are numeric and contain only one level each, <code><a href="../reference/visstat.html">visstat()</a></code> performs a simple linear regression.</p>
<p>The resulting regression plot displays the point estimate of the regression line</p>
<p><span class="math display">\[
y = a + b \cdot x,
\]</span></p>
<p>where <span class="math inline">\(y\)</span> is the response variable, <span class="math inline">\(x\)</span> is the predictor variable, <span class="math inline">\(a\)</span> is the intercept, and <span class="math inline">\(b\)</span> is the slope of the regression line.</p>
<div class="section level4" number="6.1.1">
<h4 id="residual-analysis">
<span class="header-section-number">6.1.1</span> Residual analysis<a class="anchor" aria-label="anchor" href="#residual-analysis"></a>
</h4>
<p><code><a href="../reference/visstat.html">visstat()</a></code> checks the normality of the standardised residuals from <code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code> both graphically and using the Shapiro–Wilk and Anderson-Darling tests.
If the p-values for the null hypothesis of normally distributed residuals from both tests are smaller than <span class="math inline">\(1 -\)</span><code>conf.int</code>, the title of the residual plot will display the message: “Requirement of normally distributed residuals not met”.
Regardless of the result of the residual analysis, <code><a href="../reference/visstat.html">visstat()</a></code> proceeds to perform the regression.
The title of the graphical output indicates the chosen confidence level (<code>conf.level</code>), the estimated regression parameters with their confidence intervals and p-values, and the adjusted <span class="math inline">\(R^2\)</span>.
The plot displays the raw data, the fitted regression line, and both the confidence and prediction bands corresponding to the specified <code>conf.level</code>.</p>
<p><code><a href="../reference/visstat.html">visstat()</a></code> returns a list containing the regression test statistics, the p-values from the normality tests of the standardised residuals, and the pointwise estimates of the confidence and prediction bands.</p>
</div>
<div class="section level4" number="6.1.2">
<h4 id="examples-2">
<span class="header-section-number">6.1.2</span> Examples<a class="anchor" aria-label="anchor" href="#examples-2"></a>
</h4>
<!-- The `women` reports the  average height in inches and weights in pounds (lbs) for American women aged 30–39.  -->
<!-- ```{r} -->
<!-- linreg_women <- visstat(women$height, women$weight) -->
<!-- ``` -->
<div class="section level5">
<h5 id="dataset-trees">dataset: `trees``<a class="anchor" aria-label="anchor" href="#dataset-trees"></a>
</h5>
<p>The <code>trees</code> data set contains the diameter <code>Girth</code> in inches and <code>Volume</code> in cubic ft of 31 black cherry trees.</p>
<div class="sourceCode" id="cb12"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linreg_trees</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">trees</span><span class="op">$</span><span class="va">Girth</span>, <span class="va">trees</span><span class="op">$</span><span class="va">Volume</span>,conf.level<span class="op">=</span><span class="fl">0.9</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-8-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-8-2.png" width="100%"></p>
<p>p-values greater than <code>conf.level</code> in both the Anderson-Darling normality test and the Shapiro–Wilk test of the standardised residuals indicate that the normality assumption of the residuals underlying the linear regression is met.</p>
<p>Increasing the confidence level <code>conf.level</code> from the default 0.9 to 0.99 results in wider confidence intervals of the regression parameters as well as wider confidence and prediction bands</p>
<div class="sourceCode" id="cb13"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">linreg_trees_99</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">trees</span><span class="op">$</span><span class="va">Girth</span>, <span class="va">trees</span><span class="op">$</span><span class="va">Volume</span>,conf.level <span class="op">=</span> <span class="fl">0.99</span><span class="op">)</span></span></code></pre></div>
<p>The <code>visStatistics</code> plot allows to display only the second generated plot (the assumption plot is unchanged):</p>
<div class="sourceCode" id="cb14"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/graphics/plot.default.html" class="external-link">plot</a></span><span class="op">(</span><span class="va">linreg_trees_99</span>,which<span class="op">=</span><span class="fl">2</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-10-1.png" width="100%"></p>
<!-- #### dataset: `cars`{.unnumbered} -->
<!-- The `cars` dataset reports the speed of 50 cars in miles per hour (`speed`) and the stopping distance in feet (`dist`) recorded in  the 1920s. In this example the assuptions of normally distributed residuals is not met. -->
<!-- ```{r} -->
<!-- linreg_cars <- visstat(cars$speed, cars$dist) -->
<!-- ``` -->
</div>
</div>
</div>
</div>
<div class="section level2" number="7">
<h2 id="both-variables-categorical-comparing-proportions-1">
<span class="header-section-number">7</span> Both variables categorical: Comparing proportions<a class="anchor" aria-label="anchor" href="#both-variables-categorical-comparing-proportions-1"></a>
</h2>
<p>When both variables are categorical (i.e., of class <code>factor</code>), <code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis that the two variables are independent.
Observed frequencies are typically arranged in a contingency table, where rows index the levels <span class="math inline">\(i\)</span> of the response variable and columns index the levels <span class="math inline">\(j\)</span> of the predictor variable.</p>
<div class="section level3" number="7.1">
<h3 id="pearsons-residuals-and-mosaic-plots">
<span class="header-section-number">7.1</span> Pearson’s residuals and mosaic plots<a class="anchor" aria-label="anchor" href="#pearsons-residuals-and-mosaic-plots"></a>
</h3>
<p>Mosaic plots provide a graphical representation of contingency tables, where the area of each tile is proportional to the observed cell frequency.
To aid interpretation, tiles are coloured based on Pearson residuals from a chi-squared test of independence.
These residuals measure the standardised deviation of observed from expected counts under the null hypothesis of independence.</p>
<p>Let <span class="math inline">\(O_{ij}\)</span> and <span class="math inline">\(E_{ij}\)</span> denote the observed and expected frequencies in row <span class="math inline">\(i\)</span> and column <span class="math inline">\(j\)</span> of an <span class="math inline">\(R \times C\)</span> contingency table.
The Pearson residual for each cell is defined as</p>
<p><span class="math display">\[
r_{ij} = \frac{O_{ij} - E_{ij}}{\sqrt{E_{ij}}}, \quad i = 1, \ldots, R,\quad
j = 1, \ldots, C.
\]</span> Positive residuals (shaded in blue) indicate observed counts greater than expected, while negative values suggest under-representation (shaded in red).
Colour shading thus highlights which combinations of categorical levels contribute most to the overall association.</p>
</div>
<div class="section level3" number="7.2">
<h3 id="pearsons-chi2-test-chisq-test">
<span class="header-section-number">7.2</span> Pearson’s <span class="math inline">\(\chi^2\)</span>-test (<code>chisq.test()</code>)<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-chisq-test"></a>
</h3>
<p>The test statistic of Pearson’s <span class="math inline">\(\chi^2\)</span>-test <span class="citation">[<a href="#ref-Pearson:1900">31</a>]</span> is the sum of squared Pearson residuals:</p>
<p><span class="math display">\[
\chi^2 = \sum_{i=1}^{R} \sum_{j=1}^{C} r_{ij}^2 =
\sum_{i=1}^{R} \sum_{j=1}^{C} \frac{(O_{ij} - E_{ij})^2}{E_{ij}}.
\]</span></p>
<p>The test statistic is compared to the chi-squared distribution with $ (R - 1)(C - 1)$ degrees of freedom.
The resulting p-value corresponds to the upper tail probability — that is, the probability of observing a value greater than or equal to the test statistic under the null hypothesis.</p>
<div class="section level4" number="7.2.1">
<h4 id="pearsons-chi2-test-with-yates-continuity-correction">
<span class="header-section-number">7.2.1</span> Pearson’s <span class="math inline">\(\chi^2\)</span> test with Yates’ continuity correction<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-with-yates-continuity-correction"></a>
</h4>
<p>Yates’ correction is applied to the Pearson <span class="math inline">\(\chi^2\)</span> statistic in <span class="math inline">\(2 \times 2\)</span> contingency tables (with one degree of freedom).
In this case, the approximation of the discrete sampling distribution by the continuous <span class="math inline">\(\chi^2\)</span> distribution tends to overestimate the significance level of the test.
To correct for this, Yates proposed subtracting 0.5 from each absolute difference between observed and expected counts <span class="citation">[<a href="#ref-Yates:1934">32</a>]</span>, resulting in a smaller test statistic: <span class="math display">\[
\chi^2_{\text{Yates}} = \sum_{i=1}^{2} \sum_{j=1}^{2}
\frac{(|O_{ij} - E_{ij}| - 0.5)^2}{E_{ij}}.
\]</span></p>
<p>This reduced test statistic yields a larger p-value, thereby lowering the risk of a Type I error.</p>
<p>Yates’ continuity correction is applied by default by the underlying routine <code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>.
## Fisher’s exact test (<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>)</p>
<p>The <span class="math inline">\(\chi^2\)</span> approximation is considered reliable only if no expected cell count is less than 1 and no more than 20 percent of cells have expected counts below 5 <span class="citation">[<a href="#ref-Cochran:1954">11</a>]</span>).
If this condition is not met, Fisher’s exact test <span class="citation">[<a href="#ref-Fisher:1970">33</a>]</span> (<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>) is applied instead, as it is a non-parametric method that does not rely on large-sample approximations.
The test calculates an exact p-value for testing independence by conditioning on the observed margins: the row totals <span class="math inline">\(R_i = \sum_{j=1}^C O_{ij}\)</span> and the column totals <span class="math inline">\(C_j = \sum_{i=1}^R O_{ij}\)</span>, defining the structure of the contingency table.</p>
<p>In the <span class="math inline">\(2 \times 2\)</span> case, the observed table can be written as:</p>
<p><span class="math display">\[
\begin{array}{c|cc|c}
&amp; C_1 &amp; C_2 &amp; \text{Row sums} \\\\
\hline
R_1 &amp; a &amp; b &amp; a + b \\\\
R_2 &amp; c &amp; d &amp; c + d \\\\
\hline
\text{Column sums} &amp; a + c &amp; b + d &amp; n
\end{array}
\]</span></p>
<p>Let <span class="math display">\[
O = \begin{bmatrix} a &amp; b \\\\ c &amp; d \end{bmatrix}
\]</span> denote the above observed <span class="math inline">\(2 \times 2\)</span> contingency table.
The exact probability of observing this table under the null hypothesis of independence, given the fixed margins, is given by the hypergeometric probability mass function (PMF)</p>
<p><span class="math display">\[
\mathbb{P}(O \mid
R_1, R_2, C_1, C_2) =
\frac{\binom{a + b}{a} \binom{c + d}{c}}{\binom{n}{a + c}},
\]</span></p>
<p>where <span class="math inline">\(n = a + b + c + d\)</span> is the total sample size.</p>
<!-- Because all other cell values are determined by $a$ and the fixed margins,
-->
<!-- this is often written more simply as: -->
<!-- $$ -->
<!-- P(a \mid \text{margins}) = -->
<!-- \frac{\binom{a + b}{a} \binom{c + d}{c}}{\binom{n}{a + c}}. -->
<!-- $$ -->
<p>The p-value is computed by summing the probabilities of all tables with the same margins whose probabilities under the null are less than or equal to that of the observed table.</p>
<p>For general <span class="math inline">\(R \times C\)</span> tables, <code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code> generalises this approach using the multivariate hypergeometric distribution.</p>
</div>
</div>
<div class="section level3" number="7.3">
<h3 id="test-choice-and-graphical-output">
<span class="header-section-number">7.3</span> Test choice and graphical output<a class="anchor" aria-label="anchor" href="#test-choice-and-graphical-output"></a>
</h3>
<p>If the expected frequencies are sufficiently large - specifically, if at least 80% of the cells have expected counts greater than 5 and no expected count is smaller than 1, the function uses Pearson’s <span class="math inline">\({\chi}^2\)</span>-test (<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code>).</p>
<p>Otherwise, it switches to Fisher’s exact test (<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code>) <span class="citation">[<a href="#ref-Cochran:1954">11</a>]</span>.</p>
<p>For 2-by-2 contingency tables, Yates’ continuity correction <span class="citation">[<a href="#ref-Yates:1934">32</a>]</span> is always applied to Pearson’s <span class="math inline">\({\chi}^2\)</span>-test.</p>
<p>For all tests of independence <code><a href="../reference/visstat.html">visstat()</a></code> displays a grouped column plot that includes the respective test’s p-value in the title, as well as a mosaic plot showing colour-coded Pearson residuals and the p-value of Pearson’s <span class="math inline">\(\chi^2\)</span>-test.</p>
</div>
<div class="section level3" number="7.4">
<h3 id="transforming-a-contingency-table-to-a-data-frame">
<span class="header-section-number">7.4</span> Transforming a contingency table to a data frame<a class="anchor" aria-label="anchor" href="#transforming-a-contingency-table-to-a-data-frame"></a>
</h3>
<p>The following examples for tests of categorical predictor and response are all based on the <code>HairEyeColor</code> contingency table.</p>
<p>Contingency tables must be converted to the required column-based <code>data.frame</code> using the helper function <code><a href="../reference/counts_to_cases.html">counts_to_cases()</a></code>.
The function transforms the contingency table <code>HairEyeColor</code> into <code>data.frame</code> named <code>HairEyeColourDataFrame</code>.</p>
<div class="sourceCode" id="cb15"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">HairEyeColourDataFrame</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">HairEyeColor</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
</div>
<div class="section level3" number="7.5">
<h3 id="examples-3">
<span class="header-section-number">7.5</span> Examples<a class="anchor" aria-label="anchor" href="#examples-3"></a>
</h3>
<p>In all examples of this section, we will test the null hypothesis that hair colour (“Hair”) and eye colour (“Eye”) are independent of each other.</p>
<div class="section level4" number="7.5.1">
<h4 id="pearsons-chi2-test-chisq-test-1">
<span class="header-section-number">7.5.1</span> Pearson’s <span class="math inline">\({\chi}^2\)</span>-test (<code>chisq.test()</code>)<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-chisq-test-1"></a>
</h4>
<div class="sourceCode" id="cb16"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_eye_colour_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">HairEyeColor</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">hair_eye_colour_df</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">hair_eye_colour_df</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-12-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-12-2.png" width="100%"></p>
<p>The graphical output shows that the null hypothesis of Pearson’s <span class="math inline">\(\chi^2\)</span> test – namely, that hair colour and eye colour are independent – must be rejected at the default significance level <span class="math inline">\(\alpha=0.05\)</span> (<span class="math inline">\(p = 2.33 \cdot 10^{-25} &lt;
\alpha\)</span>).
The mosaic plot indicates that the strongest deviations are due to over-representation of individuals with black hair and brown eyes, and of those with blond hair and blue eyes.
In contrast, individuals with blond hair and brown eyes are the most under-represented.</p>
<div class="section level5" number="7.5.1.1">
<h5 id="pearsons-chi2-test-with-yates-continuity-correction-1">
<span class="header-section-number">7.5.1.1</span> Pearson’s <span class="math inline">\({\chi}^2\)</span>-test with Yate’s continuity correction<a class="anchor" aria-label="anchor" href="#pearsons-chi2-test-with-yates-continuity-correction-1"></a>
</h5>
<p>In the following example, we restrict the data to participants with either black or brown hair and either brown or blue eyes, resulting in a 2-by-2 contingency table.</p>
<div class="sourceCode" id="cb17"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_black_brown_eyes_brown_blue</span> <span class="op">&lt;-</span> <span class="va">HairEyeColor</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="op">]</span></span>
<span><span class="co"># Transform to data frame</span></span>
<span><span class="va">hair_black_brown_eyes_brown_blue_df</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">hair_black_brown_eyes_brown_blue</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Chi-squared test</span></span>
<span><span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">hair_black_brown_eyes_brown_blue_df</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">hair_black_brown_eyes_brown_blue_df</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-13-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-13-2.png" width="100%"></p>
<p>Also in this reduced dataset we reject the null hypothesis of independence of the hair colors “brown” and “black” from the eye colours “brown” and ” blue”.
The mosaic plot shows that blue-eyed persons with black hair are under- represented.
Note the higher p-value of Pearson’s <span class="math inline">\({\chi}^2\)</span>-test with Yate’s continuity correction (p = 0.00354) compared to the p-value of Pearson’s <span class="math inline">\({\chi}^2\)</span>-test (p = 0.00229) shown in the mosaic plot.</p>
</div>
</div>
<div class="section level4" number="7.5.2">
<h4 id="fishers-exact-test-fisher-test">
<span class="header-section-number">7.5.2</span> Fisher’s exact test (<code>fisher.test()</code>)<a class="anchor" aria-label="anchor" href="#fishers-exact-test-fisher-test"></a>
</h4>
<p>Again, we extract a 2-by-2 contingency table from the full dataset, this time keeping only male participants with black or brown hair and hazel or green eyes.</p>
<p>Pearson’s <span class="math inline">\({\chi}^2\)</span> test applied to this table would yield an expected frequency less than 5 in one of the four cells (25% of all cells), which violates the requirement that at least 80% of the expected frequencies must be 5 or greater <span class="citation">[<a href="#ref-Cochran:1954">11</a>]</span>.</p>
<p>Therefore, <code><a href="../reference/visstat.html">visstat()</a></code> automatically selects Fisher’s exact test instead.</p>
<div class="sourceCode" id="cb18"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">hair_eye_colour_male</span> <span class="op">&lt;-</span> <span class="va">HairEyeColor</span><span class="op">[</span>, , <span class="fl">1</span><span class="op">]</span></span>
<span><span class="co"># Slice out a 2 by 2 contingency table</span></span>
<span><span class="va">black_brown_hazel_green_male</span> <span class="op">&lt;-</span> <span class="va">hair_eye_colour_male</span><span class="op">[</span><span class="fl">1</span><span class="op">:</span><span class="fl">2</span>, <span class="fl">3</span><span class="op">:</span><span class="fl">4</span><span class="op">]</span></span>
<span><span class="co"># Transform to data frame</span></span>
<span><span class="va">black_brown_hazel_green_male</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/counts_to_cases.html">counts_to_cases</a></span><span class="op">(</span><span class="fu"><a href="https://rdrr.io/r/base/as.data.frame.html" class="external-link">as.data.frame</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">)</span><span class="op">)</span></span>
<span><span class="co"># Fisher test</span></span>
<span><span class="va">fisher_stats</span> <span class="op">&lt;-</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Eye</span>, <span class="va">black_brown_hazel_green_male</span><span class="op">$</span><span class="va">Hair</span><span class="op">)</span></span></code></pre></div>
<p><img src="visStatistics_files/figure-html/unnamed-chunk-14-1.png" width="100%"><img src="visStatistics_files/figure-html/unnamed-chunk-14-2.png" width="100%"></p>
</div>
</div>
</div>
<div class="section level2" number="8">
<h2 id="saving-the-graphical-output">
<span class="header-section-number">8</span> Saving the graphical output<a class="anchor" aria-label="anchor" href="#saving-the-graphical-output"></a>
</h2>
<p>All generated graphics can be saved in any file format supported by <code>Cairo()</code>, including “png”, “jpeg”, “pdf”, “svg”, “ps”, and “tiff” in the user specified <code>plotDirectory</code>.</p>
<p>If the optional argument <code>plotName</code> is not given, the naming of the output follows the pattern <code>"testname_namey_namex."</code>, where <code>"testname"</code> specifies the selected test and <code>"namey"</code> and <code>"namex"</code> are character strings naming the selected data vectors <code>y</code> and <code>x</code>, respectively.
The suffix corresponding to the chosen <code>graphicsoutput</code> (e.g., <code>"pdf"</code>, <code>"png"</code>) is then concatenated to form the complete output file name.</p>
<p>In the following example, we store the graphics in <code>png</code> format in the <code>plotDirectory</code> <code><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir()</a></code> with the default naming convention:</p>
<div class="sourceCode" id="cb19"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="co">#Graphical output written to plotDirectory: In this example </span></span>
<span><span class="co"># a bar chart to visualise the Chi-squared test and mosaic plot showing</span></span>
<span><span class="co"># Pearson's residuals.</span></span>
<span><span class="co">#chi_squared_or_fisher_Hair_Eye.png and mosaic_complete_Hair_Eye.png</span></span>
<span><span class="va">save_fisher</span> <span class="op">=</span> <span class="fu"><a href="../reference/visstat.html">visstat</a></span><span class="op">(</span><span class="va">black_brown_hazel_green_male</span>, <span class="st">"Hair"</span>, <span class="st">"Eye"</span>,</span>
<span>        graphicsoutput <span class="op">=</span> <span class="st">"png"</span>, plotDirectory <span class="op">=</span> <span class="fu"><a href="https://rdrr.io/r/base/tempfile.html" class="external-link">tempdir</a></span><span class="op">(</span><span class="op">)</span><span class="op">)</span></span></code></pre></div>
<p>The full file path of the generated graphics are stored as the attribute <code>"plot_paths"</code> on the returned object of class <code>"visstat"</code>.</p>
<div class="sourceCode" id="cb20"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="va">paths</span> <span class="op">&lt;-</span> <span class="fu"><a href="https://rdrr.io/r/base/attr.html" class="external-link">attr</a></span><span class="op">(</span><span class="va">save_fisher</span>, <span class="st">"plot_paths"</span><span class="op">)</span></span>
<span><span class="fu"><a href="https://rdrr.io/r/base/print.html" class="external-link">print</a></span><span class="op">(</span><span class="va">paths</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] "/var/folders/5c/n85wqnh95l50qbp3s9l0rp_w0000gn/T//Rtmpcx3xsE/chi_squared_or_fisher_Hair_Eye.png"</span></span>
<span><span class="co">#&gt; [2] "/var/folders/5c/n85wqnh95l50qbp3s9l0rp_w0000gn/T//Rtmpcx3xsE/mosaic_complete_Hair_Eye.png"</span></span></code></pre></div>
<p>Remove the graphical output from <code>plotDirectory</code>:</p>
<div class="sourceCode" id="cb21"><pre class="downlit sourceCode r">
<code class="sourceCode R"><span><span class="fu"><a href="https://rdrr.io/r/base/files.html" class="external-link">file.remove</a></span><span class="op">(</span><span class="va">paths</span><span class="op">)</span></span>
<span><span class="co">#&gt; [1] TRUE TRUE</span></span></code></pre></div>
<p>When assumptions plots (residual and Q-Q plot) are generated, the corresponding plot has the prefix <code>"assumption_</code>.</p>
</div>
<div class="section level2" number="9">
<h2 id="limitations">
<span class="header-section-number">9</span> Limitations<a class="anchor" aria-label="anchor" href="#limitations"></a>
</h2>
<div class="section level3" number="9.1">
<h3 id="limitations-by-default-settings">
<span class="header-section-number">9.1</span> Limitations by default settings<a class="anchor" aria-label="anchor" href="#limitations-by-default-settings"></a>
</h3>
<p>The main purpose of this package is a decision-logic based automatic visualisation of statistical test results.
Therefore, except for the user-adjustable <code>conf.level</code> parameter, all statistical tests are applied using their default settings from the corresponding base R functions.
As a consequence, paired tests are currently not supported and <code><a href="../reference/visstat.html">visstat()</a></code> does not allow to study interactions terms between the different levels of an independent variable in an analysis of variance.
Focusing on the graphical representation of tests, only simple linear regression is implemented, as multiple linear regressions cannot be visualised.</p>
</div>
<div class="section level3" number="9.2">
<h3 id="limitations-of-of-decision-logic-based-on-p-values">
<span class="header-section-number">9.2</span> Limitations of of decision-logic based on p-values<a class="anchor" aria-label="anchor" href="#limitations-of-of-decision-logic-based-on-p-values"></a>
</h3>
<p>The decision logic of the main function <code><a href="../reference/visstat.html">visstat()</a></code> is based on the Shapiro-Wilk test for normality and the Levene-Brown-Forsythe test for variance homogeneity.
However, no single test maintains optimal Type I error rates and statistical power across all distributions <span class="citation">[<a href="#ref-Olejnik:1987">34</a>]</span>, and p-values obtained from these tests may be unreliable if their assumptions are violated.</p>
<p>Combining multiple tests with differing assumptions using simple majority voting inflates the overall Type I error rate, making such an approach inadequate.
Therefore, automated test selection based solely on p-values cannot replace the visual inspection of sample distributions provided by <code><a href="../reference/visstat.html">visstat()</a></code>.
Based on these plots, it may be necessary to override the automated choice of test in individual cases.</p>
<p>In contrast, bootstrapping methods <span class="citation">[<a href="#ref-Wilcox:2021">35</a>]</span> make minimal distributional assumptions and can provide confidence intervals for nearly any statistic.
However, bootstrapping is computationally intensive, often requiring thousands of resamples, and may perform poorly with very small sample sizes.
This runs counter to the purpose of the <code>visStatistics</code> package, which is designed to offer a rapid overview of the data, laying the groundwork for deeper analysis in subsequent steps.</p>
<p>The package targets users with basic statistical literacy, such as non-specialist professionals and students in applied fields.
It covers topics typically included in an undergraduate course on applied statistics, intentionally excluding more advanced methods like bootstrapping to keep the focus on foundational concepts.</p>
</div>
</div>
<div class="section level2" number="10">
<h2 id="implemented-tests">
<span class="header-section-number">10</span> Implemented tests<a class="anchor" aria-label="anchor" href="#implemented-tests"></a>
</h2>
<div class="section level3" number="10.1">
<h3 id="numeric-response-and-categorical-predictor">
<span class="header-section-number">10.1</span> Numeric response and categorical predictor<a class="anchor" aria-label="anchor" href="#numeric-response-and-categorical-predictor"></a>
</h3>
<p>When the response is numeric and the predictor is categorical, a test of central tendency is selected:</p>
<p><code><a href="https://rdrr.io/r/stats/t.test.html" class="external-link">t.test()</a></code>, <code><a href="https://rdrr.io/r/stats/wilcox.test.html" class="external-link">wilcox.test()</a></code>, <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>, <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>,<code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code></p>
<div class="section level4" number="10.1.1">
<h4 id="normality-assumption-check">
<span class="header-section-number">10.1.1</span> Normality assumption check<a class="anchor" aria-label="anchor" href="#normality-assumption-check"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/shapiro.test.html" class="external-link">shapiro.test()</a></code> and <code>ad.test()</code> <span class="citation">[<a href="#ref-Gross:2015">20</a>]</span></p>
</div>
<div class="section level4" number="10.1.2">
<h4 id="homoscedasticity-assumption-check">
<span class="header-section-number">10.1.2</span> Homoscedasticity assumption check<a class="anchor" aria-label="anchor" href="#homoscedasticity-assumption-check"></a>
</h4>
<p><code><a href="https://rdrr.io/r/stats/bartlett.test.html" class="external-link">bartlett.test()</a></code> and <code><a href="../reference/levene.test.html">levene.test()</a></code></p>
</div>
<div class="section level4" number="10.1.3">
<h4 id="post-hoc-tests">
<span class="header-section-number">10.1.3</span> Post-hoc tests<a class="anchor" aria-label="anchor" href="#post-hoc-tests"></a>
</h4>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/TukeyHSD.html" class="external-link">TukeyHSD()</a></code> (for <code><a href="https://rdrr.io/r/stats/aov.html" class="external-link">aov()</a></code>and <code><a href="https://rdrr.io/r/stats/oneway.test.html" class="external-link">oneway.test()</a></code>)</li>
<li>
<code><a href="https://rdrr.io/r/stats/pairwise.wilcox.test.html" class="external-link">pairwise.wilcox.test()</a></code> (for <code><a href="https://rdrr.io/r/stats/kruskal.test.html" class="external-link">kruskal.test()</a></code>)</li>
</ul>
</div>
</div>
<div class="section level3" number="10.2">
<h3 id="numeric-response-and-numeric-predictor">
<span class="header-section-number">10.2</span> Numeric response and numeric predictor<a class="anchor" aria-label="anchor" href="#numeric-response-and-numeric-predictor"></a>
</h3>
<p>When both the response and predictor are numeric, a simple linear regression model is fitted:</p>
<p><code><a href="https://rdrr.io/r/stats/lm.html" class="external-link">lm()</a></code></p>
</div>
<div class="section level3" number="10.3">
<h3 id="both-variables-categorical">
<span class="header-section-number">10.3</span> Both variables categorical<a class="anchor" aria-label="anchor" href="#both-variables-categorical"></a>
</h3>
<p>When both variables are categorical, <code><a href="../reference/visstat.html">visstat()</a></code> tests the null hypothesis of independence using one of the following:</p>
<ul>
<li>
<code><a href="https://rdrr.io/r/stats/chisq.test.html" class="external-link">chisq.test()</a></code> (default for larger samples)</li>
<li>
<code><a href="https://rdrr.io/r/stats/fisher.test.html" class="external-link">fisher.test()</a></code> (used for small expected cell counts based on Cochran’s rule)</li>
</ul>
</div>
</div>
<div class="section level2">
<h2 id="bibliography">Bibliography<a class="anchor" aria-label="anchor" href="#bibliography"></a>
</h2>
<div id="refs" class="references csl-bib-body">
<div id="ref-Rasch:2011" class="csl-entry">
<div class="csl-left-margin">[1] </div>
<div class="csl-right-inline">
<span class="smallcaps">Rasch</span>, D., <span class="smallcaps">Kubinger</span>, K. D. and <span class="smallcaps">Moder</span>, K. (2011). <a href="https://doi.org/10.1007/s00362-009-0224-x" class="external-link">The two-sample t test: Pre-testing its assumptions does not pay off</a>. <em>Stat Papers</em> <strong>52</strong> 219–31.</div>
</div>
<div id="ref-Lumley:2002" class="csl-entry">
<div class="csl-left-margin">[2] </div>
<div class="csl-right-inline">
<span class="smallcaps">Lumley</span>, T., <span class="smallcaps">Diehr</span>, P., <span class="smallcaps">Emerson</span>, S. and <span class="smallcaps">Chen</span>, L. (2002). <a href="https://doi.org/10.1146/annurev.publhealth.23.100901.140546" class="external-link">The <span>Importance</span> of the <span>Normality Assumption</span> in <span>Large Public Health Data Sets</span></a>. <em>Annu. Rev. Public Health</em> <strong>23</strong> 151–69.</div>
</div>
<div id="ref-Kwak:2017" class="csl-entry">
<div class="csl-left-margin">[3] </div>
<div class="csl-right-inline">
<span class="smallcaps">Kwak</span>, S. G. and <span class="smallcaps">Kim</span>, J. H. (2017). <a href="https://doi.org/10.4097/kjae.2017.70.2.144" class="external-link">Central limit theorem: The cornerstone of modern statistics</a>. <em>Korean J Anesthesiol</em> <strong>70</strong> 144–56.</div>
</div>
<div id="ref-Moser:1992" class="csl-entry">
<div class="csl-left-margin">[4] </div>
<div class="csl-right-inline">
<span class="smallcaps">Moser</span>, B. K. and <span class="smallcaps">Stevens</span>, G. R. (1992). <a href="https://doi.org/10.1080/00031305.1992.10475839" class="external-link">Homogeneity of variance in the two-sample means test</a>. <em>The American Statistician</em> 19–21.</div>
</div>
<div id="ref-Fagerland:2009" class="csl-entry">
<div class="csl-left-margin">[5] </div>
<div class="csl-right-inline">
<span class="smallcaps">Fagerland</span>, M. W. and <span class="smallcaps">Sandvik</span>, L. (2009). <a href="https://doi.org/10.1016/j.cct.2009.06.007" class="external-link">Performance of five two-sample location tests for skewed distributions with unequal variances</a>. <em>Contemporary Clinical Trials</em> <strong>30</strong> 490–6.</div>
</div>
<div id="ref-Delacre:2017" class="csl-entry">
<div class="csl-left-margin">[6] </div>
<div class="csl-right-inline">
<span class="smallcaps">Delacre</span>, M., <span class="smallcaps">Lakens</span>, D. and <span class="smallcaps">Leys</span>, C. (2017). <a href="https://doi.org/10.5334/irsp.82" class="external-link">Why psychologists should by default use <span>Welch</span>’s t-test instead of <span>Student</span>’s t-test</a>. <em>International Review of Social Psychology</em> <strong>30</strong> 92–101.</div>
</div>
<div id="ref-Shapiro:1965" class="csl-entry">
<div class="csl-left-margin">[7] </div>
<div class="csl-right-inline">
<span class="smallcaps">SHAPIRO</span>, S. S. and <span class="smallcaps">WILK</span>, M. B. (1965). <a href="https://doi.org/10.1093/biomet/52.3-4.591" class="external-link">An analysis of variance test for normality (complete samples)<span></span></a>. <em>Biometrika</em> <strong>52</strong> 591–611.</div>
</div>
<div id="ref-Razali:2011" class="csl-entry">
<div class="csl-left-margin">[8] </div>
<div class="csl-right-inline">
<span class="smallcaps">Razali</span>, N. M. and <span class="smallcaps">Wah</span>, Y. B. (2011). Power comparisons of <span>Shapiro-Wilk</span>, <span>Kolmogorov-Smirnov</span>, <span>Lilliefors</span> and <span>Anderson-Darling</span> tests. <em>Journal of Statistical Modeling and Analytics</em> <strong>2</strong> 21–33.</div>
</div>
<div id="ref-Ghasemi:2012" class="csl-entry">
<div class="csl-left-margin">[9] </div>
<div class="csl-right-inline">
<span class="smallcaps">Ghasemi</span>, A. and <span class="smallcaps">Zahediasl</span>, S. (2012). <a href="https://doi.org/10.5812/ijem.3505" class="external-link">Normality <span>Tests</span> for <span>Statistical Analysis</span>: <span>A Guide</span> for <span>Non-Statisticians</span></a>. <em>Int J Endocrinol Metab</em> <strong>10</strong> 486–9.</div>
</div>
<div id="ref-Brown:1974" class="csl-entry">
<div class="csl-left-margin">[10] </div>
<div class="csl-right-inline">
<span class="smallcaps">Brown</span>, M. B. and <span class="smallcaps">Forsythe</span>, A. B. (1974). <a href="https://doi.org/10.1080/01621459.1974.10482955" class="external-link">Robust <span>Tests</span> for the <span>Equality</span> of <span>Variances</span></a>. <em>Journal of the American Statistical Association</em> <strong>69</strong> 364–7.</div>
</div>
<div id="ref-Cochran:1954" class="csl-entry">
<div class="csl-left-margin">[11] </div>
<div class="csl-right-inline">
<span class="smallcaps">Cochran</span>, W. G. (1954). <a href="https://doi.org/10.2307/3001666" class="external-link">The <span>Combination</span> of <span>Estimates</span> from <span>Different Experiments</span></a>. <em>Biometrics</em> <strong>10</strong> 101.</div>
</div>
<div id="ref-Welch:1947" class="csl-entry">
<div class="csl-left-margin">[12] </div>
<div class="csl-right-inline">
<span class="smallcaps">Welch</span>, B. L. (1947). <a href="https://doi.org/10.1093/biomet/34.1-2.28" class="external-link">The generalization of <span>“student’s”</span> problem when several different population variances are involved</a>. <em>Biometrika</em> <strong>34</strong> 28–35.</div>
</div>
<div id="ref-Satterthwaite:1946" class="csl-entry">
<div class="csl-left-margin">[13] </div>
<div class="csl-right-inline">
<span class="smallcaps">Satterthwaite</span>, F. E. (1946). <a href="https://doi.org/10.2307/3002019" class="external-link">An <span>Approximate Distribution</span> of <span>Estimates</span> of <span>Variance Components</span></a>. <em>Biometrics Bulletin</em> <strong>2</strong> 110–4.</div>
</div>
<div id="ref-Mann:1947" class="csl-entry">
<div class="csl-left-margin">[14] </div>
<div class="csl-right-inline">
<span class="smallcaps">Mann</span>, H. B. and <span class="smallcaps">Whitney</span>, D. R. (1947). <a href="https://doi.org/10.1214/aoms/1177730491" class="external-link">On a test of whether one of two random variables is stochastically larger than the other</a>. <em>The Annals of Mathematical Statistics</em> <strong>18</strong> 50–60.</div>
</div>
<div id="ref-Hollander:2014" class="csl-entry">
<div class="csl-left-margin">[15] </div>
<div class="csl-right-inline">
<span class="smallcaps">Hollander</span>, M., <span class="smallcaps">Chicken</span>, E. and <span class="smallcaps">Wolfe</span>, D. A. (2014). <em>Nonparametric statistical methods</em>. John Wiley &amp; Sons, Inc, Hoboken, New Jersey.</div>
</div>
<div id="ref-Fisher:1935" class="csl-entry">
<div class="csl-left-margin">[16] </div>
<div class="csl-right-inline">
<span class="smallcaps">Fisher</span>, R. A. (1971). <em>The design of experiments</em>. Macmillan.</div>
</div>
<div id="ref-Welch:1951" class="csl-entry">
<div class="csl-left-margin">[17] </div>
<div class="csl-right-inline">
<span class="smallcaps">Welch</span>, B. L. (1951). <a href="https://doi.org/10.2307/2332579" class="external-link">On the <span>Comparison</span> of <span>Several Mean Values</span>: <span>An Alternative Approach</span></a>. <em>Biometrika</em> <strong>38</strong> 330–6.</div>
</div>
<div id="ref-Kruskal:1952" class="csl-entry">
<div class="csl-left-margin">[18] </div>
<div class="csl-right-inline">
<span class="smallcaps">Kruskal</span>, W. H. and <span class="smallcaps">Wallis</span>, W. A. (1952). <a href="https://doi.org/10.2307/2280779" class="external-link">Use of <span>Ranks</span> in <span>One-Criterion Variance Analysis</span></a>. <em>Journal of the American Statistical Association</em> <strong>47</strong> 583–621.</div>
</div>
<div id="ref-Fisher:1990" class="csl-entry">
<div class="csl-left-margin">[19] </div>
<div class="csl-right-inline">
<span class="smallcaps">Fisher</span>, R. A. and <span class="smallcaps">Yates</span>, F. (1990). <em><a href="https://doi.org/10.1093/oso/9780198522294.001.0001" class="external-link">Statistical <span>Methods</span>, <span>Experimental Design</span>, and <span>Scientific Inference</span>: <span class="nocase">A Re-issue</span> of <span>Statistical Methods</span> for <span>Research Workers</span>, the <span>Design</span> of <span>Experiments</span> and <span>Statistical Methods</span> and <span>Scientific Inference</span></a></em>. J. H. Bennett, ed Oxford University PressOxford.</div>
</div>
<div id="ref-Gross:2015" class="csl-entry">
<div class="csl-left-margin">[20] </div>
<div class="csl-right-inline">
<span class="smallcaps">Gross</span>, J. and <span class="smallcaps">Ligges</span>, U. (2015). <em><a href="https://doi.org/10.32614/CRAN.package.nortest" class="external-link">Nortest: <span>Tests</span> for normality</a></em>.</div>
</div>
<div id="ref-Yap:2011" class="csl-entry">
<div class="csl-left-margin">[21] </div>
<div class="csl-right-inline">
<span class="smallcaps">Yap</span>, B. W. and <span class="smallcaps">Sim</span>, C. H. (2011). <a href="https://doi.org/10.1080/00949655.2010.520163" class="external-link">Comparisons of various types of normality tests</a>. <em>Journal of Statistical Computation and Simulation</em> <strong>81</strong> 2141–55.</div>
</div>
<div id="ref-Kozak:2018" class="csl-entry">
<div class="csl-left-margin">[22] </div>
<div class="csl-right-inline">
<span class="smallcaps">Kozak</span>, M. and <span class="smallcaps">Piepho</span>, H.-P. (2018). <a href="https://doi.org/10.1111/jac.12220" class="external-link">What’s normal anyway? <span>Residual</span> plots are more telling than significance tests when checking <span><span class="smallcaps">ANOVA</span></span> assumptions</a>. <em>J Agronomy Crop Science</em> <strong>204</strong> 86–98.</div>
</div>
<div id="ref-Shatz:2024" class="csl-entry">
<div class="csl-left-margin">[23] </div>
<div class="csl-right-inline">
<span class="smallcaps">Shatz</span>, I. (2024). <a href="https://doi.org/10.3758/s13428-023-02072-x" class="external-link">Assumption-checking rather than (just) testing: <span>The</span> importance of visualization and effect size in statistical diagnostics</a>. <em>Behav Res</em> <strong>56</strong> 826–45.</div>
</div>
<div id="ref-Levene:1960" class="csl-entry">
<div class="csl-left-margin">[24] </div>
<div class="csl-right-inline">
<span class="smallcaps">Levene</span>, H. (1960). Robust tests for equality of variances. In <em>Contributions to probability and statistics: <span>Essays</span> in honor of harold hotelling</em> (I. Olkin, ed) pp 278–92. Stanford University Press, Stanford, CA.</div>
</div>
<div id="ref-Fox:2019" class="csl-entry">
<div class="csl-left-margin">[25] </div>
<div class="csl-right-inline">
<span class="smallcaps">Fox</span>, J. and <span class="smallcaps">Weisberg</span>, S. (2019). <em>An <span>R</span> companion to applied regression</em>. Sage, Thousand Oaks CA.</div>
</div>
<div id="ref-Abdi:2007" class="csl-entry">
<div class="csl-left-margin">[26] </div>
<div class="csl-right-inline">
<span class="smallcaps">Abdi</span>, H. (2007). The bonferonni and <span>Sidak</span> corrections for multiple comparisons. <em>Encyclopedia of Measurement and Statistics</em>.</div>
</div>
<div id="ref-Sidak:1967" class="csl-entry">
<div class="csl-left-margin">[27] </div>
<div class="csl-right-inline">
<span class="smallcaps">Šidák</span>, Z. (1967). <a href="https://doi.org/10.1080/01621459.1967.10482935" class="external-link">Rectangular confidence regions for the means of multivariate normal distributions</a>. <em>Journal of the American Statistical Association</em> <strong>62</strong> 626–33.</div>
</div>
<div id="ref-Hochberg:1987" class="csl-entry">
<div class="csl-left-margin">[28] </div>
<div class="csl-right-inline">
<span class="smallcaps">Hochberg</span>, Y. and <span class="smallcaps">Tamhane</span>, A. C. (1987). <em><a href="https://doi.org/10.1002/9780470316672" class="external-link">Multiple <span>Comparison Procedures</span></a></em>. Wiley.</div>
</div>
<div id="ref-Holm:1979" class="csl-entry">
<div class="csl-left-margin">[29] </div>
<div class="csl-right-inline">
<span class="smallcaps">Holm</span>, S. (1979). <a href="https://www.jstor.org/stable/4615733" class="external-link">A <span>Simple Sequentially Rejective Multiple Test Procedure</span></a>. <em>Scandinavian Journal of Statistics</em> <strong>6</strong> 65–70.</div>
</div>
<div id="ref-Graves:2024" class="csl-entry">
<div class="csl-left-margin">[30] </div>
<div class="csl-right-inline">
<span class="smallcaps">Graves</span>, S., <span class="smallcaps">Piepho</span>, H.-P. and <span class="smallcaps">with help from Sundar Dorai-Raj</span>, L. S. (2024). <em><a href="https://doi.org/10.32614/CRAN.package.multcompView" class="external-link"><span class="nocase">multcompView</span>: <span>Visualizations</span> of paired comparisons</a></em>.</div>
</div>
<div id="ref-Pearson:1900" class="csl-entry">
<div class="csl-left-margin">[31] </div>
<div class="csl-right-inline">
<span class="smallcaps">Pearson</span>, K. (1900). <a href="https://doi.org/10.1080/14786440009463897" class="external-link">On the criterion that a given system of deviations from the probable in the case of a correlated system of variables is such that it can be reasonably supposed to have arisen from random sampling</a>. <em>The London, Edinburgh, and Dublin Philosophical Magazine and Journal of Science</em> <strong>50</strong> 157–75.</div>
</div>
<div id="ref-Yates:1934" class="csl-entry">
<div class="csl-left-margin">[32] </div>
<div class="csl-right-inline">
<span class="smallcaps">Yates</span>, F. (1934). <a href="https://doi.org/10.2307/2983604" class="external-link">Contingency <span>Tables Involving Small Numbers</span> and the <span><span class="math inline">\(X\)</span></span>2 <span>Test</span></a>. <em>Journal of the Royal Statistical Society Series B: Statistical Methodology</em> <strong>1</strong> 217–35.</div>
</div>
<div id="ref-Fisher:1970" class="csl-entry">
<div class="csl-left-margin">[33] </div>
<div class="csl-right-inline">
<span class="smallcaps">Fisher</span>, R. A. (1970). <em>Statistical methods for research workers</em>. <span>Oliver and Boyd</span>, Edinburgh.</div>
</div>
<div id="ref-Olejnik:1987" class="csl-entry">
<div class="csl-left-margin">[34] </div>
<div class="csl-right-inline">
<span class="smallcaps">Olejnik</span>, S. F. and <span class="smallcaps">Algina</span>, J. (1987). <a href="https://doi.org/10.2307/1164627" class="external-link">Type <span>I Error Rates</span> and <span>Power Estimates</span> of <span>Selected Parametric</span> and <span>Nonparametric Tests</span> of <span>Scale</span></a>. <em>Journal of Educational Statistics</em> <strong>12</strong> 45.</div>
</div>
<div id="ref-Wilcox:2021" class="csl-entry">
<div class="csl-left-margin">[35] </div>
<div class="csl-right-inline">
<span class="smallcaps">Wilcox</span>, R. R. (2021). <em>Introduction to <span>Robust Estimation</span> and <span>Hypothesis Testing</span></em>.</div>
</div>
</div>
</div>
  </main><aside class="col-md-3"><nav id="toc" aria-label="Table of contents"><h2>On this page</h2>
    </nav></aside>
</div>



    <footer><div class="pkgdown-footer-left">
  <p>Developed by Sabine Schilling.</p>
</div>

<div class="pkgdown-footer-right">
  <p>Site built with <a href="https://pkgdown.r-lib.org/" class="external-link">pkgdown</a> 2.1.3.</p>
</div>

    </footer>
</div>





  </body>
</html>
